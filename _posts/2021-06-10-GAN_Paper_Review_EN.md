---
title: "GAN"
date: 2021-06-10 08:26:28 -0400
categories: Deep Learning
---

### GAN(Generative Adversarial Nets)

<br>
<br>
<br>
<br>
<br>
<br>

## 0. Introduction   

* In 2014, Ian J. Goodfellow published a new model called Generative Adversarial Nets (GANs).



* You read the paper by below link.

  [Generative Adversarial Nets](https://papers.nips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf)
  
  
* You would be easily aware that 'Nets' is well-known 'Network' but the exact meaning of 'Adversarial' or 'Generative' is a little bit difficult.

  
* We will review what 'GAN' is in this post.
<br>
<br>
<br>
<br>
<br>
<br>

## 1. Background

### 1.1. Generative Model   

* GAN is able to generate a data that **is likely to** exist but actually not exist.


* Below image is from the paper of Ian Goodfellow.


* The blue ones are real image and the red ones are generated by GAN.  They seem to be real one.

<br>
<br>

<p align="left">
  <img src="/assets/GAN/pic_00.png">
</p>

<br>
<br>
<br>
<br>
<br>
<br>

### 1.2. Probability Distribution

* I really hate math., but we need to know the basic background of math.(Especially, probability) because the paper is full of math.

<br>
<br>

* First, let's dive into 'Probability Distribution'

<br>
<br>

* Probability Distribution is a function that represents the probability that a random variable has a certain value.
  
  If a random variable X is the number of possible outcomes when a dice is rolled, P(X=1) is 1/6, and all P(X=1~6) have the same value (1/6).  

<br>
<br>

* There are 2 kind of random distributions, Discrete Probability Distribution & Continuous Probability Distribution.

<br>
<br>
<br>
<br>
   

#### 1.2.1. Discrete Probability Distribution

* Discrete probability distribution is a case in which the number of random variables X can be counted.

<br>

* The dice example at the previous example belongs to the discrete probability distribution.

<br>
<br>

<p align="left">
  <img src="/assets/GAN/pic_01.png">
</p>

<br>
<br>
<br>
<br>

#### 1.2.2. Continuous Probability Distribution

* Continuous Probability Distribution is a probability distribution when the number of random variables X cannot be accurately counted.

<br>

* Since the number of random variables X cannot be counted accurately, in this case, the probability distribution is expressed using the probability density function.

<br>

* Examples of this might be things like height or running performance.

<br>

* Normal distribution can also be called continuous probability distribution.

<p align="left">
  <img src="/assets/GAN/pic_02.png">
</p>

<br>
<br>
<br>

* In reality, the distribution of many data can be approximated as a normal distribution, and this fact is very useful for expressing and utilizing actual data.

<br>

* One of the example of a normal distribution in the real world is IQ.

<br>

<p align="left">
  <img src="/assets/GAN/pic_03.png">
</p>

<br>
<br>
<br>
<br>

#### 1.2.3. Probability Distribution of Image

* The term 'Probability Distribution for an Image' may seem a bit cryptic.

<br>

* If you think about it, an image can also be expressed as a point in a multidimensional feature space. ( High-dimensional Vector or Matrix )

<br>

* In other words, a model that approximates this probability distribution by expressing the image data as a probability distribution can be learned using GAN.

<br>

* 'What probability distribution is there in image data?'
  * Human faces have **statistical averages**.
  * For example, there could be values such as the relative positions of the eyes, nose, mouth, etc.
  * This means that these numbers can be expressed as probability distributions.

<br>
  
* In Image, various features can each be a random variable, which means a distribution.

<br>

* For example, below one represents 'Multivariate Probability Distribution'.

<p align="left">
  <img src="/assets/GAN/pic_04.png">
</p>

<br>
<br>
<br>
<br>
<br>
<br>

## 2. Generative Model

<br>
<br>   

### 2.1.  Generative Model vs Discriminative Model

* Machine learning or deep learning models before GAN learn patterns from data, the model outputs a specific value based on the learning result for new data and discriminates it based on value from model output.

<br>

* In other words, a discriminative model is to learn a **specific decision boundary**.

<br>

* 이에 반해, 생성 모델은 확률 분포 Data를 학습하여 실제로는 존재하지 않지만, 학습한 확률 분포와 유사한 확률 분포를 생성하는 Model을 말합니다.

<br>

* 즉, 생성 모델은 **통계적인 평균치**를 학습하고 이와 유사한 확률 분포를 생성한다는 의미입니다.

<p align="left">
  <img src="/assets/GAN/pic_05.png">
</p>

<br>
<br>
<br>

### 2.2.  생성 모델의 목표(Purpose of Generative Model)

* 앞에서 몇 번 언급한 바와 같이, 생성 모델의 목표는 확률 분포를 근사하는 모델 G를 생성하는 것입니다.

<br>

* 생성 모델은 다음과 같은 과정으로 학습을 진행하며 원래 Data의 확률 분포를 근사화하도록 학습을 진행합니다.

<br>

* G가 학습이 잘 이루어졌다면, 원본 Data와 통계적 평균이 유사한 Data를 쉽게 생성할 수 있을 것입니다.

<br>

* 이 그래프의 좀 더 자세한 설명은 나중에 하도록 하겠습니다.

<p align="left">
  <img src="/assets/GAN/pic_06.png">
</p>

<br>
<br>
<br>
<br>
<br>
<br>

## 3. GAN(Generative Adversarial Nets)

<br>
<br>

* 생성자(Generator)와 판별자(Discriminator), 2개의 Network을 활용한 생성 Model입니다.

<br>

* 아래 그림은 Tensorflow의 GAN Page에서 가져온 것이며, GAN의 기본적인 구조를 나타냅니다.

<br>

* 크게 2가지의 Model, 생성자(Generator)와 판별자(Discriminator)가 있습니다.

<br>

* Generator는 원본 Data 확률 분포를 학습하여 원본 확률 분포와 유사한 Data를 생성하는 방향으로 학습하고, Discriminator는 Generator가 생성하는 Data가 Real인지 Fake인지 구분을 잘하는 방향으로 학습합니다.

<br>

* 이 2개의 Model이 경쟁적(Adversarial)으로 학습을 진행하여 최종적으로 Generator의 성능을 향상시키는 것이 목적입니다.

<br>

<p align="left">
  <img src="/assets/GAN/pic_05-1.png">
</p>
   
<br>
<br>

* 훈련과정 동안 생성자는 점차 실제같은 이미지를 더 잘 생성하게 되고, 감별자는 점차 진짜와 가짜를 더 잘 구별하게됩니다. 이 과정은 감별자가 가짜 이미지에서 진짜 이미지를 더이상 구별하지 못하게 될때, 평형상태에 도달하게 됩니다.

<br>

<p align="left">
  <img src="/assets/GAN/pic_05-2.png">
</p>

<br>
<br>
<br>

* 판별자(Discriminator)의 역할은 생성자 G가 학습을 잘 할 수 있도록 도와주는 역할이며, 최종적으로 얻으려는 것은 생성자(Generator , G)입니다.

<br>

* 다음의 수식은 Ian의 Paper에 나오는 수식이며, GAN의 동작을 잘 설명하고 있습니다.   

<br>
<br>

<p align="left">
  <img src="/assets/GAN/pic_07.png">
</p>

<br>

* 위의 수식의 의미를 하나씩 알아보도록 하겠습니다.

<br>

<p align="left">
  <img src="/assets/GAN/pic_08.png">
</p>

<br>

* #1 : 전체 함수 V는 D와 G로 구성되어 있다.

<br>

* #2 : D는 전체 수식 V를 Maximize하려는 방향으로 학습한다.

<br>

* #3 : 반대로 G는 전체 수식 V를 Minimize하려는 방향으로 학습한다.

<br>
<br>
<br>

* 우선, D가 포함된 항부터 알아보겠습니다.   

<br>

<p align="left">
  <img src="/assets/GAN/pic_09.png">
</p>

<br>   

* #1 : 확률 분포 P에서 Data 하나(x)를 꺼냅니다.

<br>

* #2 : 꺼낸 Data(x)를 함수 D에 넣은 결과에 log를 취해서

<br>

* #3 : 기대값(E)를 구한다는 의미입니다. 여기서 기대값이란 평균을 구한다고 생각하시면 됩니다.

<br>
<br>
<br>

* G가 포함된 항을 알아보겠습니다.   

<br>

<p align="left">
  <img src="/assets/GAN/pic_10.png">
</p>

<br>

* #1 : 확률 분포 z란 Latent Vector이며, Noise Vector입니다. Latent Vector에서 z를 하나 꺼냅니다.

<br>

* #2 : z를 G에 넣어서 G가 생성한 값을 D에 넣은 결과를 1에서 뺀 후 log를 취합니다.

<br>

* #3 : log를 취한 결과를 전체 기대값(평균)을 구합니다.

<br>
<br>
<br>

* 위의 수식에서 G(z)는 Generator로써, Latent Vector z로부터 새로운 Data Instance를 생성합니다.

<br>

* D(x)는 확률 분포 x가 Real인지 Fake인지 확률을 Return해 줍니다. ( Real : 1 , Fake : 0 )

<br>
<br>

* V는 D와 G에 의해서 최적화 된다고 볼 수 있으며, 결과적으로 G는 그럴싸한 Data를 생성할 수 있을 것이라고 Paper에서는 이야기 하고 있습니다.   

<br>
<br>
<br>

### 3.1.  기대값의 계산

<br>

* 위의 수식에서 E(기대값) 계산을 하고 있는데, 실제 Code에서 구현하는 가장 간단한 방법은 단순히 모든 Data에 대해서 값을 구한 다음에 **평균**을 구하면 됩니다.

<br>

* 보통 E(기대값)은 많은 Data를 다룰 때, 평균값을 구하고자 하는 경우에 사용합니다.

<br>
<br>

<p align="left">
  <img src="/assets/GAN/pic_11.png">
</p>

* 위의 식은 원본 Data 분포에서 x를 뽑아서 logD(x)의 기대값을 계산하라는 의미입니다.   

<br>
<br>
<br>

<p align="left">
  <img src="/assets/GAN/pic_12.png">
</p>

<br>

* 위의 수식은 Latent Vector에서 z를 뽑아, log(1 - D(G(z)))의 기대값을 계산한다는 의미입니다.

<br>

* 실제 Code에서는 반복문으로 모든 Data에 대해서 계산하면 됩니다.   

<br>
<br>
<br>
<br>

### 3.2.  학습 과정

<br>
<br>

* 학습의 목표는 다음 수식으로 설명된다.   

<br>

<p align="left">
  <img src="/assets/GAN/pic_13.png">
</p>

<br>
<br>

* G가 학습이 끝나면 D는 G가 생성해낸 Real / Fake를 구분하지 못하기 때문에 확률을 1/2로 Return합니다.

<br>

* 위의 수식의 증명은 Paper에 아주 복잡한 수식으로 증명되어 있으니, 참고하시기 바랍니다.

<br>
<br>

* 학습 과정을 이전에 보여드렸던 그림으로 다시 보도록 하겠습니다.   
<br>

<p align="left">
  <img src="/assets/GAN/pic_14.png">
</p>

<br>

* 검은색 확률 분포는 원본 Data의 확률 분포를 나타내고, 초록색은 Generator가 생성하는 확률 분포, 파란색은 Discriminator의 Return 값을 나타냅니다.

<br>

* 왼쪽에서 오른쪽으로 학습이 진행될수록 생성 Model이 생성하는 확률 분포가 원본의 확률 분포를 따라가고, 판변 Model은 1/2로 수렴한다는 것을 나타냅니다.

<br>
<br>
<br>
<br>

### 3.3.  GAN의 학습 알고리즘

<br>
<br>

* 아래 Pseudo Code는 Paper에 나와있는 Code입니다.

<br>

<p align="left">
  <img src="/assets/GAN/pic_16.png">
</p>

<br>
<br>

* 먼저 D(Discriminator)를 먼저 학습합니다.

<br>

* m개의 Sample을 Latent Vector에서 추출하고, 원본 Data에서도 m개의 Sample을 추출합니다.

<br>

* D는 기울기를 관찰하면서 중간에 있는 수식을 Maximize하는 방향으로 학습을 진행합니다.

<br>

* 위와 같은 방법으로 D를 k번 반복 학습 합니다.

<br>

* 그 다음에 G(Generator)를 학습합니다. m개의 Sample을 Latent Vector에서 추출합니다.

<br>

* 아래쪽에 수식값을 Minimize하는 방향으로 학습을 진행합니다.

<br>
<br>
<br>

* 이번 Post에서는 GAN Paper를 간단하게 Review해 보았습니다. 

<br>

* 다음 Post에서는 실제 Code를 실행해 보면서 GAN을 알아보도록 하겠습니다.
