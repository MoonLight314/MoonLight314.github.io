---
title: "LoRA(Low-Rank Adaptation)"
date: 2025-03-30 08:26:28 -0400
categories: Deep Learning
---

<br>
<br>

<span style="font-size:15px; line-height: 2.2">
LoRA(Low-Rank Adaptation)는 LLM을 효율적으로 Fine-Tuning하기 위한 방법 중 하나입니다.
<br>
<br>
마이크로소프트 연구팀이 2021년에 발표한 논문 "LoRA: Low-Rank Adaptation of Large Language Models"에서 처음 소개되었습니다.
<br>
<br>
이번 Post에서는 LoRA의 핵심 Idea와 동작 방식, 장점을 자세히 설명해 드리겠습니다.
​</span>

<br>
<br>

## 1. LoRA의 배경

<br>

<span style="font-size:15px; line-height: 2.2">
OpenAI의 GPT, LLaMa 2, Claude 등과 같은 기반 Model 및 LLM은 텍스트 생성부터 언어 이해에 이르기까지 다양한 애플리케이션 분야에서 혁신적인 발전을 이루었습니다. ​
<br>
<br>
이러한 Model들은 방대한 양의 Parameter를 활용하여 탁월한 성능을 보여줍니다.  예를 들어, GPT-3는 1,750억 개의 Parameter를 가지고 있으며, 이러한 대규모 Parameter를 저장하고 관리하는 것은 상당한 메모리 사용량을 요구합니다.​
<br>
<br>
특정 작업이나 Domain에 LLM을 적용하기 위해서는 Fine-tuning이라는 과정을 거쳐야 하는데, 기존의 전체 Parameter Fine-tuning 방식은 모든 Parameter를 학습시켜야 하므로 막대한 컴퓨팅 자원을 필요로 하기 때문에 소규모 연구실이나 개인 연구자들은 꿈도 꾸지 못하는 상황입니다.
<br>
<br>
​</span>

​<br>
<br>
<br>
<br>​

## 2. LoRA의 핵심 Idea

<br>

<span style="font-size:15px; line-height: 2.2">
LoRA는 이러한 문제점을 해결하기 위해 **"Low-Rank Adaptation"**이라는 Idea를 도입했습니다.
<br>
<br>​
LoRA의 핵심 Idea는 **전체 Model을 재학습하는 대신, 원래 Model에 경량화된 작은 조각(Adapter)들을 추가하여 Model을 특정 용도에 맞게 빠르게 학습**시키는 기술입니다. 
<br>
<br>​
LLM을 fine-tuning할 때, 실제로 Model의 모든 Parameter가 동일하게 중요하게 변하는 것은 아닙니다.  즉, 가중치 변화(weight update)는 저차원 부분 공간(low-rank subspace)에 집중되는 경향이 있습니다.
<br>
<br>​
이 부분을 주목하여 LoRA는 원본 Model의 가중치를 고정하고, 각 Layer에 작은 "Adapter" 행렬을 추가합니다.  이 Adapter는 저차원 행렬 분해(low-rank decomposition) 형태로 표현됩니다.
<br>
<br>​
예를 들어, 원본 가중치 행렬이 W(d x k)이고, W에 대한 변화를 ΔW라고 할 때, LoRA는 ΔW를 직접 학습하는 대신, 훨씬 작은 두 행렬 A (d x r)와 B (r x k)의 곱으로 근사합니다.
<br>
<br>​
​</span>
​
<span style="font-size:18px; line-height: 2.2; color:brown; font-weight: bold">
ΔW = AB
<br>
 - W : 원본 Model의 가중치 행렬 (d x k) - d는 입력 차원, k는 출력 차원
<br>
<br>
 - ΔW : Fine-tuning 과정에서 발생하는 가중치 변화량 (d x k)
<br>
<br>
 - A : LoRA에서 학습되는 저차원 행렬 (d x r)
<br>
<br>
 - B : LoRA에서 학습되는 저차원 행렬 (r x k)
<br>
<br>
 - r : LoRA rank (r << d, k), 즉, 저차원 공간의 차원
​</span>
​
<br>
<br>
<br>
<br>​

## 3. LoRA의 동작 방식

<br>

<span style="font-size:15px; line-height: 2.2">
위 그림은 LoRA의 동작 방식을 전체적으로 그린 것입니다.
​</span>

![image](https://github.com/user-attachments/assets/8a388f8a-d44b-450e-8662-f5bfc4c52188)

​
<span style="font-size:15px; line-height: 2.2">
순서대로 하나씩 살펴보도록 하겠습니다.
​</span>
​
<br>

#### 1) 원본 Model Freeze

<span style="font-size:15px; line-height: 2.2">
먼저 기존의 원본 Model의 Parameter들이 Train시에 Update되지 않도록 Freeze시킵니다.
<br>
그리고, 앞서 설명한 Adapter를 원본 Model의 특정 Layer(주로 Transformer 블록의 Attention 메커니즘 부분)에 추가합니다.
<br>
Adapter는 2개의 작은 행렬 A와 B로 구성됩니다.
<br>
앞서 말씀드렸듯이, A는 (d x r) 크기의 행렬, B는 (r x k) 크기의 행렬이며, AB는 (d x k) 크기의 행렬로써, 원본 가중치 W와 동일한 크기를 가지게 됩니다.
​</span>

<br>

#### 2) Input

<span style="font-size:15px; line-height: 2.2">
Train시에는 Input값이 고정된(Frozen) Pre-trained Weights (W)와 Adapter A에 동일하게 입력됩니다.
<br>
이 과정에서 W는 업데이트되지 않습니다.
​</span>
​
<br>

#### 3) Adapter Train

<span style="font-size:15px; line-height: 2.2">
Adapter A를 통과한 값은 다시 Adapter B를 통과하게 됩니다.
<br>
이 과정을 통해 (d x k) 크기의 출력값, 즉 AB 행렬이 생성되며, A와 B는 기존 W와는 다르게 Trainable 한 Parameter로, 학습과정에서 업데이트 됩니다.
<br>
A와 B의 Parameter 수는 r(rank) 값에 의해 결정 됩니다.
​</span>
​
<br>

#### 4) 최종 출력 계산

<span style="font-size:15px; line-height: 2.2">
W를 통과한 출력값과 LoRA Adapter(AB)를 통과한 출력값을 더해서 최종 Output, W x Input + AB x Input 가 만들어지게 됩니다.
</span>
​
<br>

#### 5) Backpropagation

<span style="font-size:15px; line-height: 2.2">
이 Ouptut값을 이용해서 Loss를 계산하고, Backpropagation을 수행하여, Adapter A,B의 Parameter를 Update하게 됩니다.
</span>
​
<br>

#### 6) 이 흐름은 Inference시에도 동일합니다.

​<br>
<br>
<br>

## 4. LoRA vs 기존 Fine-tuning 방식(Fully Connected Layer 교체)

<br>

<span style="font-size:15px; line-height: 2.2">
기존 fine-tuning 방식 중 하나는 pre-trained Model의 마지막 Fully Connected Layer(혹은 Classification Head)를 제거하고, 새로운 Fully Connected Layer를 추가하여 Train하는 방식입니다. 
<br>
<br>
LoRA 방식이 기존 방식 대비 장점은 다음과 같은 것들이 있습니다.
</span>

​<br>

#### 1) Parameter 효율성

<span style="font-size:15px; line-height: 2.2">
**기존 방식** : 마지막 FC Layer를 교체하는 방식은, 마지막 Layer의 Parameter 수가 여전히 많을 수 있습니다. 특히, LLM의 경우 출력 차원(어휘 크기)이 매우 크기 때문에 FC Layer의 Parameter 수도 상당합니다.
<br>
**LoRA** : LoRA는 훨씬 적은 수의 Parameter(A와 B 행렬)만 추가하고 훈련하므로, 메모리와 계산 비용을 크게 절약할 수 있습니다.
</span>
​
<br>

#### 2) 과적합 방지

<span style="font-size:15px; line-height: 2.2">
**기존 방식** : 작은 데이터셋으로 fine-tuning할 때, 마지막 FC Layer만 훈련하더라도 과적합 위험이 있습니다.
<br>
**LoRA** : LoRA는 원본 Model의 가중치를 고정하고, 매우 적은 수의 Parameter만 훈련하므로 과적합 위험을 줄여줍니다.
</span>
​
<br>

#### 3) 모듈성 및 유연성

<span style="font-size:15px; line-height: 2.2">
**기존 방식** : FC Layer 교체 방식은 특정 작업에 특화된 새로운 Model을 생성합니다. 따라서 다른 작업에는 다시 fine-tuning해야 합니다.
<br>
**LoRA** : LoRA는 원본 Model을 변경하지 않고 Adapter만 추가하므로, 여러 작업에 대해 각각 다른 LoRA Adapter를 훈련하고 쉽게 교체할 수 있습니다. 즉, 하나의 pre-trained Model을 여러 작업에 재사용할 수 있습니다.
</span>
​
<br>

#### 4) 원본 Model 지식 보존

<span style="font-size:15px; line-height: 2.2">
**기존 방식** : FC Layer를 교체하는 경우, pre-trained Model이 가지고 있는 마지막 layer의 지식이 사라집니다.
<br>
**LoRA** : LoRA는 원본 Model을 그대로 유지하면서 작은 Adapter만 추가하므로, 원본 Model의 지식을 최대한 활용하면서 새로운 작업에 적응할 수 있습니다.
</span>
​
<br>
<br>
<br>
<br>
​
## 5. LoRA의 장점

<br>

<span style="font-size:15px; line-height: 2.2">
LoRA는 기존의 전체 Fine-tuning 방식에 비해 여러 가지 중요한 장점을 제공합니다.
</span>
​
<br>

#### 1. 속도 향상

<span style="font-size:15px; line-height: 2.2">
학습해야 하는 Parameter 수가 훨씬 적기 때문에 전체 Fine-tuning에 비해 학습 속도가 크게 향상됩니다 . 
<br>
또한, 훈련 가능한 Parameter만 저장하면 되므로 GPU 메모리 사용량이 대폭 감소하여, 더 작은 GPU에서도 대규모 Model의 Fine-tuning이 가능해집니다 . 
<br>
예를 들어, GPT-3의 경우 LoRA를 사용하면 GPU 메모리 요구량을 약 2/3까지 줄일 수 있습니다.
</span>
​
<br>

#### 2. 저장 공간 효율성

<span style="font-size:15px; line-height: 2.2">
Fine-tuning된 Model의 Checkpoint 크기가 전체 Model을 저장하는 것에 비해 훨씬 작아져 저장 공간 효율성이 높아집니다.
<br>
GPT-3의 체크포인트 크기가 LoRA를 통해 1TB에서 25MB로 감소한 사례는 이를 잘 보여줍니다.
</span>
​
<br>

#### 3. GPU 자원 요구량 감소

<span style="font-size:15px; line-height: 2.2">
GPU 자원 요구량 감소는 LLM Fine-tuning에 대한 접근성을 크게 향상시킵니다. 
<br>
높은 사양의 GPU가 없어도 LLM Fine-tuning을 시도해 볼 수 있게 되어, 소규모 연구실이나 개인 연구자들도 최첨단 Model을 활용할 수 있는 기회가 확대됩니다.
</span>
​
<br>

#### 4. 추론 시간 지연 발생없음

<span style="font-size:15px; line-height: 2.2">
추론 시 추가적인 지연 시간이 발생하지 않습니다. 
<br>
학습된 Adapter 행렬을 원래 Model에 병합하여 사용하거나, 병합하지 않고도 효율적인 계산이 가능하여 실시간 서비스 등에서도 LoRA를 적용하는 데 무리가 없습니다.
</span>
​
<br>

#### 5. 유연성

<span style="font-size:15px; line-height: 2.2">
LoRA는 다양한 작업에 대한 빠른 적응 및 유연성을 제공합니다. 
<br>
작업별로 작은 크기의 LoRA 모듈만 학습하면 되므로, 하나의 기반 Model을 다양한 특정 작업에 빠르게 적응시킬 수 있습니다. 
<br>
여러 개의 LoRA 모듈을 손쉽게 교체하여 다양한 작업을 수행할 수 있는 유연성을 제공하며 , 텍스트, 대화, 이미지 생성 등 다양한 애플리케이션에 적용 가능합니다. 
<br>
또한, LoRA는 Prefix Tuning이나 Adapters와 같은 다른 Fine-tuning 방법과 결합하여 특정 목표를 달성할 수도 있습니다.
</span>
​
<br>
<br>
​

기존 Fine-tuning과 LoRA의 비교

<style>
  table {
    border-collapse: collapse;
    width: 100%;
    table-layout: auto;
    font-size: 16px; /* 이 줄이 글자 크기 설정입니다 */
  }
  th, td {
    border: 1px solid #ccc;
    padding: 8px 12px;
    text-align: center;
    vertical-align: middle;
  }
  th {
    background-color: #f2f2f2;
  }
</style>

<table>
  <thead>
    <tr>
      <th>특징</th>
      <th>기존 Fine-tuning (Full Fine-tuning)</th>
      <th>LoRA (Low-Rank Adaptation)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>학습 Parameter 수</td>
      <td>전체 Model Parameter 수</td>
      <td>훨씬 적음 (저랭크 행렬)</td>
    </tr>
    <tr>
      <td>메모리 요구량</td>
      <td>매우 높음</td>
      <td>훨씬 낮음</td>
    </tr>
    <tr>
      <td>학습 속도</td>
      <td>느림</td>
      <td>빠름</td>
    </tr>
    <tr>
      <td>Model 크기</td>
      <td>매우 큼</td>
      <td>작음 (추가 모듈)</td>
    </tr>
    <tr>
      <td>추론 지연 시간</td>
      <td>없음</td>
      <td>없음</td>
    </tr>
    <tr>
      <td>자원 요구량</td>
      <td>높음</td>
      <td>낮음</td>
    </tr>
  </tbody>
</table>


<br>
<br>
<br>
<br>​

## 6. QLoRA

<br>

<span style="font-size:15px; line-height: 2.2">
QLoRA(Quantized LoRA)는 기존 LoRA를 더욱 발전시켜 효율성을 극대화한 fine-tuning 기법이며, QLoRA의 핵심은 4비트 양자화와 Double Quantization, 이 2가지입니다.
</span>
​
<br>
​
### 6.1. 4비트 양자화 / Double Quantization

<br>

#### 1) 4비트 NormalFloat (NF4) 양자화

<br>

<span style="font-size:15px; line-height: 2.2">
QLoRA의 가장 큰 특징은 사전 훈련된 Model의 가중치를 4비트 NormalFloat(NF4)라는 새로운 데이터 타입으로 양자화한다는 것입니다.
<br>
<br>
일반적으로 Model 가중치는 32비트 부동 소수점(float32) 또는 16비트 부동 소수점(float16/bfloat16)으로 표현됩니다.
<br>
<br>
4비트 양자화는 가중치를 4비트라는 매우 작은 단위로 표현하여 메모리 사용량을 획기적으로 줄입니다. (최대 8배 감소)
<br>
<br>
NF4는 정보 이론적으로 최적의 4비트 데이터 타입으로, 일반적인 4비트 정수형보다 더 나은 성능을 보입니다.
</span>
​
<br>

#### 2) Double Quantization

<br>

<span style="font-size:15px; line-height: 2.2">
QLoRA는 양자화 상수(quantization constants)에도 추가적인 양자화를 적용하는 "Double Quantization"을 사용합니다.
<br>
<br>
양자화 상수는 양자화 과정에서 사용되는 값들인데, 이 값들 자체도 메모리를 차지합니다.
<br>
<br>
Double Quantization은 이러한 상수들까지도 양자화하여 메모리 사용량을 더욱 줄입니다.
</span>
​​
<br>

### 6.2. LoRA vs. QLoRA

<br>

<table border="1" cellspacing="0" cellpadding="8">
  <thead>
    <tr>
      <th>특징</th>
      <th>LoRA</th>
      <th>QLoRA</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>기본 Idea</td>
      <td>저차원 Adapter(A, B) 추가</td>
      <td>4비트 양자화 + 저차원 Adapter(A, B) 추가</td>
    </tr>
    <tr>
      <td>Model 가중치</td>
      <td>원본 Model 가중치 고정</td>
      <td>원본 Model 가중치 4비트 NF4로 양자화</td>
    </tr>
    <tr>
      <td>추가 Parameter</td>
      <td>LoRA Adapter (A, B)</td>
      <td>LoRA Adapter (A, B) + 양자화 상수</td>
    </tr>
    <tr>
      <td>메모리 절약</td>
      <td>훈련 Parameter 감소</td>
      <td>훈련 Parameter 감소 + 가중치 양자화 + Double Quantization</td>
    </tr>
    <tr>
      <td>Fine-tuning 속도</td>
      <td>빠름</td>
      <td>LoRA보다 약간 느릴 수 있음 (4비트 연산)</td>
    </tr>
    <tr>
      <td>성능</td>
      <td>Full fine-tuning과 유사하거나 약간 낮음</td>
      <td>Full fine-tuning과 거의 동등한 성능</td>
    </tr>
    <tr>
      <td>하드웨어 요구 사항</td>
      <td>일반적인 GPU</td>
      <td>4비트 양자화를 지원하는 GPU (NVIDIA)</td>
    </tr>
  </tbody>
</table>

​<br>

### 6.3. QLoRA의 장점

<br>

<span style="font-size:15px; line-height: 2.2">
앞서 보셨듯이, QLoRA는 4비트 양자화와 Double Quantization을 통해 LoRA보다 훨씬 더 적은 메모리로 fine-tuning이 가능하며, 게다가 거의 동등한 성능을 유지합니다.
<br>
<br>
이러한 장점으로 인해 고가의 GPU 없이도 대규모 LLM을 fine-tuning할 수 있어, 더 많은 사람이 LLM 기술을 활용할 수 있게 합니다.
</span>
​
<br>​

### 6.4. 주의사항

<br>

#### 1) 4비트 양자화는 NVIDIA의 특정 하드웨어 기능을 활용하기 때문에 특정 HW에서만 실행가능합니다.

<br>

<span style="font-size:15px; line-height: 2.2">
QLoRA의 4비트 양자화는 NVIDIA의 "bitsandbytes" 라이브러리를 통해 구현되며, 이 라이브러리는 CUDA를 사용합니다. 
<br>
<br>
좀 더 구체적으로, QLoRA의 핵심 기능인 bnb.nn.Linear4bit Layer는 다음 조건을 만족하는 NVIDIA GPU에서 사용할 수 있습니다.
<br>
<br>​

· Compute Capability (SM) 7.5 이상: 튜링(Turing) 아키텍처(RTX 20 시리즈, T4 등) 이상
<br>
<br>
튜링(Turing): RTX 20 시리즈, Tesla T4, Quadro RTX 시리즈 등
<br>
<br>
암페어(Ampere): RTX 30 시리즈, A100, A30 등
<br>
<br>
에이다 러브레이스(Ada Lovelace): RTX 40 시리즈
<br>
<br>
호퍼(Hopper): H100 등
<br>
<br>
· CUDA 11.0 이상: QLoRA는 CUDA 11.0 이상의 환경이 필요합니다. (최신 버전 권장)

· PyTorch 1.12.0 이상: (최신 버전 권장)

​<br>
<br>

다음 GPU들에서는 동작하지 않습니다
<br>
<br>
· 맥스웰(Maxwell), 파스칼(Pascal)과 같이 Compute capability가 7.5 미만인 경우.
</span>
​
<br>

### 2) NF4 양자화 시간

<br>

<span style="font-size:15px; line-height: 2.2">
QLoRA는 fine-tuning 시 메모리 사용량을 줄이기 위해 Model 가중치를 4비트로 양자화하지만, 이 양자화는 로드 시점에 실시간으로 이루어집니다. 
<br>
<br>
즉, Model 가중치를 미리 NF4 형식으로 변환해 놓는 것이 아니라, Model을 메모리에 로드할 때 32비트(float32) 또는 16비트(float16/bfloat16) 가중치를 즉석에서 4비트 NF4로 변환하는 방식입니다.
<br>
<br>
따라서, 수십 기가바이트 크기의 Model 전체를 NF4로 변환하는 데 시간이 오래 걸리는 것은 아닙니다. 
<br>
<br>
Model 로드 시점에 일부 추가적인 연산이 필요하지만, 이로 인한 지연 시간은 크지 않은 편입니다. 
<br>
<br>
오히려 메모리 사용량이 크게 줄어들기 때문에, 전체적인 fine-tuning 시간은 단축될 수 있습니다.
</span>


### 참조

<br>

· [QLoRA - Efficient Finetuning of Quantized LLMs ](https://arxiv.org/abs/2305.14314)

· [bitsandbytes GitHub](https://github.com/TimDettmers/bitsandbytes)

· [Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA  ](https://huggingface.co/blog/4bit-transformers-bitsandbytes)

· [NVIDIA Developer Blog (Compute Capability) CUDA GPUs  NVIDIA Developer  ](https://developer.nvidia.com/cuda-gpus)

· [IBM Think. What is LoRA? ](https://www.ibm.com/think/topics/lora)

· [Cloudflare Learning. What is LoRA? ](https://www.cloudflare.com/learning/ai/what-is-lora/)

· [Coralogix AI Blog. Low-Rank Adaptation: A Closer Look at LoRA. ](https://coralogix.com/ai-blog/low-rank-adaptation-a-closer-look-at-lora/)

· [Medium article: LoRA Explained: Low-Rank Adaptation for Fine-Tuning LLMs. ](https://medium.com/@adimodi96/low-rank-adaptation-lora-explained-9e64b7b0a5f1)

· [Medium article: Low-Rank Adaptation of Large Language Models. ](https://medium.com/@tubelwj/major-negative-impacts-of-fine-tuning-large-language-models-a70ae187410a)

· [Medium article: The Intuitive Idea Behind Low-Rank Adaption (LoRA). ](https://medium.com/@ogbanugot/the-intuitive-idea-behind-low-rank-adaption-lora-cc4c5bdee90a)

· [Medium article: Low-Rank Adaptation (LoRA) Explained. ](https://magazine.sebastianraschka.com/p/lora-low-rank-adaptation-explained)

· [Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen. (2021). LoRA: Low-Rank Adaptation of Large Language Models. ](https://arxiv.org/abs/2106.09685)

· [Medium article: LoRA Explained: Low-Rank Adaptation for Fine-Tuning LLMs.](https://medium.com/@zilliz_learn/lora-explained-low-rank-adaptation-for-fine-tuning-llms-066c9bdd0b32)

· [Nexla. Low-rank adaptation of large language models explained. ](https://nexla.com/enterprise-ai/low-rank-adaptation-of-large-language-models/)

· [Medium article: Low-Rank Adaptation (LoRA) Explained. ](https://blog.ml6.eu/low-rank-adaptation-a-technical-deep-dive-782dec995772)

· [IBM Think. What is LoRA? ](https://www.ibm.com/think/topics/lora)

· [Medium article: Mastering Low-Rank Adaptation (LoRA): Enhancing Large Language Models for Efficient Adaptation. ](https://www.datacamp.com/tutorial/mastering-low-rank-adaptation-lora-enhancing-large-language-models-for-efficient-adaptation)

· [Medium article: Data Compression with Low-Rank Approximation Using Neural Networks. ](https://medium.com/@weidagang/data-compression-with-low-rank-approximation-using-neural-networks-d6a8e8426101)
