---
title: "LSTM(Long-Short Term Memory)"
date: 2020-04-07 08:26:28 -0400
categories: DeepLearning LSTM
---
# LSTM(Long-Short Term Memory)
<br>
<br>
<br>

* 이 Post는 아래 Link의 글을 참고하였습니다.

  - [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
  
  - [LONG SHORT-TERM MEMORY](http://www.bioinf.jku.at/publications/older/2604.pdf)

<br>
<br>

* LSTM은 RNN의 특별한 한 종류로써, 긴 의존기간이 필요한 학습을 할 수 있는 능력이 있습니다.

* LSTM의 목적은 명확하게 Long-Term Dependency를 제거하고자 Design되었습니다.

<br>
<br>
<br>
<br>
<br>
<br>

### 0. LSTM의 기본 구조   

<br>
<br>
<br>

<p align="center">
  <img src="/assets/LSTM_Doc_Img/pic_00.png">
</p>
   
<br>
<br>
<br>
   

* 위의 구조는 tanh를 Activation Function으로 가지는 RNN의 기본적인 구조입니다. 


* 아래의 그림은 LSTM의 기본 Cell 구조를 나타내고 있습니다.
<br>
<br>
<br>

<p align="center">
  <img src="/assets/LSTM_Doc_Img/pic_01.png">
</p>
   
<br>
<br>
<br>

* LSTM도 기본적으로 RNN과 유사한 구조를 가지지만, 몇 개의 Layer가 추가되었습니다.

<br>
<br>
<br>
<br>
<br>


   

* 본격적으로 하나씩 살펴보기 전에 기호들의 정의를 살펴보도록 하겠습니다.

<p align="center">
  <img src="/assets/LSTM_Doc_Img/pic_02.png">
</p>

<br>
<br>
<br>
<br>
<br>
<br>

### 1. Cell State
<br>
<br>
<br>
   
<p align="center">
  <img src="/assets/LSTM_Doc_Img/pic_03.png">
</p>
   

* Cell State는 LSTM의 중요 개념이며, 위의 그림에서 위쪽에 위치한 가로로 뻗은 선을 가리킵니다.

* **작은 변화만을 가지면서 가진 정보를 거의 그대로 전달한다는 것을 알 수 있습니다.**

* **LSTM은 Gate라는 구조를 통해 Cell State를 제어합니다.**

<br>
<br>
<br>
<br>

<p align="center">
  <img src="/assets/LSTM_Doc_Img/pic_04.png">
</p>

<br>
<br>
<br>

* LSTM은 총 3개의 Gate를 가지고 있고, **모두 활성화 함수로 Sigmoid를 사용합니다.**

* Sigmoid는 0~1 사이의 값을 출력하고, **이 값을 조절함으로써 Cell State를 제어합니다.**

<br>
<br>
<br>

### 2. Forget Gate Layer
<br>
<br>
<br>

* Forget Gate Layer는 cell state에서 어떤 정보를 버릴 것인지를 sigmoid layer에 의해 결정합니다.

* 이 단계에서는 h<sub>t−1</sub>과 x<sub>t</sub>를 받아서 0과 1 사이의 값을 C<sub>t−1</sub>에 보내줍니다.

* **그 값이 1이면 "모든 정보를 보존해라"가 되고, 0이면 ＂모두 갖다버려라＂가 됩니다.**

<br>
<br>
<br>

<p align="center">
  <img src="/assets/LSTM_Doc_Img/pic_05.png">
</p>

<br>
<br>
<br>   

* Language Model을 예로 들자면, Cell State에는 성별 정보를 가지고 있을 수 있어서 다음 단어의 성별에 맞는 대명사를 준비하고 있을 수도 있습니다.

* 하지만, 새로운 주어가 왔다면 기존 주어의 성별 정보를 필요가 없어지게 되겠죠.    


<br>
<br>
<br>
<br>
<br>

### 3. Input Gate Layer
<br>
<br>
<br>

* 다음에 살펴볼 Gate는 Input Gate로써, 새로운 정보 중에 어떤 것을 Cell State에 저장할 것인지 정합니다.

* 이전 Cell에서 넘어온 h<sub>t-1</sub>과 이번 Cell의 입력인 x<sub>t</sub>를 tanh를 통과시켜 새로운 C<sub>~t</sub>를 만들고, 동일한 값을 Sigmoid를 통과시켜 어느 정도를 Cell State에 추가할지 정합니다.

* Tanh와 Sigmoid를 통과한 값을 Cell State에 추가합니다.

<br>
<br>
<br>

<p align="center">
  <img src="/assets/LSTM_Doc_Img/pic_06.png">
</p>

<br>
<br>
<br>   

* Language Model에서 기존 성별을 잊어버리고, 대신 새로운 주어의 성별 정보를 Cell State에 더할 수 있는 것입니다.   

<br>
<br>
<br>
<br>

### 4. Cell State Update

<br>
<br>
<br>

* 이전 Cell State 값인 C<sub>t-1</sub>을 Update해서 새로운 C<sub>t</sub>를 만듭니다. 

* **우선 f<sub>t</sub>값을 곱해서 C<sub>t-1</sub>을 조절(잊어버리기)합니다. 그리고, 새롭게 추가되어할 값(Input Gate Layer)을 더해주어 새로운 정보를 추가하는 역할을 합니다.**

* 이 Layer는 이전 Gate에서 만들어진 값들을 단순히 계산하여 Cell State를 Update하는 역할을 합니다.

<br>
<br>
<br>

<p align="center">
  <img src="/assets/LSTM_Doc_Img/pic_07.png">
</p>

<br>
<br>
<br>
<br>
<br>
<br>

### 5. Output Gate Layer

<br>
<br>
<br>

* 마지막으로 출력을 어떻게 내보낼지 결정하는 Output Gate입니다.

* 그림에서 알 수 있듯이, **이전 Cell의 출력값 h<sub>t-1</sub>과 현재 Cell의 입력값 x<sub>t</sub>를 Sigmoid한 다음 새로운 Cell State ct값을 tanh를 통과시켜 -1 ~ 1사이의 값으로 받은 다음에 Sigmoid 값과 곱해서 출력으로 뽑아냅니다.**

* Language Model에서 생각해보면, 출력이 동사가 된다면 앞에서 본 주어가 단수인지 복수인지에 따라 그 형태가 달라질 것입니다.

<br>
<br>
<br>

<p align="center">
  <img src="/assets/LSTM_Doc_Img/pic_08.png">
</p>

<br>
<br>
<br>
