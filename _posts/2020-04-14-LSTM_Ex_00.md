---
title: "LSTM Exercise #00 - Alphabet Prediction"
date: 2020-04-14 08:26:28 -0400
categories: DeepLearning LSTM
---
# LSTM Exercise #00 - Alphabet Prediction
<br>
<br>
<br>

* 앞으로 몇번의 Post를 거쳐서, LSTM을 이용한 다양한 예제들을 다뤄볼 예정입니다.
* LSTM은 Input Data Foramt을 어떻게 구성하느냐에 따라서 성능의 차이도 많이 발생합니다.
* 저는 **Data의 Input Format에 따라서 또는 Train 형태에 따라서 LSTM API Parameter를 어떻게 구성하고 어떻게 Train 시키는지가 가장 어려웠습니다.**
* 매우 다양한 예제가 있고, 다양한 형태로 Input Format을 구성하지만 일목요연하게 설명된 자료를 찾기는 힘들었습니다.
* 제가 여러 예제들을 살펴보면서 다양한 입력 형태들을 확인한 결과를 몇 번의 Post에 나누어서 설명해 보고자 합니다.

<br>
<br>
<br>
<br>
<br>
<br>


## Alphabet Prediction   

* RNN & LSTM은 다양한 예측 문제에 사용될 수 있고, Simple한 예제부터 시작해 보도록 하겠습니다.
* 이번 예제에서 하고자 하는 것은 주어진 하나의 Alphabet 혹은 연속된 Alphabet Sequence 다음에 올 Alphabet을 예측하는 Model을 만드는 것입니다.

<br>
<br>
<br>

### 0. 준비   

* 필요한 Package들을 Load합니다.
* 우리는 Keras & Tensorflow를 사용할 예정입니다.


```python
import numpy
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.utils import np_utils
```


```python
import matplotlib
from IPython.display import set_matplotlib_formats
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"

%config IPCompleter.greedy=True

from keras.preprocessing.sequence import pad_sequences
```

* Random Seed를 고정합니다.


```python
numpy.random.seed(7)
```

<br>
<br>
<br>
<br>
<br>
  

* 우리가 학습 및 예측에 사용할 Alphabet 문자입니다.
* Utility 자료구조도 하나 정의합니다. 
  - Tensorflow에 Train Data를 입력할 때는 반드시 숫자이어야 합니다.
  - 그러나, Alphabet은 문자이기 때문에 반드시 변환이 필요하며, 문자 <-> 숫자로 상호 변화할 수 있는 Dictionary를 정의합니다.
  - 문자열을 다루는 문제에서는 대부분 이런 자료구조를 이용합니다.


```python
# 학습할 Data
alphabet = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"

# 문자 <-> 숫자 상호 변환 Util
char_to_int = dict((c, i) for i, c in enumerate(alphabet))
int_to_char = dict((i, c) for i, c in enumerate(alphabet))
```
* 사용법은 아래와 같이 사용하시면 됩니다.   


```python
char_to_int['C']
int_to_char[7]
```


    2
    'H'

<br>
<br>
<br>

* 기본적인 Train Data 생성 함수를 살펴보도록 하겠습니다.
* 아래 함수에서 seq_length 라는 변수가 중요한 변수로 사용됩니다.
* seq_length라는 값은 LSTM이 **시계열 Data 혹은 Sequence Data에서 Train시에 과거 몇 개의 Data를 Train에 사용할 것인지를 정하는 아주 아주 중요한 값입니다.**
* **Time Steps** 라는 이름으로도 문서상에서 많이 불려집니다.
* 이 함수가 하는 일을 표현하면 아래와 같습니다.


<p align="center">
  <img src="/assets/LSTM_Ex_00_Alphabet/pic_00.png">
</p>

* seq_length 값에 따라서 Train에 사용할 값을 dataX List에 추가하고, 같은 Index에 그에 해당하는 Label(Target)값을 dataY에 저장해 줍니다.
  - dataX : Train Data. 예측할 문자 전에 나오는 문자(열)
  - dataY : Label Data. 예측해야 할 문자

<br>
<br>
<br>

* **dataX는 배열의 요소가 배열인 형태로 구성됩니다.**

<br>
<br>
<br>

* 실제 Data 출력 Format을 보도록 하겠습니다.   


```python
# prepare the dataset of input to output pairs encoded as integers
seq_length = 2
dataX = []
dataY = []

for i in range(0, len(alphabet) - seq_length, 1):
    seq_in = alphabet[i:i + seq_length]
    seq_out = alphabet[i + seq_length]
    dataX.append([char_to_int[char] for char in seq_in])
    dataY.append(char_to_int[seq_out])
    print(seq_in, '->', seq_out)
```

    AB -> C
    BC -> D
    CD -> E
    DE -> F
    EF -> G
    FG -> H
    GH -> I
    HI -> J
    IJ -> K
    JK -> L
    KL -> M
    LM -> N
    MN -> O
    NO -> P
    OP -> Q
    PQ -> R
    QR -> S
    RS -> T
    ST -> U
    TU -> V
    UV -> W
    VW -> X
    WX -> Y
    XY -> Z
    
    
<br>
<br>
<br>
<br>
<br>
<br>

* 실제로 Size와 어떤 Data가 있는지 확인해 보겠습니다.


```python
print("dataX Size : " , len( dataX ) , ",  dataY Size : ", len(dataY) )
print("dataX[0] : ", dataX[0] , "  dataY[0] : " , dataY[0])
```

    dataX Size :  24 ,  dataY Size :  24
    dataX[0] :  [0, 1]   dataY[0] :  2
    
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>

### 1. LSTM의 Train Data Input Shape   

* 제가 LSTM을 공부하면서 가장 혼란스러웠던 부분이 바로 LSTM의 Train Data Input Shape이었습니다.
* 우선 LSTM의 Train Data Input Shape은 반드시 3차원 Array 형태이어야 합니다.
* Keras LSTM API는 input_shape이라는 Parameter를 통해서 Train Data Input Shape을 정합니다.
* 그러나 많은 예제에서 input_shape은 2개의 Parameter만 입력합니다.( 반드시 3차원 Array 형태라고 했는데…? )
* 먼저 input_shape의 2개 Parameter는 각각 순서대로 (time_steps , number_of_features)입니다.
* time_steps는 위에서 알아봤던 내용으로, LSTM에 과거의 Data를 얼마나 많이 입력해 줄 것인가를 정해줍니다.
* number_of_features는 하나의 Data에 얼마나 많은 Feature가 있는지를 정해줍니다.

* 예를 들어 설명해 보겠습니다.

<p align="center">
  <img src="/assets/LSTM_Ex_00_Alphabet/pic_01.png">
</p>

<br>
<br>
<br>

* 위의 그림에서 각 색깔은 하나의 Data로써, time_steps는 5이고, 개별 Data를 이루는 Feature의 수는 8이 됩니다.

* 어떤 Data가 주어지면 Feature의 수는 고정값이 될 것이고, time_steps는 Model의 성능이 가장 좋아지는 적절한 값으로 정해주면 될 것입니다. 
* 그리고 언급하지 않은, 혼란을 야기한, 나머지 하나의 값은 number of samples이라는 Parameter로 전체 Data의 개수를 뜻하며 1개 이상이라고 가정합니다.
* 즉, 실제 fit시에 입력으로 들어오는 Train Data의 Shape을 보고 적절한 값이 자동으로 채워지는 것입니다.

<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>

* 위에서 설명드린 바와 같이, Input Data Shape을 3차원 Array로 변환합니다.
* 변환은 Numpy Reshape을 쓰시면 됩니다.


```python
# [number_samples, time_steps, features] 순으로 적어줍니다.
# 우리가 다루는 Train Data는 Alhpabet 문자 하나가 Feature이기 때문에 Feature수가 1입니다.
X = numpy.reshape(dataX, (len(dataX), seq_length, 1))
```

<br>
<br>
<br>

* 만들어진 실제 Data값과 형태를 살펴보도록 하죠   

```python
X.shape
X
```
    (24, 2, 1)

    array([[[ 0],
            [ 1]],
    
           [[ 1],
            [ 2]],
    
           [[ 2],
            [ 3]],
    
           [[ 3],
            [ 4]],
    
           [[ 4],
            [ 5]],
    
           [[ 5],
            [ 6]],
    
           [[ 6],
            [ 7]],
    
           [[ 7],
            [ 8]],
    
           [[ 8],
            [ 9]],
    
           [[ 9],
            [10]],
    
           [[10],
            [11]],
    
           [[11],
            [12]],
    
           [[12],
            [13]],
    
           [[13],
            [14]],
    
           [[14],
            [15]],
    
           [[15],
            [16]],
    
           [[16],
            [17]],
    
           [[17],
            [18]],
    
           [[18],
            [19]],
    
           [[19],
            [20]],
    
           [[20],
            [21]],
    
           [[21],
            [22]],
    
           [[22],
            [23]],
    
           [[23],
            [24]]])

<br>
<br>
<br>

* 실제 Alhpabet 값이 해당하는 숫자로 변환되어 3차원 Array로 저장되어 있는 것을 알 수 있습니다.   

<br>
<br>
<br>

* 마지막으로 Normalize 과정을 거칩니다.
* Tree Based Algorithm이 아닌, Neural Net과 같은 Algorithm은 Data를 Normalize해 주어야 좋은 성능이 나옵니다.


```python
# normalize
X = X / float(len(alphabet))
X
```

    array([[[0.        ],
            [0.03846154]],
    
           [[0.03846154],
            [0.07692308]],
    
           [[0.07692308],
            [0.11538462]],
    
           [[0.11538462],
            [0.15384615]],
    
           [[0.15384615],
            [0.19230769]],
    
           [[0.19230769],
            [0.23076923]],
    
           [[0.23076923],
            [0.26923077]],
    
           [[0.26923077],
            [0.30769231]],
    
           [[0.30769231],
            [0.34615385]],
    
           [[0.34615385],
            [0.38461538]],
    
           [[0.38461538],
            [0.42307692]],
    
           [[0.42307692],
            [0.46153846]],
    
           [[0.46153846],
            [0.5       ]],
    
           [[0.5       ],
            [0.53846154]],
    
           [[0.53846154],
            [0.57692308]],
    
           [[0.57692308],
            [0.61538462]],
    
           [[0.61538462],
            [0.65384615]],
    
           [[0.65384615],
            [0.69230769]],
    
           [[0.69230769],
            [0.73076923]],
    
           [[0.73076923],
            [0.76923077]],
    
           [[0.76923077],
            [0.80769231]],
    
           [[0.80769231],
            [0.84615385]],
    
           [[0.84615385],
            [0.88461538]],
    
           [[0.88461538],
            [0.92307692]]])
            
<br>
<br>
<br>
<br>
<br>
<br>

* 이제 Target도 유사한 방식으로 준비를 합니다.
* 다른 점이 있다면, 우리가 예측하려는 것이 26개의 문자(Class)를 분류하는 Categorical Classification 문제이기 때문에 Target값을 One-Hot Encoding을 해준다는 것입니다.


```python
dataY

# one hot encode the output variable
y = np_utils.to_categorical(dataY)

y
```

    [1,
     2,
     3,
     4,
     5,
     6,
     7,
     8,
     9,
     10,
     11,
     12,
     13,
     14,
     15,
     16,
     17,
     18,
     19,
     20,
     21,
     22,
     23,
     24,
     25]


    array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
           [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
           [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
            0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
            0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
            0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
            0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
            0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
            0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
            0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
            0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],
           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],
           [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
            0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)
            
<br>
<br>
<br>

* 자, 이제 LSTM을 Train 시킬 기본적인 준비를 마쳤으니, 각 Case별로 알아보도록 하겠습니다.    


<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>

