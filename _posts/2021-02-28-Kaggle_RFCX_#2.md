---
title: "Kaggle Competition - Rainforest Connection Species Audio Detection #2"
date: 2021-02-28 08:26:28 -0400
categories: Kaggle
---
### Kaggle Competition - Rainforest Connection Species Audio Detection - Revision #02

<br>
<br>
<br>
<br>

* ì´ë²ˆ Postì—ì„œëŠ” ì§€ë‚œ ë²ˆì— ì˜¬ë ¸ì—ˆë˜ Kaggle - Rainforest Connection Species Audio Detection Compepition 2ë²ˆì§¸ Postì…ë‹ˆë‹¤.

<br>
<br>
<br>

### 0. ë³€ê²½ ë‚´ìš©

* ì´ë²ˆì—ëŠ” ì§€ë‚œ Postì—ì„œ ë‹¤ë£¨ì—ˆë˜ ë°©ë²•ê³¼ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ë„ì „í•´ ë³´ê³ ì í•©ë‹ˆë‹¤.


* ì§€ë‚œë²ˆì— ì‚¬ìš©í–ˆë˜ Featureì¸ MEL Spectogramê³¼ MFCC ì¤‘ì—ì„œ **MEL Spec.** ë§Œ ì‚¬ìš©í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.


* ì§€ë‚œ ë²ˆì—ëŠ” LightGBMì„ ì‚¬ìš©í–ˆì—ˆì§€ë§Œ, ì´ë²ˆì—ëŠ” MEL Spec.ì„ ì´ìš©í•´ì„œ **CNN**ì„ ì ìš©í•´ ë³´ê³ ì í•©ë‹ˆë‹¤.


* ì´ë²ˆ Competitionì—ì„œ ì œê³µë˜ëŠ” Datasetì€ 1216ê°œ ì´ê³ , ë¶„ë¥˜í•´ì•¼í•  ClassëŠ” ì´ 24ê°œì…ë‹ˆë‹¤.


* Datasetì´ Deep Learningì„ ëŒë¦¬ê¸°ì—” ë„ˆë¬´ë‚˜ ë¶€ì¡±í•œ ìˆ˜ì…ë‹ˆë‹¤.


* ì´ëŸ° ìƒí™©ì—ì„œëŠ” Overfittingì´ ë°œìƒí•  í™•ë¥ ì´ ë§¤ìš° ë†’ê¸° ë•Œë¬¸ì— Data Augmentationì„ ìˆ˜í–‰í•˜ê¸°ë¡œ í•˜ê² ìŠµë‹ˆë‹¤.


* CNNì„ ì‚¬ìš©í•˜ê¸°ë¡œ ê²°ì •í–ˆê³ , ìƒˆë¡­ê²Œ CNN Layer êµ¬ì¡°ë¥¼ ë§Œë“¤ê¸° ë³´ë‹¤ëŠ” ê¸°ì¡´ì— ìˆëŠ” í›Œë¥­í•œ Modelì„ ê°€ì ¸ì™€ì„œ ì‚¬ìš©í•˜ê¸°ë¡œ í•˜ê² ìŠµë‹ˆë‹¤.


* ì €ëŠ” ì´ë²ˆì— **ResNet50**ì„ ì‚¬ìš©í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.


* Data Augmentationê³¼ ResNetì„ ì‹¤ì‹œê°„ìœ¼ë¡œ Trainingì— ì‚¬ìš©í•˜ê¸°ì—” Memory ì œí•œë•Œë¬¸ì— ì´ì „ì— ë‹¤ë£¨ì—ˆë˜ **Custom Generator**ë¥¼ ì ìš©í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

<br>
<br>
<br>
<br>
<br>
<br>

### 1. Augmentation for Sound Data 

* ImageDataGeneratorì—ëŠ” ë‹¤ì–‘í•œ Augmentation ê¸°ë²•ë“¤ì´ ë§ˆë ¨ë˜ì–´ ìˆì§€ë§Œ, ì‚¬ì‹¤ ì´ ê¸°ë²•ë“¤ì€ ëª¨ë‘ Imageë“¤ì— ëŒ€í•œ Augmentation ê¸°ë²•ë“¤ì…ë‹ˆë‹¤.


* ì§€ê¸ˆ ìš°ë¦¬ê°€ ë‹¤ë£¨ëŠ” Datasetì€ CNNì„ ì´ìš©í•˜ëŠ”ê²ƒì€ ë§ì§€ë§Œ, Imageê°€ ì•„ë‹Œ Sound Dataì…ë‹ˆë‹¤.


* ê·¸ë˜ì„œ, Imageì— ì ìš©í•  ìˆ˜ ìˆëŠ” Augmentation ê¸°ë²•ë“¤ì´ ì•„ë‹Œ Soundì— ì ìš©ê°€ëŠ¥í•œ Augmentation ê¸°ë²•ë“¤ì´ í•„ìš”í•©ë‹ˆë‹¤.


* Sound Data Augmentationì— ì ìš©ê°€ëŠ¥í•œ ë‹¤ì–‘í•œ ê¸°ë²•ë“¤ì´ ì†Œê°œëœ ì¢‹ì€ ê¸€ì´ ìˆì–´ ì†Œê°œë“œë¦½ë‹ˆë‹¤.

  [RFCX: Audio Data Augmentation(Japanese+English)](https://www.kaggle.com/hidehisaarai1213/rfcx-audio-data-augmentation-japanese-english)

* ì €ë„ ì´ë²ˆ Datasetì—ëŠ” ìœ„ì—ì„œ ì†Œê°œí•œ ë‹¤ì–‘í•œ ê¸°ë²•ë“¤ì„ ì ìš©í•  ì˜ˆì •ì…ë‹ˆë‹¤.   


<br>
<br>
<br>

* Sound Data Augmentationì„ ìœ„í•´ì„œ 'colorednoise'ë¼ëŠ” Packageë¥¼ ì‚¬ìš©í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

* ìì„¸í•œ ì†Œê°œì™€ ì‚¬ìš©ë²•ì€ ì•„ë˜ Linkë¥¼ ì°¸ì¡°í•´ ì£¼ì„¸ìš”

  [colorednoise Github](https://github.com/felixpatzelt/colorednoise)
  
  [Package â€˜colorednoiseâ€™](https://cran.r-project.org/web/packages/colorednoise/colorednoise.pdf)

* colorednoise Packageë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.   


```python
!pip install colorednoise 
```

    Collecting colorednoise
      Downloading colorednoise-1.1.1.tar.gz (5.2 kB)
    Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from colorednoise) (1.19.5)
    Building wheels for collected packages: colorednoise
      Building wheel for colorednoise (setup.py) ... [?25ldone
    [?25h  Created wheel for colorednoise: filename=colorednoise-1.1.1-py3-none-any.whl size=3958 sha256=65340b48a6e373c240574bcf7e108ed4683fa59c36e081904a9372abfdf57aef
      Stored in directory: /root/.cache/pip/wheels/1a/ca/93/dfd64286aef6fc206b727bd4cf2d5c17efe34d62b918c8f4a7
    Successfully built colorednoise
    Installing collected packages: colorednoise
    Successfully installed colorednoise-1.1.1
    
    
<br>
<br>
<br>

* ê¸°íƒ€ í•„ìš”í•œ Packageë¥¼ ëª¨ë‘ Loadí•©ë‹ˆë‹¤.   


```python
import librosa
import numpy as np
import csv
from tqdm.notebook import tqdm
import pickle
from skimage.transform import resize
import pandas as pd

import tensorflow as tf

import librosa.display
import colorednoise as cn

from sklearn.model_selection import StratifiedKFold

from tensorflow.keras.models import Sequential
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout,GlobalAveragePooling2D,BatchNormalization
from tensorflow.keras import optimizers
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
```

   

   

   

* ì´ ë¬¸ì„œëŠ” Kaggle Notebookì—ì„œ ë§Œë“¤ì–´ì„œ ì•„ë˜ í•­ëª©ì€ ìë™ìœ¼ë¡œ ë“¤ì–´ê°€ëŠ” ë‚´ìš©ì´ë‹ˆ ê·¸ëƒ¥ ì°¸ê³ ë§Œ í•´ì£¼ì„¸ìš”.   


```python
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        #print(os.path.join(dirname, filename))
        pass

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
```


<br>
<br>
<br>
<br>

* ì•„ë˜ ë‚´ìš©ë“¤ì€ Sound Augmentation ê´€ë ¨ ë‚´ìš©ë“¤ì„ ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜¨ ê²ƒì…ë‹ˆë‹¤.   


```python
class AudioTransform:
    def __init__(self, always_apply=False, p=0.5):
        self.always_apply = always_apply
        self.p = p

    def __call__(self, y: np.ndarray):
        if self.always_apply:
            return self.apply(y)
        else:
            if np.random.rand() < self.p:
                return self.apply(y)
            else:
                return y

    def apply(self, y: np.ndarray):
        raise NotImplementedError


class Compose:
    def __init__(self, transforms: list):
        self.transforms = transforms

    def __call__(self, y: np.ndarray):
        for trns in self.transforms:
            y = trns(y)
        return y


class OneOf:
    def __init__(self, transforms: list):
        self.transforms = transforms

    def __call__(self, y: np.ndarray):
        n_trns = len(self.transforms)
        trns_idx = np.random.choice(n_trns)
        trns = self.transforms[trns_idx]
        return trns(y)
```

<br>
<br>

#### 1.1. GaussianNoiseSNR


```python
class GaussianNoiseSNR(AudioTransform):
    def __init__(self, always_apply=False, p=0.5, min_snr=5.0, max_snr=20.0, **kwargs):
        super().__init__(always_apply, p)

        self.min_snr = min_snr
        self.max_snr = max_snr

    def apply(self, y: np.ndarray, **params):
        snr = np.random.uniform(self.min_snr, self.max_snr)
        a_signal = np.sqrt(y ** 2).max()
        a_noise = a_signal / (10 ** (snr / 20))

        white_noise = np.random.randn(len(y))
        a_white = np.sqrt(white_noise ** 2).max()
        augmented = (y + white_noise * 1 / a_white * a_noise).astype(y.dtype)
        return augmented
```

<br>
<br>

#### 1.2.  PinkNoiseSNR


```python
class PinkNoiseSNR(AudioTransform):
    def __init__(self, always_apply=False, p=0.5, min_snr=5.0, max_snr=20.0, **kwargs):
        super().__init__(always_apply, p)

        self.min_snr = min_snr
        self.max_snr = max_snr

    def apply(self, y: np.ndarray, **params):
        snr = np.random.uniform(self.min_snr, self.max_snr)
        a_signal = np.sqrt(y ** 2).max()
        a_noise = a_signal / (10 ** (snr / 20))

        pink_noise = cn.powerlaw_psd_gaussian(1, len(y))
        a_pink = np.sqrt(pink_noise ** 2).max()
        augmented = (y + pink_noise * 1 / a_pink * a_noise).astype(y.dtype)
        return augmented        
```

<br>
<br>

#### 1.3. PitchShift


```python
class PitchShift(AudioTransform):
    def __init__(self, always_apply=False, p=0.5, max_steps=5, sr=32000):
        super().__init__(always_apply, p)

        self.max_steps = max_steps
        self.sr = sr

    def apply(self, y: np.ndarray, **params):
        n_steps = np.random.randint(-self.max_steps, self.max_steps)
        augmented = librosa.effects.pitch_shift(y, sr=self.sr, n_steps=n_steps)
        return augmented        
```

<br>
<br>

#### 1.4. TimeShift


```python
class TimeShift(AudioTransform):
    def __init__(self, always_apply=False, p=0.5, max_shift_second=2, sr=32000, padding_mode="replace"):
        super().__init__(always_apply, p)
    
        assert padding_mode in ["replace", "zero"], "`padding_mode` must be either 'replace' or 'zero'"
        self.max_shift_second = max_shift_second
        self.sr = sr
        self.padding_mode = padding_mode

    def apply(self, y: np.ndarray, **params):
        shift = np.random.randint(-self.sr * self.max_shift_second, self.sr * self.max_shift_second)
        augmented = np.roll(y, shift)
        if self.padding_mode == "zero":
            if shift > 0:
                augmented[:shift] = 0
            else:
                augmented[shift:] = 0
        return augmented
```

<br>
<br>

#### 1.5. TimeShift


```python
class VolumeControl(AudioTransform):
    def __init__(self, always_apply=False, p=0.5, db_limit=10, mode="uniform"):
        super().__init__(always_apply, p)

        assert mode in ["uniform", "fade", "fade", "cosine", "sine"], \
            "`mode` must be one of 'uniform', 'fade', 'cosine', 'sine'"

        self.db_limit= db_limit
        self.mode = mode

    def apply(self, y: np.ndarray, **params):
        db = np.random.uniform(-self.db_limit, self.db_limit)
        if self.mode == "uniform":
            db_translated = 10 ** (db / 20)
        elif self.mode == "fade":
            lin = np.arange(len(y))[::-1] / (len(y) - 1)
            db_translated = 10 ** (db * lin / 20)
        elif self.mode == "cosine":
            cosine = np.cos(np.arange(len(y)) / len(y) * np.pi * 2)
            db_translated = 10 ** (db * cosine / 20)
        else:
            sine = np.sin(np.arange(len(y)) / len(y) * np.pi * 2)
            db_translated = 10 ** (db * sine / 20)
        augmented = y * db_translated
        return augmented        
```


<br>
<br>
<br>
<br>

* ì•„ë˜ ìƒìˆ˜ë“¤ì€ ì´ì „ Revisionê³¼ ê±°ì˜ ë™ì¼í•˜ê²Œ ì‚¬ìš©í•˜ì§€ë§Œ, Sound Fileì„ Cropí•˜ëŠ” ê¸¸ì´ë¥¼ 6ì´ˆì—ì„œ 10ì´ˆë¡œ ëŠ˜ë ¸ìŠµë‹ˆë‹¤.   


```python
SLICE_TIME_WINDOWS = 10

fft = 2048
hop = 512

# Less rounding errors this way
sr = 48000
length = SLICE_TIME_WINDOWS * sr

F_MIN = 84
F_MAX = 15056

NUM_CLASS = 24
```

<br>
<br>
<br>
<br>

* ìœ„ì—ì„œ ì •ì˜í•œ ë‹¤ì–‘í•œ Sound Augmentation ê¸°ë²•ë“¤ì„ Randomí•˜ê²Œ ì ìš©í•˜ë„ë¡ í•´ì¤ë‹ˆë‹¤.   


```python
transform = Compose([
  OneOf([
      GaussianNoiseSNR(always_apply=True, min_snr=5, max_snr=20),
      PinkNoiseSNR(always_apply=True, min_snr=5.0, max_snr=20.0)
  ]),
    PitchShift(always_apply=True, max_steps=5, sr=sr),
    TimeShift(always_apply=True, max_shift_second=4, sr=sr),
    VolumeControl(always_apply=True, mode="sine")
])
```

<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>

### 2. Custom Generator

* ì´ì „ Post [Custom Generator](https://moonlight314.github.io/deeplearning/Custom_Generator/)ì—ì„œ ë‹¤ë£¨ì—ˆë˜ ì‚¬í•­ë“¤ì„ ì´ë²ˆì— ì ìš©í•´ ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.


* ì´ë²ˆì—ëŠ” Trainì— ì‚¬ìš©í•  Generatorì™€ Validationì—ì„œ ì‚¬ìš©í•  Generatorë¥¼ ë”°ë¡œ ë§Œë“¤ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.


* ë‘ Generatorì˜ ì°¨ì´ì ì€ **Augmentationì„ ì ìš©í•˜ëŠëƒ í•˜ì§€ ì•ŠëŠëƒì˜ ì°¨ì´**ë§Œ ìˆì„ ë¿ ë‹¤ë¥¸ ì°¨ì´ëŠ” ì—†ìŠµë‹ˆë‹¤.


* ì¼ë°˜ì ìœ¼ë¡œ Validation Datasetì—ëŠ” Augmentationì„ ì ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.


* 'train_gen' optionì´ Trainê³¼ Validationì„ êµ¬ë¶„í•˜ëŠ” Parameterì…ë‹ˆë‹¤. 

   


```python
class DataGenerator(tf.keras.utils.Sequence):
    
    'Generates data for Keras'
    def __init__(self, 
                 list_IDs, 
                 labels, 
                 batch_size=32, 
                 dim=(128,563), 
                 n_channels=1,
                 t_min=[],
                 t_max=[],
                 recording_id=[],
                 train=True,
                 n_classes=24, 
                 shuffle=True):
        
        'Initialization'
        self.dim = dim
        self.batch_size = batch_size
        self.labels = labels
        self.list_IDs = list_IDs
        self.n_channels = n_channels
        self.t_min = t_min
        self.t_max = t_max
        self.recording_id = recording_id
        self.n_classes = n_classes
        self.train_gen = train
        self.shuffle = shuffle
        
        self.n = 0
        self.max = self.__len__()

        self.on_epoch_end()
        

    def __len__(self):
        'Denotes the number of batches per epoch'
        return int(np.floor(len(self.list_IDs) / self.batch_size))

    def __getitem__(self, index):
        'Generate one batch of data'
        # Generate indexes of the batch
        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]

        # Find list of IDs
        list_IDs_temp = [self.list_IDs[k] for k in indexes]

        # Generate data
        X, y = self.__data_generation(list_IDs_temp)

        return X, y

    def on_epoch_end(self):
        'Updates indexes after each epoch'
        self.indexes = np.arange(len(self.list_IDs))
        if self.shuffle == True:
            np.random.shuffle(self.indexes)
            
    
    def __next__(self):
        if self.n >= self.max:
           self.n = 0
        result = self.__getitem__(self.n)
        self.n += 1
        return result
    

    def __data_generation(self, list_IDs_temp):
        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)
        # Initialization
        X = np.empty((self.batch_size, *self.dim, self.n_channels))
        y = np.empty((self.batch_size), dtype=int)

        
        # Generate data
        for i, ID in enumerate(list_IDs_temp):
            # Store sample
            wav, sr = librosa.load('/kaggle/input/rfcx-species-audio-detection/train/' + recording_id[ID] + '.flac', sr=None)

            t_start = self.t_min[ID] * sr
            t_end = self.t_max[ID] * sr
            
            # Positioning sound slice
            center = np.round((t_start + t_end) / 2)
            beginning = center - length / 2
            
            if beginning < 0:
                beginning = 0                
            
            ending = beginning + length
            
            if ending > len(wav):
                ending = len(wav)
                beginning = ending - length
            
            slice = wav[int(beginning):int(ending)]

            if self.train_gen == True:
                aug_slice = transform( slice )
            else:
                aug_slice = slice
                
        
            mel_spec = librosa.feature.melspectrogram(aug_slice, n_fft=fft, hop_length=hop, sr=sr, fmin=F_MIN, fmax=F_MAX, power=2)    
            mel_spec = librosa.core.amplitude_to_db(np.abs(mel_spec))

            mel_spec = mel_spec - np.min(mel_spec)
            mel_spec = mel_spec / np.max(mel_spec)   
            
            stacked = resize(mel_spec , (self.dim[0], self.dim[1], self.n_channels))
            X[i,] = stacked

            
            # Store class
            y[i] = self.labels[ID]
        
        return X, tf.keras.utils.to_categorical(y, num_classes=self.n_classes)
```


<br>
<br>
<br>
<br>

* ì´ì „ ë°©ë²•ê³¼ ìœ ì‚¬í•˜ê²Œ ì§„í–‰í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.


* train_tp.csvì—ì„œ Train Data ê´€ë ¨ Meta ì •ë³´ë¥¼ ì½ì–´ì„œ ì¤€ë¹„í•©ë‹ˆë‹¤.


```python
data = pd.read_csv("/kaggle/input/rfcx-species-audio-detection/train_tp.csv")
data.head()
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>recording_id</th>
      <th>species_id</th>
      <th>songtype_id</th>
      <th>t_min</th>
      <th>f_min</th>
      <th>t_max</th>
      <th>f_max</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>003bec244</td>
      <td>14</td>
      <td>1</td>
      <td>44.5440</td>
      <td>2531.250</td>
      <td>45.1307</td>
      <td>5531.25</td>
    </tr>
    <tr>
      <th>1</th>
      <td>006ab765f</td>
      <td>23</td>
      <td>1</td>
      <td>39.9615</td>
      <td>7235.160</td>
      <td>46.0452</td>
      <td>11283.40</td>
    </tr>
    <tr>
      <th>2</th>
      <td>007f87ba2</td>
      <td>12</td>
      <td>1</td>
      <td>39.1360</td>
      <td>562.500</td>
      <td>42.2720</td>
      <td>3281.25</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0099c367b</td>
      <td>17</td>
      <td>4</td>
      <td>51.4206</td>
      <td>1464.260</td>
      <td>55.1996</td>
      <td>4565.04</td>
    </tr>
    <tr>
      <th>4</th>
      <td>009b760e6</td>
      <td>10</td>
      <td>1</td>
      <td>50.0854</td>
      <td>947.461</td>
      <td>52.5293</td>
      <td>10852.70</td>
    </tr>
  </tbody>
</table>
</div>


<br>
<br>
<br>
<br>

* Generatorë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ì„œ í•„ìš”í•œ ê°’ë“¤ì„ ë¯¸ë¦¬ ë½‘ì•„ë†“ê² ìŠµë‹ˆë‹¤.   


```python
recording_id = data['recording_id'].to_list()
labels = data['species_id'].to_list()
t_min = data['t_min'].to_list()
t_max = data['t_max'].to_list()
list_IDs = [i for i in range(len(recording_id))]
```

<br>
<br>
<br>

* Train Setê³¼ Validation Setì„ 2:1 ë¹„ìœ¨ë¡œ ë‚˜ëˆ„ê² ìŠµë‹ˆë‹¤.


* Labelì˜ ê· ë“±í•œ ë°°ë¶„ì„ ìœ„í•´ StratifiedKFoldë¥¼ ì‚¬ìš©í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.


* Dataê°€ ë„ˆë¬´ ì ì–´ì„œ ê±±ì •ì´ë„¤ìš”...


```python
skf = StratifiedKFold(n_splits=3 , random_state=27 , shuffle=True)
```


```python
for train_index, val_index in skf.split(list_IDs , labels):
    print(len(train_index) , len(val_index))
```

    810 406
    811 405
    811 405
    
<br>
<br>
<br>

* StratifiedKFoldë¡œ ë‚˜ëˆˆ ê°’ë“¤ì„ Train & Validation Generatorì— ë„£ì„ ê°’ì„ ê°ê° ë¶„ë¦¬í•©ë‹ˆë‹¤.


```python
recording_id_Train = [recording_id[k] for k in train_index]
recording_id_Val = [recording_id[k] for k in val_index]

Label_Train = [labels[k] for k in train_index]
Label_Val = [labels[k] for k in val_index]

t_min_Train = [t_min[k] for k in train_index]
t_min_Val = [t_min[k] for k in val_index]

t_max_Train = [t_max[k] for k in train_index]
t_max_Val = [t_max[k] for k in val_index]

list_IDs_Train = [k for k in range(len(train_index))]
list_IDs_Val = [k for k in range(len(val_index))]
```

<br>
<br>
<br>

Trainê³¼ Validation Generatorì˜ ì°¨ì´ëŠ” Augmentationì„ ì ìš©í•˜ëŠëƒ ë§ˆëŠëƒì˜ ì°¨ì´ë¿ì…ë‹ˆë‹¤.

```python
# Train Data Generatpr
train_params = {'dim': (128,1876),
          'batch_size': 32,
          'n_classes': 24,
          'n_channels': 1,
          't_min': t_min_Train,
          't_max': t_max_Train,
          'recording_id' : recording_id_Train,
          'train' : True,
          'shuffle': True}

train_generator = DataGenerator(list_IDs = list_IDs_Train, labels = Label_Train, **train_params)
```
<br>
<br>
<br>

```python
# Train Data Generatpr
val_params = {'dim': (128,1876),
          'batch_size': 32,
          'n_classes': 24,
          'n_channels': 1,
          't_min': t_min_Val,
          't_max': t_max_Val,
          'recording_id' : recording_id_Val,
          'train' : False,
          'shuffle': True}

val_generator = DataGenerator(list_IDs = list_IDs_Val, labels = Label_Val, **val_params)
```

<br>
<br>
<br>
<br>
<br>
<br>

### 3. Metric

* ì´ë²ˆ Competitionì—ëŠ” í”íˆ ë§ì´ ì“°ì´ëŠ” Metricì´ ì•„ë‹Œ **LWLRAP(Label Weighted Label Ranking Average Precision)** ì´ë¼ëŠ” Evaluation Metricì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

<br>
<br>
<p align="center">
  <img src="/assets/RFCX_02/pic_00.png">
</p>
<br>
<br>

* ì´ë¥¼ ì˜ ì„¤ëª…í•´ ë†“ì€ ê¸€ì´ ìˆì–´ì„œ ì†Œê°œí•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.   

https://www.kaggle.com/pkmahan/understanding-lwlrap

* Scikit-Learn Siteì—ë„ ê´€ë ¨ëœ ì„¤ëª…ì„ ì°¾ì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.   

https://scikit-learn.org/stable/modules/model_evaluation.html#label-ranking-average-precision      

<br>
<br>   
<br>
<br>
   

* ì €ë„ ìœ„ì˜ ê¸€ì—ì„œ Codeë¥¼ ê°€ì ¸ì™€ì„œ Montoringì— ì‚¬ìš©í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.   

<br>
<br>   
<br>
<br>

```python
def _one_sample_positive_class_precisions(example):
    y_true, y_pred = example
    y_true = tf.reshape(y_true, tf.shape(y_pred))
    retrieved_classes = tf.argsort(y_pred, direction='DESCENDING')
#     shape = tf.shape(retrieved_classes)
    class_rankings = tf.argsort(retrieved_classes)
    retrieved_class_true = tf.gather(y_true, retrieved_classes)
    retrieved_cumulative_hits = tf.math.cumsum(tf.cast(retrieved_class_true, tf.float32))

    idx = tf.where(y_true)[:, 0]
    i = tf.boolean_mask(class_rankings, y_true)
    r = tf.gather(retrieved_cumulative_hits, i)
    c = 1 + tf.cast(i, tf.float32)
    precisions = r / c

    dense = tf.scatter_nd(idx[:, None], precisions, [y_pred.shape[0]])
    return dense

# @tf.function
class LWLRAP(tf.keras.metrics.Metric):
    def __init__(self, num_classes, name='lwlrap'):
        super().__init__(name=name)

        self._precisions = self.add_weight(
            name='per_class_cumulative_precision',
            shape=[num_classes],
            initializer='zeros',
        )

        self._counts = self.add_weight(
            name='per_class_cumulative_count',
            shape=[num_classes],
            initializer='zeros',
        )

    def update_state(self, y_true, y_pred, sample_weight=None):
        precisions = tf.map_fn(
            fn=_one_sample_positive_class_precisions,
            elems=(y_true, y_pred),
            dtype=(tf.float32),
        )

        increments = tf.cast(precisions > 0, tf.float32)
        total_increments = tf.reduce_sum(increments, axis=0)
        total_precisions = tf.reduce_sum(precisions, axis=0)

        self._precisions.assign_add(total_precisions)
        self._counts.assign_add(total_increments)        

    def result(self):
        per_class_lwlrap = self._precisions / tf.maximum(self._counts, 1.0)
        per_class_weight = self._counts / tf.reduce_sum(self._counts)
        overall_lwlrap = tf.reduce_sum(per_class_lwlrap * per_class_weight)
        return overall_lwlrap

    def reset_states(self):
        self._precisions.assign(self._precisions * 0)
        self._counts.assign(self._counts * 0)
```

<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>

### 4. Model   

* ì, ì´ì œ Trainì— ì‚¬ìš©í•  Modelì„ êµ¬ì„±í•´ ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.   


```python
model = Sequential()
```
   
<br>
<br>
<br>
<br>
   

* ì²˜ìŒì— ë§ì”€ë“œë ¸ë“¯ì´, ResNet50ì„ ì‚¬ìš©í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.   

* Tensorflowì—ì„œëŠ” ResNet50ì„ ì œê³µí•´ ì£¼ê³  ìˆëŠ”ë°, ì•„ë˜ Linkì—ì„œ ìì„¸í•œ ë‚´ìš©ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

  [ResNet50 in Tensorflow](https://www.tensorflow.org/api_docs/python/tf/keras/applications/ResNet50)
  
  
* ResNetì˜ ì¢…ë¥˜ëŠ” ê·œëª¨(?)ì— ë”°ë¼ì„œ ë‹¤ì–‘í•œë°, ê°€ì¥ ë§Œë§Œí•œ ResNet50ì„ ì‚¬ìš©í•´ ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.


* ResNet50ì€ ê¸°ë³¸ì ìœ¼ë¡œ ImageNet Datasetìœ¼ë¡œ Trainingë˜ì—ˆìŠµë‹ˆë‹¤. ì¦‰, Feature Extractionì„ í•  ë•Œ ImageNetê³¼ ìœ ì‚¬í•œ Imageì— ëŒ€í•´ì„œëŠ” ì˜ í• ì§€ ëª°ë¼ë„, ì´ë²ˆ Competitionì—ì„œ ì‚¬ìš©í•˜ëŠ” MEL Spec.ì™€ ê°™ì€ Imageì—ëŠ” ë§ì§€ ì•Šì„ ë“¯ í•©ë‹ˆë‹¤.


* ê·¸ë˜ì„œ, ì´ë²ˆì—ëŠ” Weightë¥¼ Loadí•˜ì§€ ì•Šê³ , ResNet50ì˜ Structureë§Œ ë¹Œë ¤ì˜¤ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.


* ê·¸ë¦¬ê³ , ResNet50ì˜ Input Shapeì€ (224,224,3)ì´ì§€ë§Œ, MEL Spec.ì€ Shapeì´ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— Input Shapeì„ ì¬ì •ì˜í•˜ì˜€ìŠµë‹ˆë‹¤.

<br>
<br>
<br>

```python
model.add( ResNet50(include_top = False, 
                    input_shape=(256, 1876, 1),
                    weights = None ))
```

<br>
<br>
<br>

* Extracted Featureë¥¼ ë°›ì•„ì„œ ë¶„ë¥˜í•  Simple MLPë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.   


```python
model.add(GlobalAveragePooling2D()) 

model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(1024, activation='relu' , kernel_initializer = tf.keras.initializers.he_normal()))

model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(NUM_CLASS, activation = 'softmax' , kernel_initializer = tf.keras.initializers.he_normal()))
```


<br>
<br>
<br>

* ìµœì¢…ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ë¥¼ ê°€ì§€ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.


* ResNet50 ì „ì²´ì˜ Weightë¥¼ ì „ë¶€ Trainì‹œí‚¤ê¸° ë•Œë¬¸ì— Trainable params ìˆ˜ê°€ ë§ë„¤ìš”.


```python
model.summary()
```

    Model: "sequential"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    resnet50 (Functional)        (None, 8, 59, 2048)       23581440  
    _________________________________________________________________
    global_average_pooling2d (Gl (None, 2048)              0         
    _________________________________________________________________
    batch_normalization (BatchNo (None, 2048)              8192      
    _________________________________________________________________
    dropout (Dropout)            (None, 2048)              0         
    _________________________________________________________________
    dense (Dense)                (None, 1024)              2098176   
    _________________________________________________________________
    batch_normalization_1 (Batch (None, 1024)              4096      
    _________________________________________________________________
    dropout_1 (Dropout)          (None, 1024)              0         
    _________________________________________________________________
    dense_1 (Dense)              (None, 24)                24600     
    =================================================================
    Total params: 25,716,504
    Trainable params: 25,657,240
    Non-trainable params: 59,264
    _________________________________________________________________
    
<br>
<br>
<br>

* OptimizerëŠ” Adamì„ ì‚¬ìš©í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.


* Evaluation Metricì€ ì´ì „ì— ì†Œê°œí•´ë“œë¦° LWLRAPë¥¼ ì‚¬ìš©í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.


```python
sgd = optimizers.SGD(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True)
adam = optimizers.Adam(lr = 0.001)

model.compile(optimizer = adam, 
              loss = 'categorical_crossentropy', 
              metrics = [LWLRAP(num_classes = NUM_CLASS)],
             )
```
<br>
<br>
<br>

* Callbackì€ Validation Setì—ì„œ LWLRAPê°’ì´ ê°€ì¥ ë†’ì€ ê²ƒì„ ì €ì¥í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.


```python
# set model callbacks to save best model
filepath = "Saved-model-best.hdf5"

my_callbacks = [ModelCheckpoint(filepath, 
                                monitor='val_lwlrap', 
                                verbose=1,
                                save_best_only=True, 
                                mode='max')]
```

<br>
<br>
<br>
* ì´ì œ ëŒ€ë§ì˜ Trainì‹œì‘ì…ë‹ˆë‹¤ !


* Generatorë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, Kaggle Notebook Env.ëŠ” TF 2.xxë¥¼ ì‚¬ìš©í•˜ê³  ìˆê¸° ë•Œë¬¸ì— ê·¸ëƒ¥ .fitìœ¼ë¡œë„ Trainì´ ë©ë‹ˆë‹¤.


* ì¼ë‹¨ 10ë²ˆë§Œ ëŒë ¤ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.


```python
#history = model.fit_generator(
history = model.fit(
        train_generator,
        epochs = 10,
        validation_data=val_generator,
        callbacks=my_callbacks,
        verbose=1
)
```

    Epoch 1/10
    25/25 [==============================] - 858s 34s/step - loss: 3.8363 - lwlrap: 0.1664 - val_loss: 3.3410 - val_lwlrap: 0.1465
    
    Epoch 00001: val_lwlrap did not improve from 0.15455
    Epoch 2/10
    25/25 [==============================] - 848s 34s/step - loss: 3.8105 - lwlrap: 0.1541 - val_loss: 3.2576 - val_lwlrap: 0.1667
    
    Epoch 00002: val_lwlrap improved from 0.15455 to 0.16667, saving model to Saved-model-best.hdf5
    Epoch 3/10
    25/25 [==============================] - 863s 35s/step - loss: 3.7783 - lwlrap: 0.1677 - val_loss: 3.3092 - val_lwlrap: 0.1631
    
    Epoch 00003: val_lwlrap did not improve from 0.16667
    Epoch 4/10
    25/25 [==============================] - 872s 35s/step - loss: 3.8039 - lwlrap: 0.1689 - val_loss: 3.4925 - val_lwlrap: 0.1941
    
    Epoch 00004: val_lwlrap improved from 0.16667 to 0.19413, saving model to Saved-model-best.hdf5
    Epoch 5/10
    25/25 [==============================] - 869s 35s/step - loss: 3.7036 - lwlrap: 0.1813 - val_loss: 3.3664 - val_lwlrap: 0.1477
    
    Epoch 00005: val_lwlrap did not improve from 0.19413
    Epoch 6/10
    25/25 [==============================] - 862s 35s/step - loss: 3.6492 - lwlrap: 0.1848 - val_loss: 3.2578 - val_lwlrap: 0.1963
    
    Epoch 00006: val_lwlrap improved from 0.19413 to 0.19625, saving model to Saved-model-best.hdf5
    Epoch 7/10
    25/25 [==============================] - 855s 34s/step - loss: 3.7575 - lwlrap: 0.1706 - val_loss: 3.3313 - val_lwlrap: 0.1642
    
    Epoch 00007: val_lwlrap did not improve from 0.19625
    Epoch 8/10
    25/25 [==============================] - 878s 35s/step - loss: 3.6909 - lwlrap: 0.1683 - val_loss: 3.3364 - val_lwlrap: 0.1547
    
    Epoch 00008: val_lwlrap did not improve from 0.19625
    Epoch 9/10
    25/25 [==============================] - 867s 35s/step - loss: 3.6804 - lwlrap: 0.1811 - val_loss: 3.3591 - val_lwlrap: 0.1454
    
    Epoch 00009: val_lwlrap did not improve from 0.19625
    Epoch 10/10
    25/25 [==============================] - 877s 35s/step - loss: 3.6514 - lwlrap: 0.1711 - val_loss: 3.2316 - val_lwlrap: 0.1690
    
    Epoch 00010: val_lwlrap did not improve from 0.19625
    

<br>
<br>
<br>

* 10íšŒ ì •ë„ë§Œ ëŒë ¤ë³´ì•˜ìŠµë‹ˆë‹¤ë§Œ, ê²°ê³¼ì ìœ¼ë¡œ ì„±ê³µì ì´ì§€ëŠ” ì•ŠëŠ”ê²ƒ ê°™ìŠµë‹ˆë‹¤.
<br>

* ìš°ì„ , ë‹¤í–‰ìŠ¤ëŸ½ê²Œë„ Train Setì—ì„œì˜ LWLRAPê°’ê³¼ Validation Setì—ì„œì˜ LWLRAP ê°’ì´ ëª¨ë‘ ë¹„ìŠ·í•˜ê²Œ ë‚˜ì˜¤ê³  ìˆëŠ” ê²ƒìœ¼ë¡œ ë³´ì•„, ê°€ì¥ ê±±ì •í–ˆë˜ Overfittingì€ ë°œìƒí•˜ì§€ ì•ŠëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤.
<br>

* ì•„ë§ˆë„ Data Augmentationì´ë‚˜ ëª‡ëª‡ ì¥ì¹˜ë“¤ì´ ì˜í–¥ì„ ì¤€ ê²ƒ ê°™ìŠµë‹ˆë‹¤.
<br>

* í•˜ì§€ë§Œ,ê²°ë¡ ì ìœ¼ë¡œ ì´ Modelì€ ì‚¬ìš©í•˜ì§€ ëª»í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤.  
<br>

* ë¨¼ì € LWLRAPê°’ì´ Trainingì„ ì§„í–‰í•´ë„ í¬ê²Œ ê°œì„ ë˜ì§€ ì•Šê³  ìˆìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  LWLRAP ê°’ ìì²´ê°€ ë„ˆë¬´ ë‚®ìŠµë‹ˆë‹¤. ( LWLRAP ê°’ì€ 1.00ì´ ìµœê³ ê°’ì…ë‹ˆë‹¤. )
<br>

* ì‚¬ì‹¤, ì´ Modelì€ 17ë²ˆì§¸ì´ë©°, ì´ì „ì— ë‹¤ì–‘í•œ ë°©ë²•ê³¼ Featureë“¤ë¡œ Testí•´ ë³¸ ê²°ê³¼, ê°€ì¥ í° ë¬¸ì œëŠ” ì ì€ ì–‘ì˜ Datasetë•Œë¬¸ì— Overfittingì´ ê°€ì¥ í° ë¬¸ì œì˜€ìŠµë‹ˆë‹¤.
<br>

* Overfittingì€ Sound Augmentationìœ¼ë¡œ ì–´ëŠ ì •ë„ í•´ê²°ì´ ëœ ê²ƒ ê°™ìœ¼ë‚˜, ë‚®ì€ LWLRAPê°’ì€ ì—¬ì „íˆ ë¬¸ì œì…ë‹ˆë‹¤.
<br>

* Algorithmì´ë‚˜ Modelì˜ ë¬¸ì œë¼ê¸° ë³´ë‹¤ëŠ” Featureì˜ ë¬¸ì œë¡œ ë³´ì´ë©°, ì´ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ì„œ ì¢€ ë” Studyë¥¼ í•´ ë´ì•¼ í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤.
<br>

* ResNetì˜ Weightì˜ ìˆ˜ê°€ ë§ì•„ì„œì¸ì§€ Localì—ì„œì˜ Trainì€ Resourceë¥¼ ë§ì´ ì¡ì•„ë¨¹ëŠ” ê²½í–¥ì„ ë³´ì˜€ê³ , GPUì—ì„œë„ Train ì‹œê°„ë„ ë§ì´ ê±¸ë ¸ìŠµë‹ˆë‹¤.

  ( ì´ Notebookë„ Kaggleì—ì„œ ì œê³µí•˜ëŠ” GPUì—ì„œ ëŒë ¸ìŠµë‹ˆë‹¤. )
<br>  
  
* ëŒ€ìš©ëŸ‰ Datasetì´ Kaggleì—ì„œë„ ë§ì•„ì§€ëŠ” ê²½í–¥ì´ì–´ì„œ ìµœê·¼ Competitionì—ì„œëŠ” TFRecord Formatë„ ê°™ì´ ì œê³µí•´ì£¼ëŠ” Competitionì´ ë§ì•„ì§€ê³  ìˆìŠµë‹ˆë‹¤.
<br>

* ì´ëŠ” ë‹¤ë¶„íˆ Tensorflow & TPU ì¡°í•©ì„ ì‚¬ìš©í•˜ë¼ëŠ” ë°°ë ¤(í˜¹ì€ ì••ë°•)ë¼ê³  ìƒê°í•©ë‹ˆë‹¤.
<br>

* ë‹¤ìŒì— ì‹œê°„ë˜ë©´ TFRecordë¥¼ ë‹¤ë£¨ëŠ” ë°©ë²•ì— ëŒ€í•´ì„œë„ í•œ ë²ˆ ì•Œì•„ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.
<br>

* ì´ë²ˆ PostëŠ” ì—¬ê¸°ê¹Œì§€ í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.  ë„ì›€ì´ ë˜ì…¨ê¸°ë¥¼ ë°”ëë‹ˆë‹¤.
