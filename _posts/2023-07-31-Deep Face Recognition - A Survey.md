---
title: "Deep Face Recognition - A Survey"
categories: Deep Learning
---
# Deep Face Recognition - A Survey

<br>

## Abstract

<br>
<br>

**Deep learning applies multiple processing layers to learn representations of data with multiple levels of feature extraction.**  
Deep learningì€ multiple processing layerë¥¼ ì ìš©í•˜ì—¬ representations of dataë¥¼ í•™ìŠµí•˜ê³ , ì—¬ëŸ¬ ë‹¨ê³„ì˜ feature extractionì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

<br>

**This emerging technique has reshaped the research landscape of face recognition (FR) since 2014, launched by the breakthroughs of DeepFace and DeepID.**  
ì´ ìƒˆë¡œìš´ ê¸°ìˆ ì€ DeepFaceì™€ DeepIDë¥¼ ì‹œì‘ìœ¼ë¡œ 2014ë…„ ì´í›„ ì–¼êµ´ ì¸ì‹(FR) ì—°êµ¬ì— ìƒˆë¡œìš´ ì¥ì„ ì—´ì—ˆìŠµë‹ˆë‹¤.

<br>

**Since then, deep learning technique, characterized by the hierarchical architecture to stitch together pixels into invariant face representation, has dramatically improved the state-of-the-art performance and fostered successful real-world applications.**  
ì´í›„ë¡œ deep learning technique, ì¦‰ ê³„ì¸µì ì¸ êµ¬ì¡°ë¥¼ í†µí•´ Pixelì„ invariant face representationë¡œ ì¡°í•©í•˜ëŠ” ê²ƒì´ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚¤ê³  ì„±ê³µì ì¸ ì‹¤ì œ ì‘ìš© í”„ë¡œê·¸ë¨ì„ ì´‰ì§„í•˜ì˜€ìŠµë‹ˆë‹¤.

<br>

**In this survey, we provide a comprehensive review of the recent developments on deep FR,covering broad topics on algorithm designs, databases, protocols, and application scenes.**  
ì´ ë…¼ë¬¸ì—ì„œëŠ” ìµœê·¼ deep FRì˜ ë°œì „ì— ëŒ€í•œ í¬ê´„ì ì¸ ë¦¬ë·°ë¥¼ ì œê³µí•˜ë©°, algorithm designs, databases, protocols, application scenesì— ëŒ€í•œ ê´‘ë²”ìœ„í•œ ì£¼ì œë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.

<br>

**First, we summarize different network architectures and loss functions proposed in the rapid evolution of the deep FR methods.**  
ì²«ì§¸, Deep FR ë°©ë²•ë¡ ì˜ ë¹ ë¥¸ ì§„í™”ì—ì„œ ì œì•ˆëœ ë‹¤ì–‘í•œ Network Architectureì™€ Loss Functionë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.

<br>

**Second, the related face processing methods are categorized into two classes: â€œone-to-many augmentationâ€ and â€œmany-to-one normalizationâ€.**  
ë‘˜ì§¸, ê´€ë ¨ ì–¼êµ´ ì²˜ë¦¬ ë°©ë²•ì€ "one-to-many augmentation"ê³¼ "many-to-one normalization" ë‘ ê°€ì§€ í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜ë©ë‹ˆë‹¤.

<br>

**Then, we summarize and compare the commonly used databases for both model training and evaluation.**  
ê·¸ëŸ° ë‹¤ìŒ, Model Trainê³¼ Evaluationë¥¼ ìœ„í•´ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” Databaseë¥¼ ìš”ì•½í•˜ê³  ë¹„êµí•©ë‹ˆë‹¤.

<br>

**Third, we review miscellaneous scenes in deep FR, such as cross-factor, heterogenous, multiple-media and industrial scenes.**  
ì…‹ì§¸, ìš°ë¦¬ëŠ” êµì°¨ ìš”ì¸(cross-factor), ì´ì¢…(heterogenous), ë‹¤ì¤‘ ë¯¸ë””ì–´(multiple-media) ë° ì‚°ì—… ì”¬(industrial scenes) ë“±ì˜ Deep FRì—ì„œ ë‹¤ì–‘í•œ Sceneì„ ê²€í† í•©ë‹ˆë‹¤.

<br>

**Finally, the technical challenges and several promising directions are highlighted.**  
ë§ˆì§€ë§‰ìœ¼ë¡œ, ê¸°ìˆ ì ì¸ ë„ì „ê³¼ ëª‡ ê°€ì§€ ìœ ë§í•œ ë°©í–¥ì„ ê°•ì¡°í•©ë‹ˆë‹¤.  


<br>
<br>
<br>

## I. INTRODUCTION  

<br>

**Face recognition (FR) has been the prominent biometric technique for identity authentication and has been widely used in many areas, such as military, finance, public security and daily life.**  
ì–¼êµ´ì¸ì‹(FR)ì€ ì‹ ì›ì¸ì¦ì„ ìœ„í•œ ëŒ€í‘œì ì¸ ìƒì²´ì¸ì‹ ê¸°ìˆ ë¡œ êµ°ì‚¬, ê¸ˆìœµ, ì¹˜ì•ˆ, ì¼ìƒìƒí™œ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ë„ë¦¬ í™œìš©ë˜ê³  ìˆë‹¤.

<br>

**FR has been a long-standing research topic in the CVPR community.**  
FRì€ CVPR ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ì˜¤ëœ ì—°êµ¬ ì£¼ì œì˜€ìŠµë‹ˆë‹¤.

<br>

**In the early 1990s, the study of FR became popular following the introduction of the historical Eigenface approach.**  
1990ë…„ëŒ€ ì´ˆ, ì—­ì‚¬ì ì¸ Eigenface ì ‘ê·¼ ë°©ì‹ì´ ë„ì…ë˜ë©´ì„œ FRì— ëŒ€í•œ ì—°êµ¬ê°€ ëŒ€ì¤‘í™”ë˜ì—ˆìŠµë‹ˆë‹¤.

<br>

**The milestones of feature-based FR over the past years are presented in Fig. 1, in which the times of four major technical streams are highlighted.**  
ì§€ë‚œ ëª‡ ë…„ ë™ì•ˆ feature-based FRì˜ ì´ì •í‘œê°€ ê·¸ë¦¼ 1ì— ë‚˜ì™€ ìˆìœ¼ë©°, ì—¬ê¸°ì—ì„œ ë„¤ ê°€ì§€ ì£¼ìš” ê¸°ìˆ  íë¦„ì´ í‘œì‹œë©ë‹ˆë‹¤.

<br>

<br>
<br>
<p align="center">
  <img src="/assets/DeepFaceRecognitionSurvey/Fig_01.png">
</p>
<br>
<br>

**The holistic approaches derive the low-dimensional representation through certain distribution assumptions, such as linear subspace, manifold, and sparse representation.**  
ì „ì²´ì ì¸ ì ‘ê·¼ ë°©ì‹ì€ linear subspace, manifold, sparse representationê³¼ ê°™ì€ íŠ¹ì • ë¶„í¬ ê°€ì •ì„ í†µí•´ low-dimensional representationì„ ë„ì¶œí•©ë‹ˆë‹¤.

<br>

**This idea dominated the FR community in the 1990s and 2000s.**  
ì´ ì•„ì´ë””ì–´ëŠ” 1990ë…„ëŒ€ì™€ 2000ë…„ëŒ€ì— FR ì»¤ë®¤ë‹ˆí‹°ë¥¼ ì§€ë°°í–ˆìŠµë‹ˆë‹¤.

<br>

**However, a well-known problem is that these theoretically plausible holistic methods fail to address the uncontrolled facial changes that deviate from their prior assumptions.**  
ê·¸ëŸ¬ë‚˜ ë¬¸ì œëŠ” ì´ëŸ¬í•œ ì´ë¡ ì ìœ¼ë¡œ ê·¸ëŸ´ë“¯í•œ ë°©ë²•ì´ ì´ì „ ê°€ì •ì—ì„œ ë²—ì–´ë‚˜ëŠ” í†µì œë˜ì§€ ì•Šì€ ì–¼êµ´ ë³€í™”(uncontrolled facial changes)ë¥¼ í•´ê²°í•˜ì§€ ëª»í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.

<br>

**In the early 2000s, this problem gave rise to local-feature-based FR.**  
2000ë…„ëŒ€ ì´ˆ, ì´ ë¬¸ì œëŠ” local-feature-based FRì„ ë°œìƒì‹œì¼°ìŠµë‹ˆë‹¤. 

<br>

**Gabor and LBP, as well as their multilevel and high-dimensional extensions, achieved robust performance through some invariant properties of local filtering.**  
Gabor ë° LBPì™€ ê·¸ë“¤ì˜ high-dimensional extensionsì€ invariant properties of local filteringì„ í†µí•´ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

<br>

**Unfortunately, handcrafted features suffered from a lack of distinctiveness and compactness.**  
ë¶ˆí–‰íˆë„ handcrafted featuresì€ ë…íŠ¹í•¨ê³¼ ê°„ê²°í•¨ì´ ë¶€ì¡±í–ˆìŠµë‹ˆë‹¤.

<br>

**In the early 2010s, learning-based local descriptors were introduced to the FR community, in which local filters are learned for better distinctiveness and the encoding codebook is learned for better compactness.**  
2010ë…„ëŒ€ ì´ˆë°˜ì— learning-based local descriptorsê°€ FR ì»¤ë®¤ë‹ˆí‹°ì— ë„ì…ë˜ì—ˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ local filtersëŠ” ë” ë‚˜ì€ ì‹ë³„ì„ ìœ„í•´ í•™ìŠµë˜ê³  ë” ë‚˜ì€ ì••ì¶•ì„ ìœ„í•´ encoding codebookì´ í•™ìŠµë©ë‹ˆë‹¤.

<br>

**However, these shallow representations still have an inevitable limitation on robustness against the complex nonlinear facial appearance variations.**  
ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ì–•ì€ í‘œí˜„ì€ ì—¬ì „íˆ ë³µì¡í•œ ë¹„ì„ í˜• ì–¼êµ´ ëª¨ì–‘ ë³€í˜•ì— ëŒ€í•œ ê²¬ê³ ì„±ì— ë¶ˆê°€í”¼í•œ ì œí•œì´ ìˆìŠµë‹ˆë‹¤.

<br>

**In general, traditional methods attempted to recognize human face by one or two layer representations, such as filtering responses, histogram of the feature codes, or distribution of the dictionary atoms.**  
ì¼ë°˜ì ìœ¼ë¡œ ì „í†µì ì¸ ë°©ë²•ì€ í•„í„°ë§ ì‘ë‹µ, íŠ¹ì§• ì½”ë“œì˜ íˆìŠ¤í† ê·¸ë¨ ë˜ëŠ” ì‚¬ì „ ì›ìì˜ ë¶„í¬ì™€ ê°™ì€ í•˜ë‚˜ ë˜ëŠ” ë‘ ê°œì˜ ë ˆì´ì–´ í‘œí˜„ìœ¼ë¡œ ì‚¬ëŒì˜ ì–¼êµ´ì„ ì¸ì‹í•˜ë ¤ê³  ì‹œë„í–ˆìŠµë‹ˆë‹¤.

<br>

**The research community studied intensively to separately improve the preprocessing, local descriptors, and feature transformation, but these approaches improved FR accuracy slowly.**  
ì—°êµ¬ ì»¤ë®¤ë‹ˆí‹°ëŠ” preprocessing, local descriptors ë° feature transformationì„ ê°œë³„ì ìœ¼ë¡œ ê°œì„ í•˜ê¸° ìœ„í•´ ì§‘ì¤‘ì ìœ¼ë¡œ ì—°êµ¬í–ˆì§€ë§Œ ì´ëŸ¬í•œ ì ‘ê·¼ ë°©ì‹ì€ FR ì •í™•ë„ë¥¼ ëŠë¦¬ê²Œ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.

<br>

**Whatâ€™s worse, most methods aimed to address one aspect of unconstrained facial changes only, such as lighting, pose, expression, or disguise.**
ì„¤ìƒê°€ìƒìœ¼ë¡œ, ëŒ€ë¶€ë¶„ì˜ ë°©ë²•ì€ ì¡°ëª…, í¬ì¦ˆ, í‘œì • ë˜ëŠ” ë³€ì¥ê³¼ ê°™ì€ ì œí•œë˜ì§€ ì•Šì€ ì–¼êµ´ ë³€í™”ì˜ í•œ ì¸¡ë©´ë§Œì„ ë‹¤ë£¨ëŠ” ê²ƒì„ ëª©í‘œë¡œ í–ˆìŠµë‹ˆë‹¤.

<br>

**There was no any integrated technique to address these unconstrained challenges integrally.**  
ì´ëŸ¬í•œ ì œí•œë˜ì§€ ì•Šì€ ë¬¸ì œë¥¼ í†µí•©ì ìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ìˆëŠ” í†µí•© ê¸°ìˆ ì€ ì—†ì—ˆìŠµë‹ˆë‹¤.

<br>

**As a result, with continuous efforts of more than a decade, â€œshallowâ€ methods only improved the accuracy of the LFW benchmark to about 95%, which indicates that â€œshallowâ€ methods are insufficient to extract stable identity feature invariant to real-world changes.**  
ê·¸ ê²°ê³¼, 10ë…„ ì´ìƒì˜ ì§€ì†ì ì¸ ë…¸ë ¥ìœ¼ë¡œ "shallow" ë°©ë²•ì€ LFW benchmarkì˜ ì •í™•ë„ë¥¼ ì•½ 95%ê¹Œì§€ í–¥ìƒì‹œì¼°ì„ ë¿ì´ë©°, ì´ëŠ” "shallow" ë°©ë²•ìœ¼ë¡œëŠ” ì‹¤ì œ ë³€í™”ì— ì˜í–¥ì„ ë°›ì§€ ì•ŠëŠ” ì•ˆì •ì ì¸ ì‹ë³„ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ê¸°ì— ì¶©ë¶„í•˜ì§€ ì•ŠìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. .

<br>

**Due to the insufficiency of this technical, facial recognition systems were often reported with unstable performance or failures with countless false alarms in real-world applications.**  
ì´ëŸ° ê¸°ìˆ ì˜ ë¶€ì¡±ìœ¼ë¡œ ì¸í•´ ì•ˆë©´ ì¸ì‹ ì‹œìŠ¤í…œì€ ì‹¤ì œ ì‘ìš© í”„ë¡œê·¸ë¨ì—ì„œ ë¬´ìˆ˜í•œ ì˜¤ê²½ë³´ì™€ í•¨ê»˜ ë¶ˆì•ˆì •í•œ ì„±ëŠ¥ ë˜ëŠ” ì‹¤íŒ¨ë¡œ ë³´ê³ ë˜ëŠ” ê²½ìš°ê°€ ë§ì•˜ìŠµë‹ˆë‹¤.

<br>

**But all that changed in 2012 when AlexNet won the ImageNet competition by a large margin using a technique called deep learning.**  
ê·¸ëŸ¬ë‚˜ 2012ë…„ AlexNetì´ Deep Learningì´ë¼ëŠ” ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬ ImageNet ê²½ìŸì—ì„œ í° ì°¨ì´ë¡œ ìš°ìŠ¹í•˜ë©´ì„œ ëª¨ë“  ê²ƒì´ ë°”ë€Œì—ˆìŠµë‹ˆë‹¤.

<br>

**Deep learning methods, such as convolutional neural networks, use a cascade of multiple layers of processing units for feature extraction and transformation.**  
convolutional  neural networkê³¼ ê°™ì€ Deep Learning ë°©ë²•ì€ feature extraction and transformationì„ ìœ„í•´ ì—¬ëŸ¬ ê³„ì¸µì˜ ì²˜ë¦¬ ì¥ì¹˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

<br>

**They learn multiple levels of representations that correspond to different levels of abstraction.**  
ë‹¤ì–‘í•œ ìˆ˜ì¤€ì˜ ì¶”ìƒí™”ì— í•´ë‹¹í•˜ëŠ” ì—¬ëŸ¬ ìˆ˜ì¤€ì˜ í‘œí˜„ì„ ë°°ì›ë‹ˆë‹¤.

<br>

**The levels form a hierarchy of concepts, showing strong invariance to the face pose, lighting, and expression changes, as shown in Fig. 2.**  
ë ˆë²¨ì€ ê°œë…ì˜ ê³„ì¸µ êµ¬ì¡°ë¥¼ í˜•ì„±í•˜ë©° ê·¸ë¦¼ 2ì™€ ê°™ì´ ì–¼êµ´ í¬ì¦ˆ, ì¡°ëª… ë° í‘œì • ë³€í™”ì— ê°•í•œ ë¶ˆë³€ì„±ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

<br>
<br>
<p align="center">
  <img src="/assets/DeepFaceRecognitionSurvey/Fig_02.png">
</p>
<br>
<br>

**It can be seen from the figure that the first layer of the deep neural network is somewhat similar to the Gabor feature found by human scientists with years of experience.**  
deep neural networkì˜ ì²« ë²ˆì§¸ ê³„ì¸µì€ ì˜¤ëœ ê²½í—˜ì„ ê°€ì§„ ì¸ê°„ ê³¼í•™ìë“¤ì´ ë°œê²¬í•œ Gabor ê¸°ëŠ¥ê³¼ ë‹¤ì†Œ ìœ ì‚¬í•˜ë‹¤ëŠ” ê²ƒì„ ê·¸ë¦¼ì—ì„œ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<br>

**The second layer learns more complex texture features. The features of the third layer are more complex, and some simple structures have begun to appear, such as high-bridged nose and big eyes.**  
ë‘ ë²ˆì§¸ ë ˆì´ì–´ëŠ” ë” ë³µì¡í•œ í…ìŠ¤ì²˜ ê¸°ëŠ¥ì„ í•™ìŠµí•©ë‹ˆë‹¤. ì„¸ ë²ˆì§¸ ì¸µì˜ íŠ¹ì§•ì€ ë” ë³µì¡í•˜ê³  ë†’ì€ ì½§ëŒ€ì™€ í° ëˆˆê³¼ ê°™ì€ ë‹¨ìˆœí•œ êµ¬ì¡°ê°€ ë‚˜íƒ€ë‚˜ê¸° ì‹œì‘í–ˆìŠµë‹ˆë‹¤.

<br>

**In the fourth, the network output is enough to explain a certain facial attribute, which can make a special response to some clear abstract concepts such as smile, roar, and even blue eye.**  
ë„¤ ë²ˆì§¸ì—ì„œ Network ì¶œë ¥ì€ íŠ¹ì • ì–¼êµ´ ì†ì„±ì„ ì„¤ëª…í•˜ê¸°ì— ì¶©ë¶„í•˜ë©° ì´ëŠ” ë¯¸ì†Œ, í¬íš¨, íŒŒë€ ëˆˆê³¼ ê°™ì€ ì¼ë¶€ ëª…í™•í•œ ì¶”ìƒ ê°œë…ì— ëŒ€í•´ íŠ¹ë³„í•œ ë°˜ì‘ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<br>

**In conclusion, in deep convolutional neural networks (CNN), the lower layers automatically learn the features similar to Gabor and SIFT designed for years or even decades (such as initial layers in Fig. 2), and the higher layers further learn higher level abstraction.**  
ê²°ë¡ ì ìœ¼ë¡œ deep convolutional neural network(CNN)ì—ì„œ í•˜ìœ„ ê³„ì¸µì€ ìˆ˜ë…„ ë˜ëŠ” ìˆ˜ì‹­ë…„ ë™ì•ˆ ì„¤ê³„ëœ Gabor ë° SIFTì™€ ìœ ì‚¬í•œ ê¸°ëŠ¥(ì˜ˆ: ê·¸ë¦¼ 2ì˜ ì´ˆê¸° ê³„ì¸µ)ì„ ìë™ìœ¼ë¡œ í•™ìŠµí•˜ê³  ìƒìœ„ ê³„ì¸µì€ ìƒìœ„ ìˆ˜ì¤€ ì¶”ìƒí™”ë¥¼ ì¶”ê°€ë¡œ í•™ìŠµí•©ë‹ˆë‹¤. .

<br>

**Finally, the combination of these higher level abstraction represents facial identity with unprecedented stability.**  
ë§ˆì§€ë§‰ìœ¼ë¡œ ì´ëŸ¬í•œ ìƒìœ„ ìˆ˜ì¤€ ì¶”ìƒí™”ì˜ ì¡°í•©ì€ ì „ë¡€ ì—†ëŠ” ì•ˆì •ì„±ìœ¼ë¡œ ì–¼êµ´ì˜ ì •ì²´ì„±ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.    

<br>
<br>

**In 2014, DeepFace achieved the SOTA accuracy on the famous LFW benchmark, approaching human performance on the unconstrained condition for the first time (DeepFace: 97.35% vs. Human: 97.53%), by training a 9-layer model on 4 million facial images.**  
2014ë…„ì— DeepFaceëŠ” ìœ ëª…í•œ LFW benchmarkì—ì„œ SOTA ì •í™•ë„ë¥¼ ë‹¬ì„±í–ˆìœ¼ë©°, 400ë§Œ ê°œì˜ ì–¼êµ´ ì´ë¯¸ì§€ì— ëŒ€í•œ 9ê³„ì¸µ Modelì„ Trainí•˜ì—¬ ì œì•½ ì—†ëŠ” ì¡°ê±´ì—ì„œ ì²˜ìŒìœ¼ë¡œ ì¸ê°„ì˜ ì„±ëŠ¥ì— ì ‘ê·¼í–ˆìŠµë‹ˆë‹¤(DeepFace: 97.35% ëŒ€ ì¸ê°„: 97.53%). .

<br>

**Inspired by this work, research focus has shifted to deep-learning-based approaches,and the accuracy was dramatically boosted to above 99.80% in just three years.**  
ì´ ì‘ì—…ì— ì˜ê°ì„ ë°›ì•„ ì—°êµ¬ ì´ˆì ì€ Deep Learning ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ìœ¼ë¡œ ì „í™˜ë˜ì—ˆìœ¼ë©° ì •í™•ë„ëŠ” ë¶ˆê³¼ 3ë…„ ë§Œì— 99.80% ì´ìƒìœ¼ë¡œ í¬ê²Œ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.

<br>

**Deep learning technique has reshaped the research landscape of FR in almost all aspects such as algorithm designs, training/test datasets, application scenarios and even the evaluation protocols.**  
Deep Learning ê¸°ìˆ ì€ ì•Œê³ ë¦¬ì¦˜ ì„¤ê³„, training/test datasets, application scenarios ë° Evaluation protocolê³¼ ê°™ì€ ê±°ì˜ ëª¨ë“  ì¸¡ë©´ì—ì„œ FRì˜ ì—°êµ¬ í™˜ê²½ì„ ì¬êµ¬ì„±í–ˆìŠµë‹ˆë‹¤.

<br>

**Therefore, it is of great significance to review the breakthrough and rapid development process in recent years.**  
ë”°ë¼ì„œ ìµœê·¼ ëª‡ ë…„ê°„ì˜ í˜ì‹ ì ì´ê³  ê¸‰ì†í•œ ë°œì „ ê³¼ì •ì„ ê²€í† í•˜ëŠ” ê²ƒì€ í° ì˜ë¯¸ê°€ ìˆìŠµë‹ˆë‹¤.

<br>

**There have been several surveys on FR and its subdomains, and they mostly summarized and compared a diverse set of techniques related to a specific FR scene, such as illumination-invariant FR, 3D FR, pose-invariant FR.**  
FRê³¼ ê·¸ í•˜ìœ„ ì˜ì—­ì— ëŒ€í•œ ì—¬ëŸ¬ ì¡°ì‚¬ê°€ ìˆì—ˆê³ , ê·¸ë“¤ì€ ì£¼ë¡œ ì¡°ëª… ë¶ˆë³€ FR, 3D FR, í¬ì¦ˆ ë¶ˆë³€ FRê³¼ ê°™ì€ íŠ¹ì • FR ì¥ë©´ê³¼ ê´€ë ¨ëœ ë‹¤ì–‘í•œ ê¸°ìˆ  ì„¸íŠ¸ë¥¼ ìš”ì•½í•˜ê³  ë¹„êµí–ˆìŠµë‹ˆë‹¤.

<br>

**Unfortunately, due to their earlier publication dates, none of them covered the deep learning methodology that is most successful nowadays.**  
ë¶ˆí–‰í•˜ê²Œë„, ê·¸ë“¤ì˜ ë°œí‘œ ë‚ ì§œê°€ ë” ë¹¨ëê¸° ë•Œë¬¸ì— ê·¸ë“¤ ì¤‘ ëˆ„êµ¬ë„ ì˜¤ëŠ˜ë‚  ê°€ì¥ ì„±ê³µì ì¸ Deep Learning ë°©ë²•ë¡ ì„ ë‹¤ë£¨ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.

<br>

**This survey focuses only on recognition problem.**  
ì´ ì¡°ì‚¬ëŠ” ì¸ì‹ ë¬¸ì œì—ë§Œ ì´ˆì ì„ ë§ì¶”ê³  ìˆë‹¤.

<br>

**one can refer to Ranjan et al. for a brief review of a full deep FR pipeline with detection and alignment, or refer to Jin et al. for a survey of face alignment.**  
brief review of a full deep FR pipeline with detection and alignmentì— ê´€ë ¨ëœ ë‚´ìš©ì€ Ranjan et al.ì„ ì°¸ê³ í•˜ë©´ ë˜ê³ , survey of face alignment ê´€ë ¨ ë‚´ìš©ì€ Jin et al.ì„ ì°¸ê³ í•˜ë©´ ëœë‹¤.

<br>

**Specifically, the major contributions of this survey are as follows:**  
êµ¬ì²´ì ìœ¼ë¡œ ì´ ì¡°ì‚¬ì˜ ì£¼ìš” ê¸°ì—¬ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

<br>

**â€¢ A systematic review on the evolution of the network architectures and loss functions for deep FR is provided.**  
â€¢ ì‹¬ì¸µ FRì— ëŒ€í•œ Network Architecture ë° loss functionsì˜ ì§„í™”ì— ëŒ€í•œ ì²´ê³„ì ì¸ ê²€í† ê°€ ì œê³µë©ë‹ˆë‹¤.

<br>

**Various loss functions are categorized into Euclideandistance-based loss, angular/cosine-margin-based loss and softmax loss and its variations.**  
ë‹¤ì–‘í•œ Loss FunctionëŠ” Euclideandistance-based loss, angular/cosine-margin-based loss ë° softmax loss and its variationsìœ¼ë¡œ ë¶„ë¥˜ë©ë‹ˆë‹¤.

<br>

**Both the mainstream network architectures, such as Deepface, DeepID series, VGGFace, FaceNet, and VGGFace2, and other architectures designed for FR are covered.**  
Deepface, DeepID ì‹œë¦¬ì¦ˆ, VGGFace, FaceNet ë° VGGFace2ì™€ ê°™ì€ ì£¼ë¥˜ Network Architectureì™€ FRìš©ìœ¼ë¡œ ì„¤ê³„ëœ ê¸°íƒ€ Architectureë¥¼ ëª¨ë‘ ë‹¤ë£¹ë‹ˆë‹¤.

<br>

**â€¢ We categorize the new face processing methods based on deep learning, such as those used to handle recognition difficulty on pose changes, into two classes: â€œone-tomany augmentationâ€ and â€œmany-to-one normalizationâ€, and discuss how emerging generative adversarial network(GAN) facilitates deep FR.**  
â€¢ í¬ì¦ˆ ë³€í™”ì— ëŒ€í•œ ì¸ì‹ ì–´ë ¤ì›€ì„ ì²˜ë¦¬í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ê²ƒê³¼ ê°™ì€ Deep Learningì— ê¸°ë°˜í•œ ìƒˆë¡œìš´ ì–¼êµ´ ì²˜ë¦¬ ë°©ë²•ì„ "one-tomany augmentation"ì™€ "many-to-one normalization"ì˜ ë‘ ê°€ì§€ í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜í•˜ê³ , generative adversarial network(GAN)ì´ ì–´ë–»ê²Œ deep FRì„ ìš©ì´í•˜ê²Œ í•˜ëŠ”ì§€ë„ ì•Œì•„ë´…ë‹ˆë‹¤.

<br>

**â€¢ We present a comparison and analysis on public available databases that are of vital importance for both model training and testing.**  
â€¢ Model training and testing ëª¨ë‘ì— ë§¤ìš° ì¤‘ìš”í•œ ê³µê°œ Databaseì— ëŒ€í•œ ë¹„êµ ë° ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.

<br>

**Major FR benchmarks, such as LFW, IJB-A/B/C, Megaface, and MSCeleb-1M, are reviewed and compared, in term of the four aspects: training methodology, evaluation tasks and metrics, and recognition scenes, which provides an useful reference for training and testing deep FR.**  
LFW, IJB-A/B/C, Megaface ë° MSCeleb-1Mê³¼ ê°™ì€ ì£¼ìš” FR benchmarkë¥¼ training methodology, Evaluation Task ë° metrics, recognition scenesì˜ ë„¤ ê°€ì§€ ì¸¡ë©´ì—ì„œ ê²€í† í•˜ê³  ë¹„êµí•©ë‹ˆë‹¤. Deep FR Train ë° í…ŒìŠ¤íŠ¸ì— ìœ ìš©í•œ ì°¸ê³  ìë£Œì…ë‹ˆë‹¤.

<br>

**â€¢ Besides the general purpose tasks defined by the major databases, we summarize a dozen scenario-specific databases and solutions that are still challenging for deep learning, such as anti-attack, cross-pose FR, and cross-age FR.**  
â€¢ ì£¼ìš” Databaseì—ì„œ ì •ì˜í•œ ë²”ìš© ì‘ì—… ì™¸ì—ë„ anti-attack, cross-pose FR ë° cross-age FRê³¼ ê°™ì´ Deep Learningì— ì—¬ì „íˆ ë„ì „ì ì¸ 12ê°œì˜ ì‹œë‚˜ë¦¬ì˜¤ë³„ Database ë° ì†”ë£¨ì…˜ì„ ìš”ì•½í•©ë‹ˆë‹¤.

<br>

**By reviewing specially designed methods for these unsolved problems, we attempt to reveal the important issues for future research on deep FR, such as adversarial samples, algorithm/data biases, and model interpretability.**  
ì´ëŸ¬í•œ ë¯¸í•´ê²° ë¬¸ì œë¥¼ ìœ„í•´ íŠ¹ë³„íˆ ì„¤ê³„ëœ ë°©ë²•ì„ ê²€í† í•¨ìœ¼ë¡œì¨ ìš°ë¦¬ëŠ” adversarial samples, algorithm/data biases ê·¸ë¦¬ê³  model interpretabilityê³¼ ê°™ì€ deep FRì— ëŒ€í•œ í–¥í›„ ì—°êµ¬ì˜ ì¤‘ìš”í•œ ë¬¸ì œë¥¼ ë°íˆë ¤ê³  ì‹œë„í•©ë‹ˆë‹¤.

<br>

**The remainder of this survey is structured as follows.**  
ì´ ì¡°ì‚¬ì˜ ë‚˜ë¨¸ì§€ ë¶€ë¶„ì€ ë‹¤ìŒê³¼ ê°™ì´ êµ¬ì„±ë©ë‹ˆë‹¤.

<br>

**In Section II, we introduce some background concepts and terminologies, and then we briefly introduce each component of FR.**  
IIì¥ì—ì„œëŠ” ëª‡ ê°€ì§€ background conceptsê³¼ terminologiesë¥¼ ì†Œê°œí•˜ê³  FRì˜ ê° êµ¬ì„± ìš”ì†Œë¥¼ ê°„ëµí•˜ê²Œ ì†Œê°œí•©ë‹ˆë‹¤.

<br>

**In Section III, different network architectures and loss functions are presented. Then, we summarize the face processing algorithms and the datasets.**  
ì„¹ì…˜ IIIì—ì„œëŠ” ë‹¤ì–‘í•œ Network Architectureì™€ Loss Functionì„ ì†Œê°œí•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ face processing algorithmsê³¼ datasetsë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.

<br>

**In Section V, we briefly introduce several methods of deep FR used for different scenes.**  
ì„¹ì…˜ Vì—ì„œëŠ” ë‹¤ì–‘í•œ scenesì— ì‚¬ìš©ë˜ëŠ” ëª‡ ê°€ì§€ deep FR ë°©ë²•ì„ ê°„ëµí•˜ê²Œ ì†Œê°œí•©ë‹ˆë‹¤.

<br>

**Finally, the conclusion of this paper and discussion of future works are presented in Section VI.**  
ë§ˆì§€ë§‰ìœ¼ë¡œ ë³¸ ë…¼ë¬¸ì˜ ê²°ë¡ ê³¼ í–¥í›„ ì—°êµ¬ì— ëŒ€í•œ ë…¼ì˜ëŠ” â…¥ì¥ì—ì„œ ì œì‹œí•œë‹¤.  

<br>

## II. OVERVIEW  

<br>
<br>
<br>
<br>

### A. Components of Face Recognition

**As mentioned in [32], there are three modules needed for FR system, as shown in Fig. 3.**  
Fig. 3ì— ë³´ì—¬ì§€ëŠ” ê²ƒì²˜ëŸ¼ FR ì‹œìŠ¤í…œì—ëŠ” ì„¸ ê°€ì§€ Moduleì´ í•„ìš”í•©ë‹ˆë‹¤.

<br>
<br>
<p align="center">
  <img src="/assets/DeepFaceRecognitionSurvey/Fig_03.png">
</p>
<br>
<br>

**First, a face detector is used to localize faces in images or videos.**  
ì²«ì§¸, face detectorëŠ” ì´ë¯¸ì§€ë‚˜ ë¹„ë””ì˜¤ì—ì„œ ì–¼êµ´ì„ ì°¾ì•„ëƒ…ë‹ˆë‹¤.

<br>

**Second, with the facial landmark detector, the faces are aligned to normalized canonical coordinates.**  
ë‘˜ì§¸, facial landmark detectorë¥¼ í†µí•´ normalized canonical coordinatesë¡œ ì •ë ¬ë©ë‹ˆë‹¤.

<br>

**Third, the FR module is implemented with these aligned face images.**  
ì…‹ì§¸, ì´ aligned ì–¼êµ´ ì´ë¯¸ì§€ë“¤ë¡œ FR Moduleì´ êµ¬í˜„ë©ë‹ˆë‹¤.

<br>

**We only focus on the FR module throughout the remainder of this paper.**  
ì´ ë…¼ë¬¸ì˜ ë‚˜ë¨¸ì§€ ë¶€ë¶„ì—ì„œëŠ” FR Moduleì—ë§Œ ì§‘ì¤‘í•©ë‹ˆë‹¤.

<br>

**Before a face image is fed to an FR module, face antispoofing, which recognizes whether the face is live or spoofed, is applied to avoid different types of attacks.**  
ì–¼êµ´ ì´ë¯¸ì§€ê°€ FR Moduleë¡œ ì „ë‹¬ë˜ê¸° ì „ì—, ì–¼êµ´ì´ ì‹¤ì œì¸ì§€ ê°€ì§œì¸ì§€ë¥¼ ì¸ì‹í•˜ëŠ” face antispoofingì´ ì ìš©ë˜ì–´ ë‹¤ì–‘í•œ ìœ í˜•ì˜ ê³µê²©ì„ ë°©ì§€í•©ë‹ˆë‹¤.

<br>

**Then, recognition can be performed.**  
ê·¸ëŸ° ë‹¤ìŒ, ì¸ì‹ì´ ìˆ˜í–‰ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<br>

**As shown in Fig. 3(c), an FR module consists of face processing, deep feature extraction and face matching, and it can be described as follows:**  
Fig. 3(c)ì—ì„œ ë³´ì—¬ì§€ëŠ” ê²ƒì²˜ëŸ¼, FR Moduleì€ ì–¼êµ´ ì²˜ë¦¬, Deep feature extraction, face matchingìœ¼ë¡œ êµ¬ì„±ë˜ë©°, ë‹¤ìŒê³¼ ê°™ì´ ì„¤ëª…ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤:  

<br>
<br>
<p align="center">
  <img src="/assets/DeepFaceRecognitionSurvey/Formula_01.png">
</p>
<br>
<br>

**where Ii and Ij are two face images, respectively.**  
Iiì™€ IjëŠ” ê°ê° ë‘ ì–¼êµ´ ì´ë¯¸ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

<br>

**P stands for face processing to handle intra-personal variations before training and testing, such as poses, illuminations, expressions and occlusions.**  
PëŠ” í•™ìŠµê³¼ í…ŒìŠ¤íŠ¸ ì „ì— ìì„¸, ì¡°ëª…, í‘œì • ë° ê°€ë¦¼ í˜„ìƒê³¼ ê°™ì€ ê°œì¸ ë‚´ ë³€ë™ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ì–¼êµ´ ì²˜ë¦¬ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.

<br>

**F denotes feature extraction, which encodes the identity information.**  
FëŠ” ì‹ ì› ì •ë³´ë¥¼ Encodingí•˜ëŠ” feature extractionì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

<br>

**The feature extractor is learned by loss functions when training, and is utilized to extract features of faces when testing.**  
feature extractorëŠ” í•™ìŠµ ì‹œ Loss Functionë¥¼ í†µí•´ í•™ìŠµë˜ë©°, í…ŒìŠ¤íŠ¸ ì‹œ features of facesë¥¼ ì¶”ì¶œí•˜ëŠ”ë° ì‚¬ìš©ë©ë‹ˆë‹¤.

<br>

**M means a face matching algorithm used to compute similarity scores of features to determine the specific identity of faces.**  
Mì€ ì–¼êµ´ì˜ íŠ¹ì • ì‹ ì›ì„ ê²°ì •í•˜ê¸° ìœ„í•´ í”¼ì²˜ì˜ similarity scoresë¥¼ ê³„ì‚°í•˜ëŠ”ë° ì‚¬ìš©ë˜ëŠ” face matching algorithmì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

<br>

**Different from object classification, the testing identities are usually disjoint from the training data in FR, which makes the learned classifier cannot be used to recognize testing faces.**  
object classificationì™€ ë‹¬ë¦¬, ì–¼êµ´ ì¸ì‹(FR)ì—ì„œ í…ŒìŠ¤íŠ¸ ì‹ ì›ì€ ëŒ€ê°œ í•™ìŠµ ë°ì´í„°ì™€ ë¶„ë¦¬ë˜ì–´ ìˆì–´, í•™ìŠµëœ classifierëŠ” í…ŒìŠ¤íŠ¸ ì–¼êµ´ì„ ì¸ì‹í•˜ëŠ”ë° ì‚¬ìš©ë  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

<br>

**Therefore, face matching algorithm is an essential part in FR.**  
ë”°ë¼ì„œ, face matching algorithmì€ FRì—ì„œ í•„ìˆ˜ì ì¸ ë¶€ë¶„ì…ë‹ˆë‹¤.  

<br>
<br>
<br>

## 1) Face Processing  

<br>
<br>

**Although deep-learning-based approaches have been widely used, Mehdipour et al. [46] proved that various conditions, such as poses, illuminations, expressions and occlusions, still affect the performance of deep FR.**  
Deep Learning ê¸°ë°˜ì˜ ì ‘ê·¼ë²•ë“¤ì´ ë„ë¦¬ ì‚¬ìš©ë˜ê³  ìˆì§€ë§Œ, Mehdipour ë“±[46]ì€ í¬ì¦ˆ, ì¡°ëª…, í‘œì •, ê°€ë¦¼ ë“± ë‹¤ì–‘í•œ ì¡°ê±´ë“¤ì´ ì—¬ì „íˆ Deep FRì˜ ì„±ëŠ¥ì— ì˜í–¥ì„ ë¯¸ì¹œë‹¤ëŠ” ê²ƒì„ ì¦ëª…í–ˆìŠµë‹ˆë‹¤.

<br>

**Accordingly, face processing is introduced to address this problem.**  
ë”°ë¼ì„œ, ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ face processingì´ ë„ì…ë˜ì—ˆìŠµë‹ˆë‹¤.

<br>

**The face processing methods are categorized as â€œone-to-many augmentationâ€ and â€œmany-to-one normalizationâ€, as shown in Table I.**  
face processing methodë“¤ì€ Table Iì— ë³´ì—¬ì§„ ê²ƒê³¼ ê°™ì´ "one-to-many augmentation"ê³¼ "many-to-one normalization"ë¡œ ë¶„ë¥˜ë©ë‹ˆë‹¤.

<br>

<br>
<br>
<p align="center">
  <img src="/assets/DeepFaceRecognitionSurvey/Table_01.png">
</p>
<br>
<br>

**â€¢ â€œOne-to-many augmentationâ€. These methods generate many patches or images of the pose variability from a single image to enable deep networks to learn poseinvariant representations.**  
â€¢ "One-to-many augmentation". ì´ ë°©ë²•ë“¤ì€ single imageì—ì„œ í¬ì¦ˆì˜ ë³€ë™ì„±ì— ëŒ€í•œ ë§ì€ íŒ¨ì¹˜ë‚˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ì—¬ Deep Networkê°€ í¬ì¦ˆì— ë¶ˆë³€í•œ í‘œí˜„ì„ í•™ìŠµí•˜ë„ë¡ í•©ë‹ˆë‹¤.

<br>

**â€¢ â€œMany-to-one normalizationâ€. These methods recover the canonical view of face images from one or many images of a nonfrontal view; then, FR can be performed as if it were under controlled conditions.**  
â€¢ "Many-to-one normalization". ì´ ë°©ë²•ë“¤ì€ í•˜ë‚˜ ë˜ëŠ” ë§ì€ ìˆ˜ì˜ ë¹„ì •ë©´ ì´ë¯¸ì§€(nonfrontal view)ë“¤ë¡œë¶€í„° ì–¼êµ´ ì´ë¯¸ì§€ì˜ ì •ê·œí™”ëœ ë·°(canonical view of face images)ë¥¼ ë³µêµ¬í•˜ê³ , ê·¸ëŸ° ë‹¤ìŒì— FRì„ ì œì–´ëœ ì¡°ê±´í•˜ì—ì„œ ìˆ˜í–‰ëœ ê²ƒì²˜ëŸ¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<br>

**Note that we mainly focus on deep face processing method designed for pose variations in this paper, since pose is widely regarded as a major challenge in automatic FR applications and other variations can be solved by the similar methods.**  
ì£¼ì˜í•  ì ì€ ìš°ë¦¬ê°€ ë³¸ ë…¼ë¬¸ì—ì„œ ì£¼ë¡œ í¬ì¦ˆ ë³€ë™ì„±ì— ëŒ€í•´ ì„¤ê³„ëœ Deep face processing ë°©ë²•ì— ì§‘ì¤‘í•œë‹¤ëŠ” ê²ƒì¸ë°, ì´ëŠ” í¬ì¦ˆê°€ ìë™ FR ì‘ìš© í”„ë¡œê·¸ë¨ì—ì„œ ì£¼ìš” ë„ì „ ê³¼ì œë¡œ ë„ë¦¬ ì¸ì‹ë˜ê³  ìˆìœ¼ë©°, ë‹¤ë¥¸ ë³€ë™ì„±ë“¤ì€ ë¹„ìŠ·í•œ ë°©ë²•ë“¤ë¡œ í•´ê²°ë  ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.      

<br>
<br>
<br>


## 2) Deep Feature Extraction: Network Architecture. 

<br>
<br>

**The architectures can be categorized as backbone and assembled networks, as shown in Table II.**  
Architectureë“¤ì€ í‘œ IIì— í‘œì‹œëœ ëŒ€ë¡œ backbone ë° assembled Networkë¡œ ë¶„ë¥˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<br>

<br>
<br>
<p align="center">
  <img src="/assets/DeepFaceRecognitionSurvey/Table_02.png">
</p>
<br>
<br>

**Inspired by the extraordinary success on the ImageNet [74] challenge, the typical CNN architectures, e.g. AlexNet, VGGNet, GoogleNet, ResNet and SENet, are introduced and widely used as the baseline models in FR (directly or slightly modified).**  
ImageNet [74] ì±Œë¦°ì§€ì˜ ë†€ë¼ìš´ ì„±ê³µì— ì˜ê°ì„ ë°›ì•„ ì „í˜•ì ì¸ CNN Architecture, ì˜ˆë¥¼ ë“¤ì–´ AlexNet, VGGNet, GoogleNet, ResNet ë° SENetì´ ë„ì…ë˜ì–´ FRì˜ ê¸°ë³¸ Modelë¡œ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤(ì§ì ‘ ë˜ëŠ” ì•½ê°„ ìˆ˜ì •ë¨).

<br>

**In addition to the mainstream, some assembled networks, e.g. multi-task networks and multi-input networks, are utilized in FR.**  
ì£¼ë¥˜ ì™¸ì—ë„ ì¼ë¶€ assembled networks, ì˜ˆë¥¼ ë“¤ì–´, multi-task networks ë° multi-input networksëŠ” FRì—ì„œ í™œìš©ë©ë‹ˆë‹¤. 

<br>

**Hu et al. shows that accumulating the results of assembled networks provides an increase in performance compared with an individual network.**  
Huet al.ì€ assembled networkì˜ ê²°ê³¼ë¥¼ ëˆ„ì í•˜ë©´ ê°œë³„ Networkì— ë¹„í•´ ì„±ëŠ¥ì´ í–¥ìƒë¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

<br>
<br>
<br>

## Loss Function 

<br>
<br>

**The softmax loss is commonly used as the supervision signal in object recognition, and it encourages the separability of features.**  
softmax lossëŠ” ì¼ë°˜ì ìœ¼ë¡œ ê°ì²´ ì¸ì‹ì—ì„œ supervision signalë¡œ ì‚¬ìš©ë˜ë©° íŠ¹ì§•ì˜ ë¶„ë¦¬ì„±ì„ ì¥ë ¤í•©ë‹ˆë‹¤.

<br>

**However, the softmax loss is not sufficiently effective for FR because intra-variations could be larger than inter-differences and more discriminative features are required when recognizing different people.**  
ê·¸ëŸ¬ë‚˜ softmax lossëŠ” inter-differenceë³´ë‹¤ intra-variationì´ í´ ìˆ˜ ìˆê³  ë‹¤ë¥¸ ì‚¬ëŒì„ ì¸ì‹í•  ë•Œ ë” ë§ì€ discriminative íŠ¹ì§•ì´ í•„ìš”í•˜ê¸° ë•Œë¬¸ì— FRì— ëŒ€í•´ ì¶©ë¶„íˆ íš¨ê³¼ì ì´ì§€ ì•ŠìŠµë‹ˆë‹¤.

<br>

**Many works focus on creating novel loss functions to make features not only more separable but also discriminative, as shown in Table III.**  
ë§ì€ ì‘ì—…ì€ í‘œ IIIì— ë‚˜ì™€ ìˆëŠ” ê²ƒì²˜ëŸ¼ featuresë¥¼ ë” ë¶„ë¦¬ ê°€ëŠ¥í•  ë¿ë§Œ ì•„ë‹ˆë¼ ì°¨ë³„ì ìœ¼ë¡œ ë§Œë“¤ê¸° ìœ„í•´ ìƒˆë¡œìš´ ì†ì‹¤ ê¸°ëŠ¥ì„ ë§Œë“œëŠ” ë° ì¤‘ì ì„ ë‘¡ë‹ˆë‹¤.    

<br>
<br>
<br>
<br>
<p align="center">
  <img src="/assets/DeepFaceRecognitionSurvey/Table_03.png">
</p>
<br>
<br>

## 3) Face Matching by Deep Features 

<br>
<br>
<br>

**FR can be categorized as face verification and face identification.**  
FRì€ face verificationê³¼ face identificationë¡œ ë¶„ë¥˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<br>

**In either scenario, a set of known subjects is initially enrolled in the system (the gallery), and during testing, a new subject (the probe) is presented.**  
ë‘ ì‹œë‚˜ë¦¬ì˜¤ ëª¨ë‘ set of known subjectsê°€ ì²˜ìŒì— ì‹œìŠ¤í…œ(Gallery)ì— ë“±ë¡ë˜ê³  í…ŒìŠ¤íŠ¸ ì¤‘ì— new subject (the probe)ê°€ í‘œì‹œë©ë‹ˆë‹¤.

<br>

**After the deep networks are trained on massive data with the supervision of an appropriate loss function, each of the test images is passed through the networks to obtain a deep feature representation.**  
Deep Networkê°€ ì ì ˆí•œ Loss Functionì˜ ê°ë… í•˜ì— ë°©ëŒ€í•œ ë°ì´í„°ì— ëŒ€í•´ í•™ìŠµëœ í›„ ê° í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ê°€ Networkë¥¼ í†µê³¼í•˜ì—¬ deep feature representationì„ ì–»ìŠµë‹ˆë‹¤.

<br>

**Using cosine distance or L2 distance, face verification computes one-to-one similarity between the gallery and probe to determine whether the two images are of the same subject, whereas face identification computes one-to-many similarity to determine the specific identity of a probe face.**  
cosine distance or L2 distanceë¥¼ ì‚¬ìš©í•˜ì—¬ ì–¼êµ´ í™•ì¸(face verification)ì€ galleryì™€ probe ê°„ì˜ one-to-one similarityì„ ê³„ì‚°í•˜ì—¬ ë‘ ì´ë¯¸ì§€ê°€ ë™ì¼í•œ ëŒ€ìƒì¸ì§€ ì—¬ë¶€ë¥¼ ê²°ì •í•˜ëŠ” ë°˜ë©´, ì–¼êµ´ ì‹ë³„(face identification)ì€ one-to-many similarityì„ ê³„ì‚°í•˜ì—¬ specific identity of a probe faceë¥¼ ê²°ì •í•©ë‹ˆë‹¤.

<br>

**In addition to these, other methods are introduced to postprocess the deep features such that the face matching is performed efficiently and accurately, such as metric learning, sparse-representation-based classifier (SRC), and so forth.**  
ì´ ì™¸ì—ë„ metric learning, sparse-representation-based classifier(SRC) ë“±ê³¼ ê°™ì´ ì–¼êµ´ ë§¤ì¹­ì´ íš¨ìœ¨ì ì´ê³  ì •í™•í•˜ê²Œ ìˆ˜í–‰ë˜ë„ë¡ deep featuresì„ postprocessí•˜ëŠ” ë‹¤ë¥¸ ë°©ë²•ì´ ë„ì…ë©ë‹ˆë‹¤.

<br>

**To sum up, we present FR modules and their commonlyused methods in Fig. 4 to help readers to get a view of the whole FR.**  
ìš”ì•½í•˜ë©´ ë…ìê°€ ì „ì²´ FRì„ ë³¼ ìˆ˜ ìˆë„ë¡ ê·¸ë¦¼ 4ì— FR Moduleê³¼ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤.

<br>
<br>
<p align="center">
  <img src="/assets/DeepFaceRecognitionSurvey/Fig_04.png">
</p>
<br>
<br>

**In deep FR, various training and testing face databases are constructed, and different architectures and losses of deep FR always follow those of deep object classification and are modified according to unique characteristics of FR.**  
deep FRì—ì„œëŠ” ë‹¤ì–‘í•œ Train ë° í…ŒìŠ¤íŠ¸ ì–¼êµ´ Databaseê°€ êµ¬ì¶•ë˜ë©° deep FRì˜ ë‹¤ì–‘í•œ Architectureì™€ ì†ì‹¤ì€ í•­ìƒ deep object classificationì„ ë”°ë¥´ê³  FRì˜ ê³ ìœ í•œ íŠ¹ì„±ì— ë”°ë¼ ìˆ˜ì •ë©ë‹ˆë‹¤.

<br>

**Moreover, in order to address unconstrained facial changes, face processing methods are further designed to handle poses, expressions and occlusions variations.**  
ë˜í•œ ì œì•½ ì—†ëŠ” ì–¼êµ´ ë³€í™”ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ í¬ì¦ˆ, í‘œì • ë° íìƒ‰ ë³€í™”ë¥¼ ì²˜ë¦¬í•˜ë„ë¡ ì–¼êµ´ ì²˜ë¦¬ ë°©ë²•ì´ ì¶”ê°€ë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.

<br>

**Benefiting from these strategies, deep FR system significantly improves the SOTA and surpasses human performance.**  
ì´ëŸ¬í•œ ì „ëµì˜ ì´ì ì„ í™œìš©í•˜ì—¬ Deep FR ì‹œìŠ¤í…œì€ SOTAë¥¼ í¬ê²Œ ê°œì„ í•˜ê³  ì¸ê°„ì„ ëŠ¥ê°€í•©ë‹ˆë‹¤.

<br>

**When the applications of FR becomes more and more mature in general scenario, recently, different solutions are driven for more difficult specific scenarios, such as cross-pose FR, crossage FR, video FR.**  
ì¼ë°˜ì ì¸ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ FRì˜ ì ìš©ì´ ì ì  ë” ì„±ìˆ™í•´ì§ì— ë”°ë¼ ìµœê·¼ì—ëŠ” êµì°¨ í¬ì¦ˆ FR, êµì°¨ FR, ë¹„ë””ì˜¤ FRê³¼ ê°™ì´ ë” ì–´ë ¤ìš´ íŠ¹ì • ì‹œë‚˜ë¦¬ì˜¤ì— ëŒ€í•´ ì„œë¡œ ë‹¤ë¥¸ ì†”ë£¨ì…˜ì´ êµ¬ë™ë©ë‹ˆë‹¤.      

<br>
<br>
<br>
<br>

## III. NETWORK ARCHITECTURE AND TRAINING LOSS

<br>
<br>
<br>

**For most applications, it is difficult to include the candidate faces during the training stage, which makes FR become a â€œzero-shotâ€ learning task.**  
ëŒ€ë¶€ë¶„ì˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ Train ë‹¨ê³„ ë™ì•ˆ í›„ë³´ ì–¼êµ´ì„ í¬í•¨í•˜ëŠ” ê²ƒì€ ì–´ë µê¸° ë•Œë¬¸ì— FRì€ "ì œë¡œ ìƒ·" í•™ìŠµ ì‘ì—…ì´ ë©ë‹ˆë‹¤.

<br>

**Fortunately, since all human faces share a similar shape and texture, the representation learned from a small proportion of faces can generalize well to the rest.**  
ë‹¤í–‰ìŠ¤ëŸ½ê²Œë„ ëª¨ë“  ì‚¬ëŒì˜ ì–¼êµ´ì€ ë¹„ìŠ·í•œ ëª¨ì–‘ê³¼ ì§ˆê°ì´ê¸° ë•Œë¬¸ì— ì–¼êµ´ì˜ ì‘ì€ ë¶€ë¶„ì—ì„œ í•™ìŠµëœ í‘œí˜„ì€ ë‚˜ë¨¸ì§€ ì–¼êµ´ì— ì˜ ì¼ë°˜í™”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<br>

**Based on this theory, a straightforward way to improve generalized performance is to include as many IDs as possible in the training set.**
ì´ ì´ë¡ ì— ë”°ë¼ ì¼ë°˜í™”ëœ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê°„ë‹¨í•œ ë°©ë²•ì€ Training Setì— ê°€ëŠ¥í•œ í•œ ë§ì€ IDë¥¼ í¬í•¨í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

<br>

**For example, Internet giants such as Facebook and Google have reported their deep FR system trained by 106 âˆ’ 107 IDs.**  
ì˜ˆë¥¼ ë“¤ì–´, Facebook ë° Googleê³¼ ê°™ì€ ê±°ëŒ€ ì¸í„°ë„· ê¸°ì—…ì€ 106 - 107 IDë¡œ Trainëœ Deep FR ì‹œìŠ¤í…œì„ ë³´ê³ í–ˆìŠµë‹ˆë‹¤.

<br>

**Unfortunately, these personal datasets, as well as prerequisite GPU clusters for distributed model training, are not accessible for academic community.**  
ì•ˆíƒ€ê¹ê²Œë„ ì´ëŸ¬í•œ ê°œì¸ Datasetì™€ ë¶„ì‚° Model êµìœ¡ì„ ìœ„í•œ í•„ìˆ˜ GPU í´ëŸ¬ìŠ¤í„°ëŠ” í•™ê³„ì—ì„œ ì•¡ì„¸ìŠ¤í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

<br>

**Currently, public available training databases for academic research consist of only 103âˆ’105 IDs.**  
í˜„ì¬ í•™ìˆ  ì—°êµ¬ë¥¼ ìœ„í•´ ê³µê°œì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ Train DatabaseëŠ” 103-105ê°œì˜ IDë¡œë§Œ êµ¬ì„±ë©ë‹ˆë‹¤.

<br>

**Instead, academic community makes effort to design effective loss functions and adopts efficient architectures to make deep features more discriminative using the relatively small training data sets.**  
ëŒ€ì‹  í•™ê³„ì—ì„œëŠ” íš¨ê³¼ì ì¸ Loss Functionë¥¼ ì„¤ê³„í•˜ê¸° ìœ„í•´ ë…¸ë ¥í•˜ê³  ìƒëŒ€ì ìœ¼ë¡œ ì‘ì€ Train Datasetë¥¼ ì‚¬ìš©í•˜ì—¬ ê¹Šì€ íŠ¹ì„±ì„ ë³´ë‹¤ ì‹ë³„í•  ìˆ˜ ìˆë„ë¡ íš¨ìœ¨ì ì¸ Architectureë¥¼ ì±„íƒí•©ë‹ˆë‹¤.

<br>

**For instance, the accuracy of most popular LFW benchmark has been boosted from 97% to above 99.8% in the pasting four years, as enumerated in Table IV.**  
ì˜ˆë¥¼ ë“¤ì–´, ê°€ì¥ ì¸ê¸° ìˆëŠ” LFW benchmarkì˜ ì •í™•ë„ëŠ” ì§€ë‚œ 4ë…„ ë™ì•ˆ 97%ì—ì„œ 99.8% ì´ìƒìœ¼ë¡œ í–¥ìƒë˜ì—ˆìœ¼ë©° í‘œ IVì— ì—´ê±°ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

<br>

**In this section, we survey the research efforts on different loss functions and network architectures that have significantly improved deep FR methods.**
ì´ ì„¹ì…˜ì—ì„œëŠ” Deep FR ë°©ë²•ì„ í¬ê²Œ ê°œì„ í•œ ë‹¤ì–‘í•œ loss functions ë° Network Architectureì— ëŒ€í•œ ì—°êµ¬ ë…¸ë ¥ì„ ì¡°ì‚¬í•©ë‹ˆë‹¤.  

<br>
<br>
<br>
<p align="center">
  <img src="/assets/DeepFaceRecognitionSurvey/Table_04.png">
</p>
<br>
<br>

### A. Evolution of Discriminative Loss Functions

<br>
<br>
<br>

**Inheriting from the object classification network such as AlexNet, the initial Deepface and DeepID adopted cross-entropy based softmax loss for feature learning.**  
AlexNetê³¼ ê°™ì€ ê°ì²´ ë¶„ë¥˜ Networkì—ì„œ ìƒì†ë°›ì€ ì´ˆê¸° Deepface ë° DeepIDëŠ” feature learningì„ ìœ„í•´ cross-entropy based softmax lossë¥¼ ì±„íƒí–ˆìŠµë‹ˆë‹¤.

<br>

**After that, people realized that the softmax loss is not sufficient by itself to learn discriminative features, and more researchers began to explore novel loss functions for enhanced generalization ability.**  
ê·¸ í›„ ì‚¬ëŒë“¤ì€ softmax lossë§Œìœ¼ë¡œëŠ” íŒë³„ ê¸°ëŠ¥ì„ í•™ìŠµí•˜ê¸°ì— ì¶©ë¶„í•˜ì§€ ì•Šë‹¤ëŠ” ê²ƒì„ ê¹¨ë‹¬ì•˜ê³  ë” ë§ì€ ì—°êµ¬ìë“¤ì´ ì¼ë°˜í™” ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ìƒˆë¡œìš´ Loss Functionë¥¼ íƒìƒ‰í•˜ê¸° ì‹œì‘í–ˆìŠµë‹ˆë‹¤.

<br>

**This becomes the hottest research topic in deep FR research, as illustrated in Fig. 5.**
ì´ëŠ” ê·¸ë¦¼ 5ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ Deep FR ì—°êµ¬ì—ì„œ ê°€ì¥ ëœ¨ê±°ìš´ ì—°êµ¬ ì£¼ì œê°€ ë©ë‹ˆë‹¤.

<br>
<br>
<p align="center">
  <img src="/assets/DeepFaceRecognitionSurvey/Fig_05.png">
</p>
<br>
<br>

**Before 2017, Euclidean-distance-based loss played an important role; In 2017, angular/cosine-margin-based loss as well as feature and weight normalization became popular.**  
2017ë…„ ì´ì „ì—ëŠ” Euclidean-distance-based lossê°€ ì¤‘ìš”í•œ ì—­í• ì„ í–ˆìŠµë‹ˆë‹¤. 2017ë…„ì—ëŠ” angular/cosine-margin-based lossì™€ feature and weight normalizationê°€ ì¸ê¸°ë¥¼ ëŒì—ˆìŠµë‹ˆë‹¤.

<br>

**It should be noted that, although some loss functions share the similar basic idea, the new one is usually designed to facilitate the training procedure by easier parameter or sample selection.**  
ì¼ë¶€ Loss Functionì€ ìœ ì‚¬í•œ ê¸°ë³¸ ì•„ì´ë””ì–´ë¥¼ ê³µìœ í•˜ì§€ë§Œ ìƒˆë¡œìš´ Loss FunctionëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë” ì‰¬ìš´ ë§¤ê°œë³€ìˆ˜ ë˜ëŠ” ìƒ˜í”Œ ì„ íƒì„ í†µí•´ Train ì ˆì°¨ë¥¼ ìš©ì´í•˜ê²Œ í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.    

<br>
<br>
<br>
<br>

### 1) Euclidean-distance-based Loss  

<br>

**Euclidean-distancebased loss is a metric learning method that embeds images into Euclidean space in which intra-variance is reduced and inter-variance is enlarged.**  
Euclidean-distance-based lossëŠ” intra-varianceë¥¼ ì¤„ì´ê³  inter-varianceë¥¼ í™•ëŒ€í•œ Euclidean ê³µê°„ì— ì´ë¯¸ì§€ë¥¼ ì‚½ì…í•˜ëŠ” ë©”íŠ¸ë¦­ í•™ìŠµ ë°©ë²•ì…ë‹ˆë‹¤.

<br>

**The contrastive loss and the triplet loss are the commonly used loss functions.**  
Contrastive lossì™€ triplet lossëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” loss functionì´ë‹¤.

<br>

**The contrastive loss requires face image pairs, and then pulls together positive pairs and pushes apart negative pairs.**  
ëŒ€ì¡°ì ì¸ ì†ì‹¤ì—ëŠ” ì–¼êµ´ ì´ë¯¸ì§€ ìŒì´ í•„ìš”í•˜ë©° ì–‘ìˆ˜ ìŒì„ í•¨ê»˜ ë‹¹ê¸°ê³  ìŒìˆ˜ ìŒì„ ë°€ì–´ëƒ…ë‹ˆë‹¤.  

<br>
<br>
<p align="center">
  <img src="/assets/DeepFaceRecognitionSurvey/Formula_02.png">
</p>
<br>
<br>

**where yij = 1 means xi and xj are matching samples and yij = 0 means non-matching samples. f(Â·) is the feature embedding,  + and 
âˆ’ control the margins of the matching and non-matching pairs respectivel**  

<br>

**DeepID2 combined the face identification (softmax) and verification (contrastive loss) supervisory signals to learn a discriminative representation, and joint Bayesian (JB) was applied to obtain a robust embedding space.**  
DeepID2ëŠ” ì–¼êµ´ ì‹ë³„(softmax)ê³¼ ê²€ì¦(contrastive loss) ê°ë… ì‹ í˜¸ë¥¼ ê²°í•©í•˜ì—¬ ì°¨ë³„ì ì¸ í‘œí˜„ì„ í•™ìŠµí•˜ê³  joint Bayesian(JB)ì„ ì ìš©í•˜ì—¬ ê°•ë ¥í•œ ì„ë² ë”© ê³µê°„ì„ í™•ë³´í–ˆìŠµë‹ˆë‹¤.

<br>

**Extending from DeepID2, DeepID2+ increased the dimension of hidden representations and added supervision to early convolutional layers.**  
DeepID2ì—ì„œ í™•ì¥ëœ DeepID2+ëŠ” ìˆ¨ê²¨ì§„ í‘œí˜„ì˜ ì°¨ì›ì„ ë†’ì´ê³  ì´ˆê¸° convolutional ë ˆì´ì–´ì— ê°ë… ê¸°ëŠ¥ì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.

<br>

**DeepID3 further introduced VGGNet and GoogleNet to their work.**  
DeepID3ëŠ” VGGNetê³¼ GoogleNetì„ ì‘ì—…ì— ì¶”ê°€ë¡œ ë„ì…í–ˆìŠµë‹ˆë‹¤.

<br>

**However, the main problem with the contrastive loss is that the margin parameters are often difficult to choose.**  
ê·¸ëŸ¬ë‚˜ ëŒ€ì¡° ì†ì‹¤(contrastive loss )ì˜ ì£¼ìš” ë¬¸ì œëŠ” ë§ˆì§„ ë§¤ê°œë³€ìˆ˜(margin parameters )ë¥¼ ì„ íƒí•˜ê¸° ì–´ë ¤ìš´ ê²½ìš°ê°€ ë§ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.

<br>

**Contrary to contrastive loss that considers the absolute distances of the matching pairs and non-matching pairs, triplet loss considers the relative difference of the distances between them.**  
ì¼ì¹˜í•˜ëŠ” ìŒê³¼ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” ìŒì˜ ì ˆëŒ€ ê±°ë¦¬ë¥¼ ê³ ë ¤í•˜ëŠ” Contrastive lossì™€ ë‹¬ë¦¬ triplet lossëŠ” ê·¸ë“¤ ì‚¬ì´ì˜ ê±°ë¦¬ì˜ ìƒëŒ€ì ì¸ ì°¨ì´ë¥¼ ê³ ë ¤í•©ë‹ˆë‹¤.

<br>

**Along with FaceNet proposed by Google, Triplet loss was introduced into FR.**  
Googleì—ì„œ ì œì•ˆí•œ FaceNetê³¼ í•¨ê»˜ Triplet lossê°€ FRì— ë„ì…ë˜ì—ˆìŠµë‹ˆë‹¤.

<br>

**It requires the face triplets, and then it minimizes the distance between an anchor and a positive sample of the same identity and maximizes the distance between the anchor and a negative sample of a different identity.**  
ê·¸ê²ƒì€ ì–¼êµ´ ì‚¼ì¤‘í•­(face triplets)ì„ í•„ìš”ë¡œ í•˜ê³  ì•µì»¤ì™€ ë™ì¼í•œ ì‹ ì›ì˜ positive sample ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ìµœì†Œí™”í•˜ê³  ì•µì»¤ì™€ ë‹¤ë¥¸ ì‹ ì›ì˜ negative sample ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ìµœëŒ€í™”í•©ë‹ˆë‹¤.  

<br>
<br>  

**Inspired by FaceNet, TPE and TSE learned a linear projection W to construct triplet loss.**  
FaceNetì—ì„œ ì˜ê°ì„ ë°›ì€ TPEì™€ TSEëŠ” triplet lossì„ êµ¬ì„±í•˜ê¸° ìœ„í•´ ì„ í˜• í”„ë¡œì ì…˜ Wë¥¼ í•™ìŠµí–ˆìŠµë‹ˆë‹¤.

<br>

**Other methods optimize deep models using both triplet loss and softmax loss.**  
ë‹¤ë¥¸ ë°©ë²•ì€ triplet lossì™€ softmax lossë¥¼ ëª¨ë‘ ì‚¬ìš©í•˜ì—¬ deep Modelì„ ìµœì í™”í•©ë‹ˆë‹¤.

<br>

**They first train networks with softmax and then fine-tune them with triplet loss.**  
ê·¸ë“¤ì€ ë¨¼ì € softmaxë¡œ Networkë¥¼ Trainì‹œí‚¨ ë‹¤ìŒ triplet lossë¡œ ë¯¸ì„¸ ì¡°ì •í•©ë‹ˆë‹¤.

<br>
<br>

**However, the contrastive loss and triplet loss occasionally encounter training instability due to the selection of effective training samples, some paper begun to explore simple alternatives.**  
ê·¸ëŸ¬ë‚˜ Contrastive lossì™€ triplet lossëŠ” ë•Œë•Œë¡œ íš¨ê³¼ì ì¸ Train ìƒ˜í”Œì˜ ì„ íƒìœ¼ë¡œ ì¸í•´ Train ë¶ˆì•ˆì •ì„±ì— ì§ë©´í•˜ë©°, ì¼ë¶€ ë…¼ë¬¸ì—ì„œëŠ” ê°„ë‹¨í•œ ëŒ€ì•ˆì„ íƒìƒ‰í•˜ê¸° ì‹œì‘í–ˆìŠµë‹ˆë‹¤.

<br>

**Center loss and its variants are good choices for reducing intra-variance.**  
ì¤‘ì‹¬ ì†ì‹¤(Center loss )ê³¼ ê·¸ variantsì€ ë‚´ë¶€ ë¶„ì‚°(intra-variance)ì„ ì¤„ì´ê¸° ìœ„í•œ ì¢‹ì€ ì„ íƒì…ë‹ˆë‹¤.

<br>

**The center loss learned a center for each class and penalized the distances between the deep features and their corresponding class centers.**  
ì¤‘ì‹¬ ì†ì‹¤ì€ ê° í´ë˜ìŠ¤ì˜ ì¤‘ì‹¬ì„ í•™ìŠµí•˜ê³  deep featuresê³¼ í•´ë‹¹ class centers ì‚¬ì´ì˜ ê±°ë¦¬ì— ë¶ˆì´ìµì„ ì¤ë‹ˆë‹¤.

<br>

**This loss can be defined as follows:**  
ì´ ì†ì‹¤ì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  

<br>
<br>
<p align="center">
  <img src="/assets/DeepFaceRecognitionSurvey/Formula_03.png">
</p>
<br>
<br>

**where xi denotes the i-th deep feature belonging to the yi-th class and cyi denotes the yi-th class center of deep features.**  
ì—¬ê¸°ì„œ xiëŠ” yië²ˆì§¸ í´ë˜ìŠ¤ì— ì†í•˜ëŠ” ië²ˆì§¸ Deep featureë¥¼ ë‚˜íƒ€ë‚´ê³  cyiëŠ” Deep featuresì˜ yië²ˆì§¸ í´ë˜ìŠ¤ ì¤‘ì‹¬ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

<br>

**To handle the long-tailed data, a range loss, which is a variant of center loss, is used to minimize k greatest rangeâ€™s harmonic mean values in one class and maximize the shortest interclass distance within one batch.**  
long-tailed ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ í•˜ë‚˜ì˜ í´ë˜ìŠ¤ì—ì„œ k greatest rangeâ€™s harmonic mean ê°’ì„ ìµœì†Œí™”í•˜ê³  í•œ ë°°ì¹˜ ë‚´ì—ì„œ shortest interclass distanceë¥¼ ìµœëŒ€í™”í•˜ê¸° ìœ„í•´ ì¤‘ì‹¬ ì†ì‹¤ì˜ ë³€í˜•ì¸ ë²”ìœ„ ì†ì‹¤ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

<br>

**Wu et al. proposed a center-invariant loss that penalizes the difference between each center of classes.**  
Wuet al.ì€ í´ë˜ìŠ¤ì˜ ê° ì„¼í„° ì‚¬ì´ì˜ ì°¨ì´ì— í˜ë„í‹°ë¥¼ ì£¼ëŠ” center-invariant lossì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤.

<br>

**Deng et al. selected the farthest intraclass samples and the nearest inter-class samples to compute a margin loss.**  
Deng et al.ì€ ë§ˆì§„ ì†ì‹¤ì„ ê³„ì‚°í•˜ê¸° ìœ„í•´ ê°€ì¥ ë¨¼ ë‚´ë¶€ í´ë˜ìŠ¤ ìƒ˜í”Œê³¼ ê°€ì¥ ê°€ê¹Œìš´ í´ë˜ìŠ¤ ê°„ ìƒ˜í”Œì„ ì„ íƒí–ˆìŠµë‹ˆë‹¤.

<br>

**However, the center loss and its variants suffer from massive GPU memory consumption on the classification layer, and prefer balanced and sufficient training data for each identity.**  
ê·¸ëŸ¬ë‚˜ center lossê³¼ ê·¸ ë³€í˜•ì€ ë¶„ë¥˜ ê³„ì¸µì—ì„œ ë§‰ëŒ€í•œ GPU ë©”ëª¨ë¦¬ ì†Œë¹„ë¡œ ì¸í•´ ì–´ë ¤ì›€ì„ ê²ªê³  ìˆìœ¼ë©° ê° ì‹ ì›ì— ëŒ€í•´ ê· í˜• ìˆê³  ì¶©ë¶„í•œ Train ë°ì´í„°ë¥¼ ì„ í˜¸í•©ë‹ˆë‹¤.  

<br>
<br>
<br>
<br>

### 2) Angular/cosine-margin-based Loss  

<br>
<br>

**In 2017, people had a deeper understanding of loss function in deep FR and thought that samples should be separated more strictly to avoid misclassifying the difficult samples.**  
2017ë…„ì— ì‚¬ëŒë“¤ì€ Deep FRì˜ Loss Functionì— ëŒ€í•´ ë” ê¹Šì´ ì´í•´í–ˆê³  ì–´ë ¤ìš´ ìƒ˜í”Œì„ ì˜ëª» ë¶„ë¥˜í•˜ì§€ ì•Šë„ë¡ ìƒ˜í”Œì„ ë” ì—„ê²©í•˜ê²Œ ë¶„ë¦¬í•´ì•¼ í•œë‹¤ê³  ìƒê°í–ˆìŠµë‹ˆë‹¤.

<br>

**Angular/cosinemargin-based loss is proposed to make learned features potentially separable with a larger angular/cosine distance.**  
Angular/cosinemargin-based lossì€ larger angular/cosine distanceë¡œ ì ì¬ì ìœ¼ë¡œ ë¶„ë¦¬í•  ìˆ˜ ìˆëŠ” í•™ìŠµëœ ê¸°ëŠ¥ì„ ë§Œë“¤ê¸° ìœ„í•´ ì œì•ˆë©ë‹ˆë‹¤.

<br>

**The decision boundary in softmax loss is (W1 âˆ’ W2) x + b1 âˆ’ b2 = 0, where x is feature vector, Wi and bi are weights and bias in softmax loss, respectively.**  
softmax ì†ì‹¤ì˜ ê²°ì • ê²½ê³„ëŠ” (W1 âˆ’ W2) x + b1 âˆ’ b2 = 0ì…ë‹ˆë‹¤. ì—¬ê¸°ì„œ xëŠ” íŠ¹ì§• ë²¡í„°ì´ê³  Wiì™€ biëŠ” ê°ê° softmax ì†ì‹¤ì˜ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì…ë‹ˆë‹¤.

<br>

**Liu et al. reformulated the original softmax loss into a large-margin softmax (L-Softmax) loss.**  
Liu et al.ì€ ì›ë˜ì˜ softmax lossë¥¼ ë§ˆì§„ì´ í° Softmax(L-Softmax) ì†ì‹¤ë¡œ ì¬ê³µì‹í™”í–ˆìŠµë‹ˆë‹¤.

<br>

**They constrain b1 = b2 = 0, so the decision boundaries for class 1 and class 2 become kxk (kW1k cos (mÎ¸1) âˆ’ kW2k cos (Î¸2)) = 0 and kxk (kW1k kW2k cos (Î¸1) âˆ’ cos (mÎ¸2)) = 0, respectively, where m is a positive integer introducing an angular margin, and Î¸i is the angle between Wi and x.**  
b1 = b2 = 0ìœ¼ë¡œ ì œí•œí•˜ë¯€ë¡œ í´ë˜ìŠ¤ 1ê³¼ í´ë˜ìŠ¤ 2ì— ëŒ€í•œ ê²°ì • ê²½ê³„ëŠ” kxk(kW1k cos(mÎ¸1) âˆ’ kW2k cos(Î¸2)) = 0 ë° kxk(kW1k kW2k cos(Î¸1) âˆ’ cos(mÎ¸2))ê°€ ë©ë‹ˆë‹¤. = 0, ì—¬ê¸°ì„œ mì€ ê°ë„ ë§ˆì§„ì„ ë„ì…í•˜ëŠ” ì–‘ì˜ ì •ìˆ˜ì´ê³  Î¸iëŠ” Wiì™€ x ì‚¬ì´ì˜ ê°ë„ì…ë‹ˆë‹¤.

<br>

**Due to the nonmonotonicity of the cosine function, a piece-wise function is applied in L-softmax to guarantee the monotonicity.**  
cosine functionì˜ ë¹„ë‹¨ì¡°ì„±(nonmonotonicity )ìœ¼ë¡œ ì¸í•´ ë‹¨ì¡°ì„±ì„ ë³´ì¥í•˜ê¸° ìœ„í•´ L-softmaxì—ì„œ êµ¬ê°„ í•¨ìˆ˜ë¥¼ ì ìš©í•©ë‹ˆë‹¤.

<br>

**The loss function is defined as follows:**  
Loss FunctionëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë©ë‹ˆë‹¤.  

<br>
<br>
<p align="center">
  <img src="/assets/DeepFaceRecognitionSurvey/Formula_04.png">
</p>
<br>
<br>

where  

<br>
<br>
<p align="center">
  <img src="/assets/DeepFaceRecognitionSurvey/Formula_05.png">
</p>
<br>
<br>

**Based on L-Softmax, A-Softmax loss further normalized the weight W by L2 norm (kWk = 1) such that the normalized vector will lie on a hypersphere, and then the discriminative face features can be learned on a hypersphere manifold with an angular margin (Fig. 6).**  
L-Softmaxë¥¼ ê¸°ë°˜ìœ¼ë¡œ A-Softmax ì†ì‹¤ì€ ê°€ì¤‘ì¹˜ Wë¥¼ L2 í‘œì¤€(kWk = 1)ìœ¼ë¡œ ë” ì •ê·œí™”í•˜ì—¬ ì •ê·œí™”ëœ ë²¡í„°ê°€ í•˜ì´í¼ìŠ¤í”¼ì–´ì— ë†“ì´ê²Œ í•œ ë‹¤ìŒ ê°ì´ ìˆëŠ” í•˜ì´í¼ìŠ¤í”¼ì–´ ë§¤ë‹ˆí´ë“œì—ì„œ ì‹ë³„ ê°€ëŠ¥í•œ ì–¼êµ´ íŠ¹ì§•ì„ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ê·¸ë¦¼ 6).

<br>
<br>
<p align="center">
  <img src="/assets/DeepFaceRecognitionSurvey/Fig_06.png">
</p>
<br>
<br>

**Liu et al. [108] introduced a deep hyperspherical convolution network (SphereNet) that adopts hyperspherical convolution as its basic convolution operator and is supervised by angular-margin-based loss.**  
Liu et al. [108]ì€ ê¸°ë³¸ convolutional ì—°ì‚°ìë¡œ hyperspherical convolutional ì„ ì±„íƒí•˜ê³  ê°ë„-ë§ˆì§„ ê¸°ë°˜ ì†ì‹¤ì— ì˜í•´ ê°ë…ë˜ëŠ” ì‹¬ì¸µ í•˜ì´í¼ìŠ¤í˜ë¦¬ì»¬ convolutional  Network(SphereNet)ë¥¼ ë„ì…í–ˆìŠµë‹ˆë‹¤.

<br>

**To overcome the optimization difficulty of L-Softmax and A-Softmax, which incorporate the angular margin in a multiplicative manner, ArcFace and CosFace, AMS loss respectively introduced an additive angular/cosine margin cos(Î¸ + m) and cosÎ¸ âˆ’ m.**  
ê°ë„ ë§ˆì§„ì„ ê³±ì…ˆ ë°©ì‹ìœ¼ë¡œ í†µí•©í•˜ëŠ” L-Softmax ë° A-Softmax, ArcFace ë° CosFaceì˜ ìµœì í™” ì–´ë ¤ì›€ì„ ê·¹ë³µí•˜ê¸° ìœ„í•´ AMS ì†ì‹¤ì€ ê°ê° additive angular/cosine margin cos(Î¸ + m) ë° cosÎ¸ âˆ’ mì„ ë„ì…í–ˆìŠµë‹ˆë‹¤.

<br>

**They are extremely easy to implement without tricky hyperparameters Î», and are more clear and able to converge without the softmax supervision.**  
ê¹Œë‹¤ë¡œìš´ hyperparameters Î» ì—†ì´ êµ¬í˜„í•˜ê¸°ê°€ ë§¤ìš° ì‰½ê³  softmax ê°ë… ì—†ì´ ë” ëª…í™•í•˜ê³  ìˆ˜ë ´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<br>

**The decision boundaries under the binary classification case are given in Table V.**  
ì´ì§„ ë¶„ë¥˜ ì‚¬ë¡€ ì•„ë˜ì˜ decision boundariesëŠ” í‘œ Vì— ë‚˜ì™€ ìˆìŠµë‹ˆë‹¤.

<br>
<br>

<br>
<br>
<p align="center">
  <img src="/assets/DeepFaceRecognitionSurvey/Table_05.png">
</p>
<br>
<br>

**Based on large margin, FairLoss and AdaptiveFace further proposed to adjust the margins for different classes adaptively to address the problem of unbalanced data.**  
í° ë§ˆì§„ì„ ê¸°ë°˜ìœ¼ë¡œ FairLoss ë° AdaptiveFaceëŠ” ë¶ˆê· í˜• ë°ì´í„° ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì„œë¡œ ë‹¤ë¥¸ í´ë˜ìŠ¤ì˜ ë§ˆì§„ì„ ì ì‘ì ìœ¼ë¡œ ì¡°ì •í•  ê²ƒì„ ì¶”ê°€ë¡œ ì œì•ˆí–ˆìŠµë‹ˆë‹¤.

<br>

**Compared to Euclidean-distance-based loss, angular/cosinemargin-based loss explicitly adds discriminative constraints on a hypershpere manifold, which intrinsically matches the prior that human face lies on a manifold.**  
Euclidean-distance-based lossê³¼ ë¹„êµí•  ë•Œ, angular/cosinemargin-based lossì€ ì¸ê°„ì˜ ì–¼êµ´ì´ ë‹¤ì–‘ì²´ì— ë†“ì´ëŠ” ì´ì „ê³¼ ë³¸ì§ˆì ìœ¼ë¡œ ì¼ì¹˜í•˜ëŠ” ì´ˆê´‘ê° ë‹¤ì–‘ì²´ì— ì°¨ë³„ì  ì œì•½ì„ ëª…ì‹œì ìœ¼ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤.

<br>

**However, Wang et al. showed that angular/cosine-margin-based loss can achieve better results on a clean dataset, but is vulnerable to noise and becomes worse than center loss and softmax in the high-noise region as shown in Fig. 7.**  
ê·¸ëŸ¬ë‚˜ Wang et al.ì€ angular/cosine-margin-based lossì€ ê¹¨ë—í•œ Datasetì—ì„œ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆì§€ë§Œ ë…¸ì´ì¦ˆì— ì·¨ì•½í•˜ê³  ê·¸ë¦¼ 7ê³¼ ê°™ì´ ë…¸ì´ì¦ˆê°€ ë§ì€ ì˜ì—­ì—ì„œ center loss ë° softmaxë³´ë‹¤ ë‚˜ë¹ ì§‘ë‹ˆë‹¤.

<br>
<br>
<br>
  <img src="/assets/DeepFaceRecognitionSurvey/Fig_07.png">
<p align="center">
</p>
<br>
<br>

### 3) Softmax Loss and its Variations

<br>
<br>

**In 2017, in addition to reformulating softmax loss into an angular/cosine-marginbased loss as mentioned above, some works tries to normalize the features and weights in loss functions to improve the model performance, which can be written as follows:**  
2017ë…„ì—ëŠ” ìœ„ì—ì„œ ì–¸ê¸‰í•œ ëŒ€ë¡œ softmax ì†ì‹¤ì„ angular/cosine-marginbased lossë¡œ ì¬ê³µì‹í™”í•˜ëŠ” ê²ƒ ì™¸ì—ë„ ì¼ë¶€ ì‘ì—…ì—ì„œëŠ” Model ì„±ëŠ¥ì„ ê°œì„ í•˜ê¸° ìœ„í•´ Loss Functionì˜ ê¸°ëŠ¥ê³¼ ê°€ì¤‘ì¹˜ë¥¼ ì •ê·œí™”í•˜ë ¤ê³  ì‹œë„í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.    

<br>
<br>
<br>
  <img src="/assets/DeepFaceRecognitionSurvey/Formula_06.png">
<p align="center">
</p>
<br>
<br>

**where Î± is a scaling parameter, x is the learned feature vector, W is weight of last fully connected layer.**  
ì—¬ê¸°ì„œ Î±ëŠ” ìŠ¤ì¼€ì¼ë§ íŒŒë¼ë¯¸í„°, xëŠ” í•™ìŠµëœ feature vector, WëŠ” ë§ˆì§€ë§‰ ì™„ì „ ì—°ê²° ë ˆì´ì–´ì˜ ê°€ì¤‘ì¹˜ì…ë‹ˆë‹¤.

<br>

**Scaling x to a fixed radius Î± is important, as Wang et al. proved that normalizing both features and weights to 1 will make the softmax loss become trapped at a very high value on the training set.**  
Wang et al.ì²˜ëŸ¼ xë¥¼ ê³ ì • ë°˜ê²½ Î±ë¡œ ìŠ¤ì¼€ì¼ë§í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. íŠ¹ì§•ê³¼ ê°€ì¤‘ì¹˜ë¥¼ ëª¨ë‘ 1ë¡œ ì •ê·œí™”í•˜ë©´ Softmax ì†ì‹¤ì´ Train ì„¸íŠ¸ì—ì„œ ë§¤ìš° ë†’ì€ ê°’ì— ê°‡íˆê²Œ ëœë‹¤ëŠ” ê²ƒì„ ì¦ëª…í–ˆìŠµë‹ˆë‹¤.

<br>

**After that, the loss function, e.g. softmax, can be performed using the normalized features and weights.**  
ê·¸ í›„ Loss Function, ì˜ˆë¥¼ ë“¤ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. softmaxëŠ” ì •ê·œí™”ëœ íŠ¹ì§•ê³¼ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<br>

**Some papers first normalized the weights only and then added angular/cosine margin into loss functions to make the learned features be discriminative.**  
ì¼ë¶€ ë…¼ë¬¸ì—ì„œëŠ” ë¨¼ì € ê°€ì¤‘ì¹˜ë§Œ ì •ê·œí™”í•œ ë‹¤ìŒ Loss Functionì— angular/cosine marginì„ ì¶”ê°€í•˜ì—¬ í•™ìŠµëœ featuresì„ êµ¬ë³„í•˜ë„ë¡ í–ˆìŠµë‹ˆë‹¤.

<br>

**In contrast, some works, such as, adopted feature normalization only to overcome the bias to the sample distribution of the softmax.**  
ëŒ€ì¡°ì ìœ¼ë¡œ, ì¼ë¶€ ì‘ì—…ì€ softmaxì˜ ìƒ˜í”Œ ë¶„í¬ì— ëŒ€í•œ í¸í–¥ì„ ê·¹ë³µí•˜ê¸° ìœ„í•´ì„œë§Œ ê¸°ëŠ¥ ì •ê·œí™”ë¥¼ ì±„íƒí–ˆìŠµë‹ˆë‹¤.

<br>

**Based on the observation of [125] that the L2-norm of features learned using the softmax loss is informative of the quality of the face, L2-softmax enforced all the features to have the same L2-norm by feature normalization such that similar attention is given to good quality frontal faces and blurry faces with extreme pose.**  
softmax ì†ì‹¤ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•œ íŠ¹ì§•ì˜ L2-normì´ ì–¼êµ´ì˜ í’ˆì§ˆì— ëŒ€í•œ ì •ë³´ë¥¼ ì œê³µí•œë‹¤ëŠ” [125]ì˜ ê´€ì°°ì„ ê¸°ë°˜ìœ¼ë¡œ L2-softmaxëŠ” íŠ¹ì§• ì •ê·œí™”ë¥¼ í†µí•´ ëª¨ë“  íŠ¹ì§•ì´ ë™ì¼í•œ L2-normì„ ê°–ë„ë¡ ê°•ì œí–ˆìŠµë‹ˆë‹¤. ì¢‹ì€ í’ˆì§ˆì˜ ì •ë©´ ì–¼êµ´ê³¼ ê·¹ë‹¨ì ì¸ í¬ì¦ˆì˜ íë¦¿í•œ ì–¼êµ´ì— ë¶€ì—¬ë©ë‹ˆë‹¤.  

<br>
<br>

**Ring loss encouraged the norm of samples being value R (a learned parameter) rather than explicit enforcing through a hard normalization operation.**  
Ring lossì€ hard normalization operationì„ í†µí•´ ëª…ì‹œì ìœ¼ë¡œ ì ìš©í•˜ê¸°ë³´ë‹¤ëŠ” ê°’ R(í•™ìŠµëœ ë§¤ê°œë³€ìˆ˜)ì´ ë˜ëŠ” ìƒ˜í”Œì˜ í‘œì¤€ì„ ì¥ë ¤í–ˆìŠµë‹ˆë‹¤.

<br>

**Moreover, normalizing both features and weights has become a common strategy.**  
ë˜í•œ featuresì™€ weightsë¥¼ ëª¨ë‘ ì •ê·œí™”í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì¸ ì „ëµì´ ë˜ì—ˆìŠµë‹ˆë‹¤.

<br>

**Wang et al. [110] explained the necessity of this normalization operation from both analytic and geometric perspectives.**  
Wang et al. [110]ì€ ë¶„ì„ì  ê´€ì ê³¼ ê¸°í•˜í•™ì  ê´€ì  ëª¨ë‘ì—ì„œ ì´ ì •ê·œí™” ì‘ì—…ì˜ í•„ìš”ì„±ì„ ì„¤ëª…í–ˆìŠµë‹ˆë‹¤.

<br>

**After normalizing features and weights, CoCo loss [112] optimized the cosine distance among data features, and Hasnat et al. used the von MisesFisher (vMF) mixture model as the theoretical basis to develop a novel vMF mixture loss and its corresponding vMF deep features.**  
features ì™€ weightsë¥¼ ì •ê·œí™”í•œ í›„ CoCo loss[112]ëŠ” data featuresê°„ì˜ cosine distanceë¥¼ ìµœì í™”í–ˆìœ¼ë©° Hasnat et al. von MisesFisher(vMF)ì€ í˜¼í•© Modelì„ ì´ë¡ ì  ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ìƒˆë¡œìš´ vMF í˜¼í•© ì†ì‹¤ ë° í•´ë‹¹ vMF Deep Featuresì„ ê°œë°œí–ˆìŠµë‹ˆë‹¤.  

<br>
<br>
<br>

### B. Evolution of Network Architecture

<br>
<br>

### 1) Backbone Network

<br>
<br>

#### Mainstream architectures. 

<br>
<br>

**The commonly used network architectures of deep FR have always followed those of deep object classification and evolved from AlexNet to SENet rapidly.**  
Deep FRì˜ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” Network ArchitectureëŠ” í•­ìƒ Deep Object Classificationì„ ë”°ëìœ¼ë©° AlexNetì—ì„œ SENetìœ¼ë¡œ ë¹ ë¥´ê²Œ ë°œì „í–ˆìŠµë‹ˆë‹¤.

<br>

**We present the most influential architectures of deep object classification and deep face recognition in chronological order 1 in Fig. 8.**  
deep object classification ë° deep face recognition ì˜ ê°€ì¥ ì˜í–¥ë ¥ ìˆëŠ” Architectureë¥¼ ê·¸ë¦¼ 8ì˜ ì—°ëŒ€ìˆœ 1ë¡œ ì œì‹œí•©ë‹ˆë‹¤.

<br>
<br>
<br>
  <img src="/assets/DeepFaceRecognitionSurvey/Fig_08.png">
<p align="center">
</p>
<br>
<br>

**In 2012, AlexNet was reported to achieve the SOTA recognition accuracy in the ImageNet large-scale visual recognition competition (ILSVRC) 2012, exceeding the previous best results by a large margin.**  
2012ë…„ AlexNetì€ ImageNet ëŒ€ê·œëª¨ ì‹œê° ì¸ì‹ ëŒ€íšŒ(ILSVRC) 2012ì—ì„œ SOTA ì¸ì‹ ì •í™•ë„ë¥¼ ë‹¬ì„±í•˜ì—¬ ì´ì „ ìµœê³  ê²°ê³¼ë¥¼ í¬ê²Œ ë›°ì–´ë„˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ê³ ë˜ì—ˆìŠµë‹ˆë‹¤.

<br>

**AlexNet consists of five convolutional layers and three fully connected layers, and it also integrates various techniques, such as rectified linear unit (ReLU), dropout, data augmentation, and so forth.**  
AlexNetì€ 5ê°œì˜ Convolutional Layerì™€ 3ê°œì˜ Fully Connected Layerë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©° ReLU(Rectified Linear Unit), Dropout, Data Augmentation ë“±ì˜ ë‹¤ì–‘í•œ ê¸°ìˆ ì„ í†µí•©í•˜ê³  ìˆìŠµë‹ˆë‹¤.

<br>

**ReLU was widely regarded as the most essential component for making deep learning possible.**  
ReLUëŠ” Deep Learningì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ë° ê°€ì¥ í•„ìˆ˜ì ì¸ êµ¬ì„± ìš”ì†Œë¡œ ë„ë¦¬ ê°„ì£¼ë˜ì—ˆìŠµë‹ˆë‹¤.

<br>

**Then, in 2014, VGGNet proposed a standard network architecture that used very small 3 Ã— 3 convolutional filters throughout and doubled the number of feature maps after the 2Ã—2 pooling.**  
ê·¸ë¦¬ê³  2014ë…„ì— VGGNetì€ ë§¤ìš° ì‘ì€ 3Ã—3 ì»¨ë²Œë£¨ì…˜ í•„í„°ë¥¼ ì „ì²´ì ìœ¼ë¡œ ì‚¬ìš©í•˜ê³  2Ã—2 í’€ë§ í›„ íŠ¹ì§• ë§µì˜ ìˆ˜ë¥¼ ë‘ ë°°ë¡œ ëŠ˜ë¦° í‘œì¤€ Network Architectureë¥¼ ì œì•ˆí–ˆìŠµë‹ˆë‹¤.

<br>

**It increased the depth of the network to 16-19 weight layers, which further enhanced the flexibility to learn progressive nonlinear mappings by deep architectures.**  
ê·¸ê²ƒì€ Networkì˜ ê¹Šì´ë¥¼ 16-19ê°œì˜ ê°€ì¤‘ì¹˜ ë ˆì´ì–´ë¡œ ì¦ê°€ì‹œì¼°ê³ , ì´ëŠ” ì‹¬ì¸µ Architectureì— ì˜í•œ ì ì§„ì  ë¹„ì„ í˜• ë§¤í•‘ì„ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ìœ ì—°ì„±ì„ ë”ìš± í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.

<br>

**In 2015, the 22-layer GoogleNet introduced an â€œinception moduleâ€ with the concatenation of hybrid feature maps, as well as two additional intermediate softmax supervised signals.**  
2015ë…„ì— 22ê³„ì¸µ GoogleNetì€ hybrid feature mapsì„ ì—°ê²°í•˜ëŠ” "inception Module"ê³¼ 2ê°œì˜ ì¶”ê°€ ì¤‘ê°„ softmax supervised signalsë¥¼ ë„ì…í–ˆìŠµë‹ˆë‹¤.

<br>

**It performs several convolutions with different receptive fields (1 Ã— 1, 3 Ã— 3 and 5 Ã— 5) in parallel, and concatenates all feature maps to merge the multi-resolution information.**  
ì„œë¡œ ë‹¤ë¥¸ ìˆ˜ìš© í•„ë“œ(1 Ã— 1, 3 Ã— 3 ë° 5 Ã— 5)ë¡œ ì—¬ëŸ¬ convolutional ì„ ë³‘ë ¬ë¡œ ìˆ˜í–‰í•˜ê³  ëª¨ë“  feature mapsì„ ì—°ê²°í•˜ì—¬ ë‹¤ì¤‘ í•´ìƒë„ ì •ë³´ë¥¼ ë³‘í•©í•©ë‹ˆë‹¤.        

<br>
<br>

**In 2016, ResNet proposed to make layers learn a residual mapping with reference to the layer inputs F(x) := H(x) âˆ’ x rather than directly learning a desired underlying mapping H(x) to ease the training of very deep networks (up to 152 layers).**  
2016ë…„ì— ResNetì€ ê³„ì¸µì´ ì›í•˜ëŠ” ê¸°ë³¸ ë§¤í•‘ H(x)ë¥¼ ì§ì ‘ í•™ìŠµí•˜ëŠ” ëŒ€ì‹  ê³„ì¸µ ì…ë ¥ F(x) := H(x) âˆ’ xë¥¼ ì°¸ì¡°í•˜ì—¬ residual ë§¤í•‘ì„ í•™ìŠµí•˜ë„ë¡ ì œì•ˆí•˜ì—¬ ë§¤ìš° ê¹Šì€ Networkì˜ Trainì„ ìš©ì´í•˜ê²Œ í–ˆìŠµë‹ˆë‹¤. (ìµœëŒ€ 152ê°œ ë ˆì´ì–´).

<br>

**The original mapping is recast into F(x) + x and can be realized by â€œshortcut connectionsâ€.**  
ì›ë˜ ë§¤í•‘ì€ F(x) + xë¡œ ì¬êµ¬ì„±ë˜ë©° "shortcut connections"ë¡œ ì‹¤í˜„ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<br>

**As the champion of ILSVRC 2017, SENet introduced a â€œSqueeze-and-Excitationâ€ (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels.**  
ILSVRC 2017ì˜ ì±”í”¼ì–¸ì¸ SENetì€ ì±„ë„ ê°„ì˜ ìƒí˜¸ ì˜ì¡´ì„±ì„ ëª…ì‹œì ìœ¼ë¡œ modellingí•˜ì—¬ ì±„ë„ë³„ ê¸°ëŠ¥ ì‘ë‹µì„ ì ì‘ì ìœ¼ë¡œ ì¬ë³´ì •í•˜ëŠ” "Squeeze-and-Excitation"(SE) ë¸”ë¡ì„ ë„ì…í–ˆìŠµë‹ˆë‹¤.

<br>

**These blocks can be integrated with modern architectures, such as ResNet, and improves their representational power.**  
ì´ëŸ¬í•œ ë¸”ë¡ì€ ResNetê³¼ ê°™ì€ ìµœì‹  Architectureì™€ í†µí•©ë  ìˆ˜ ìˆìœ¼ë©° í‘œí˜„ë ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

<br>

**With the evolved architectures and advanced training techniques, such as batch normalization (BN), the network becomes deeper and the training becomes more controllable.**  
batch normalization (BN)ê³¼ ê°™ì€ ì§„í™”ëœ Architectureì™€ ê³ ê¸‰ Train ê¸°ìˆ ì„ í†µí•´ Networkê°€ ë” ê¹Šì–´ì§€ê³  Trainì„ ë” ì˜ ì œì–´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<br>

**Following these architectures in object classification, the networks in deep FR are also developed step by step, and the performance of deep FR is continually improving.**  
ê°ì²´ ë¶„ë¥˜ì—ì„œ ì´ëŸ¬í•œ Architectureì— ë”°ë¼ deep FRì˜ Networkë„ ë‹¨ê³„ì ìœ¼ë¡œ ê°œë°œë˜ë©° deep FRì˜ ì„±ëŠ¥ì€ ì§€ì†ì ìœ¼ë¡œ í–¥ìƒë˜ê³  ìˆìŠµë‹ˆë‹¤.

<br>

**We present these mainstream architectures of deep FR in Fig. 9.**  
ê·¸ë¦¼ 9ì—ì„œ ì´ëŸ¬í•œ ì‹¬ì¸µ FRì˜ ì£¼ë¥˜ Architectureë¥¼ ì œì‹œí•©ë‹ˆë‹¤.

<br>
<br>
<br>
  <img src="/assets/DeepFaceRecognitionSurvey/Fig_09.png">
<p align="center">
</p>
<br>
<br>

**In 2014, DeepFace was the first to use a nine-layer CNN with several locally connected layers.**  
2014ë…„ì— DeepFaceëŠ” ë¡œì»¬ë¡œ ì—°ê²°ëœ ì—¬ëŸ¬ ê³„ì¸µì´ ìˆëŠ” 9ê³„ì¸µ CNNì„ ì²˜ìŒìœ¼ë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.

<br>

**With 3D alignment for face processing, it reaches an accuracy of 97.35% on LFW. In 2015, FaceNet used a large private dataset to train a GoogleNet.**  
ì–¼êµ´ ì²˜ë¦¬ë¥¼ ìœ„í•œ 3D ì •ë ¬ë¡œ LFWì—ì„œ 97.35%ì˜ ì •í™•ë„ì— ë„ë‹¬í•©ë‹ˆë‹¤. 2015ë…„ì— FaceNetì€ ëŒ€ê·œëª¨ ê°œì¸ Datasetë¥¼ ì‚¬ìš©í•˜ì—¬ GoogleNetì„ êµìœ¡í–ˆìŠµë‹ˆë‹¤.

<br>

**It adopted a triplet loss function based on triplets of roughly aligned matching/nonmatching face patches generated by a novel online triplet mining method and achieved good performance of 99.63%.**  
ìƒˆë¡œìš´ ì˜¨ë¼ì¸ triplet mining ë°©ë²•ìœ¼ë¡œ ìƒì„±ëœ ëŒ€ëµì ìœ¼ë¡œ ì •ë ¬ëœ ì¼ì¹˜/ë¹„ì¼ì¹˜ ì•ˆë©´ íŒ¨ì¹˜ì˜ ì‚¼ì¤‘í•­ì„ ê¸°ë°˜ìœ¼ë¡œ triplet loss Functionë¥¼ ì±„íƒí–ˆìœ¼ë©° 99.63%ì˜ ì¢‹ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

<br>

**In the same year, VGGface designed a procedure to collect a large-scale dataset from the Internet.**  
ê°™ì€ í•´ì— VGGfaceëŠ” ì¸í„°ë„·ì—ì„œ ëŒ€ê·œëª¨ Datasetë¥¼ ìˆ˜ì§‘í•˜ëŠ” ì ˆì°¨ë¥¼ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤.

<br>

**It trained the VGGNet on this dataset and then fine-tuned the networks via a triplet loss function similar to FaceNet.**  
ì´ Datasetì—ì„œ VGGNetì„ êµìœ¡í•œ ë‹¤ìŒ FaceNetê³¼ ìœ ì‚¬í•œ triplet Loss Functionë¥¼ í†µí•´ Networkë¥¼ ë¯¸ì„¸ ì¡°ì •í–ˆìŠµë‹ˆë‹¤.

<br>

**VGGface obtains an accuracy of 98.95%.**  
VGGfaceëŠ” 98.95%ì˜ ì •í™•ë„ë¥¼ ì–»ìŠµë‹ˆë‹¤.

<br>

**In 2017, SphereFace [84] used a 64-layer ResNet architecture and proposed the angular softmax (A-Softmax) loss to learn discriminative face features with angular margin.**  
2017ë…„ì— SphereFace[84]ëŠ” 64ê³„ì¸µ ResNet Architectureë¥¼ ì‚¬ìš©í•˜ê³  ê°ë„ ë§ˆì§„ì„ ê°€ì§„ ì°¨ë³„ì ì¸ ì–¼êµ´ íŠ¹ì§•ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ angular softmax (A-Softmax) ì†ì‹¤ì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤.

<br>

**It boosts the achieves to 99.42% on LFW. In the end of 2017, a new largescale face dataset, namely VGGface2, was introduced, which consists of large variations in pose, age, illumination, ethnicity and profession.**  
LFWì—ì„œ ë‹¬ì„±ë¥ ì„ 99.42%ë¡œ ë†’ì…ë‹ˆë‹¤. 2017ë…„ ë§ì— ìƒˆë¡œìš´ ëŒ€ê·œëª¨ ì–¼êµ´ Datasetì¸ VGGface2ê°€ ë„ì…ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ DatasetëŠ” í¬ì¦ˆ, ì—°ë ¹, ì¡°ëª…, ë¯¼ì¡± ë° ì§ì—…ì˜ í° ë³€í™”ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

<br>

**Cao et al. first trained a SENet with MS-celeb-1M dataset and then fine-tuned the model with VGGface2, and achieved the SOTA performance on the IJB-A and IJB-B.**  
Caoet al.ì€ ë¨¼ì € MS-celeb-1M Datasetë¡œ SENetì„ êµìœ¡í•œ ë‹¤ìŒ VGGface2ë¡œ Modelì„ ë¯¸ì„¸ ì¡°ì •í•˜ê³  IJB-A ë° IJB-Bì—ì„œ SOTA ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.   

<br>
<br>
<br>

### Light-weight networks. 

<br>
<br>
<br>

**Using deeper neural network with hundreds of layers and millions of parameters to achieve higher accuracy comes at cost.**  
ë” ë†’ì€ ì •í™•ë„ë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ìˆ˜ë°± ê°œì˜ ë ˆì´ì–´ì™€ ìˆ˜ë°±ë§Œ ê°œì˜ ë§¤ê°œë³€ìˆ˜ê°€ ìˆëŠ” ì‹¬ì¸µ neural networkì„ ì‚¬ìš©í•˜ë©´ ë¹„ìš©ì´ ë°œìƒí•©ë‹ˆë‹¤.

<br>

**Powerful GPUs with larger memory size are needed, which makes the applications on many mobiles and embedded devices impractical.**  
ë” í° ë©”ëª¨ë¦¬ í¬ê¸°ì˜ ê°•ë ¥í•œ GPUê°€ í•„ìš”í•˜ë¯€ë¡œ ë§ì€ ëª¨ë°”ì¼ ë° ì„ë² ë””ë“œ ì¥ì¹˜ì˜ ì‘ìš© í”„ë¡œê·¸ë¨ì´ ë¹„ì‹¤ìš©ì ì…ë‹ˆë‹¤.

<br>

**To address this problem, light-weight networks are proposed.**  
ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê²½ëŸ‰ Networkê°€ ì œì•ˆë©ë‹ˆë‹¤.

<br>

**Light CNN proposed a max-feature-map (MFM) activation function that introduces the concept of maxout in the fully connected layer to CNN.**  
Light CNNì€ Fully Connected Layerì˜ maxout ê°œë…ì„ CNNì— ë„ì…í•œ MFM(max-feature-map) í™œì„±í™” í•¨ìˆ˜ë¥¼ ì œì•ˆí–ˆìŠµë‹ˆë‹¤.

<br>

**The MFM obtains a compact representation and reduces the computational cost. Sun et al. proposed to sparsify deep networks iteratively from the previously learned denser models based on a weight selection criterion.**  
MFMì€ ê°„ê²°í•œ í‘œí˜„ì„ ì–»ê³  ê³„ì‚° ë¹„ìš©ì„ ì¤„ì…ë‹ˆë‹¤. Sun et al. ê°€ì¤‘ì¹˜ ì„ íƒ ê¸°ì¤€ì„ ê¸°ë°˜ìœ¼ë¡œ ì´ì „ì— í•™ìŠµëœ ë°€ë„ê°€ ë†’ì€ Modelì—ì„œ Deep Networkë¥¼ ë°˜ë³µì ìœ¼ë¡œ í¬ì†Œí™”í•˜ë„ë¡ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤.

<br>

**MobiFace adopted fast downsampling and bottleneck residual block with the expansion layers and achieved high performance with 99.7% on LFW database.**  
MobiFaceëŠ” í™•ì¥ ë ˆì´ì–´ì™€ í•¨ê»˜ fast downsampling ë° bottleneck residual blockì„ ì±„íƒí–ˆìœ¼ë©° LFW Databaseì—ì„œ 99.7%ì˜ ë†’ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

<br>

**Although some other light-weight CNNs, such as SqueezeNet, MobileNet, ShuffleNet and Xception, are still not widely used in FR, they deserve more attention.**  
SqueezeNet, MobileNet, ShuffleNet ë° Xceptionê³¼ ê°™ì€ ì¼ë¶€ ë‹¤ë¥¸ light-weight CNNì€ ì•„ì§ FRì—ì„œ ë„ë¦¬ ì‚¬ìš©ë˜ì§€ ì•Šì§€ë§Œ ë” ë§ì€ ê´€ì‹¬ì„ ë°›ì„ ê°€ì¹˜ê°€ ìˆìŠµë‹ˆë‹¤.    

<br>
<br>
<br>

### Adaptive-architecture networks

<br>
<br>
<br>

**Considering that designing architectures manually by human experts are timeconsuming and error-prone processes, there is growing interest in adaptive-architecture networks which can find well-performing architectures, e.g. the type of operation every layer executes (pooling, convolution, etc) and hyper-parameters associated with the operation (number of filters, kernel size and strides for a convolutional layer, etc), according to the specific requirements of training and testing data.**  
ì¸ê°„ ì „ë¬¸ê°€ê°€ ìˆ˜ë™ìœ¼ë¡œ Architectureë¥¼ ì„¤ê³„í•˜ëŠ” ê²ƒì€ ì‹œê°„ì´ ë§ì´ ê±¸ë¦¬ê³  ì˜¤ë¥˜ê°€ ë°œìƒí•˜ê¸° ì‰¬ìš´ í”„ë¡œì„¸ìŠ¤ë¼ëŠ” ì ì„ ê³ ë ¤í•  ë•Œ ì„±ëŠ¥ì´ ì¢‹ì€ Architectureë¥¼ ì°¾ì„ ìˆ˜ ìˆëŠ” ì ì‘í˜• Architecture Networkì— ëŒ€í•œ ê´€ì‹¬ì´ ë†’ì•„ì§€ê³  ìˆìŠµë‹ˆë‹¤. Train ë° Test Dataì˜ íŠ¹ì • ìš”êµ¬ ì‚¬í•­ì— ë”°ë¼ ëª¨ë“  ë ˆì´ì–´ê°€ ì‹¤í–‰í•˜ëŠ” ì‘ì—… ìœ í˜•(í’€ë§, convolutional  ë“±) ë° ì‘ì—…ê³¼ ê´€ë ¨ëœ hyper-parameters(number of filters, kernel size and strides for a convolutional layer, etc).

<br>

**Currently, neural architecture search (NAS) is one of the promising methodologies, which has outperformed manually designed architectures on some tasks such as image classification or semantic segmentation.**  
í˜„ì¬ NAS(Neural Architecture Search)ëŠ” ìœ ë§í•œ ë°©ë²•ë¡  ì¤‘ í•˜ë‚˜ë¡œ ì´ë¯¸ì§€ ë¶„ë¥˜ ë˜ëŠ” ì˜ë¯¸ ë¶„í• ê³¼ ê°™ì€ ì¼ë¶€ ì‘ì—…ì—ì„œ ìˆ˜ë™ìœ¼ë¡œ ì„¤ê³„ëœ Architectureë¥¼ ëŠ¥ê°€í•©ë‹ˆë‹¤.

<br>

**Zhu et al. integrated NAS technology into face recognition.**  
Zhuet al. NAS ê¸°ìˆ ì„ ì–¼êµ´ ì¸ì‹ì— í†µí•©í–ˆìŠµë‹ˆë‹¤.

<br>

**They used reinforcement learning algorithm (policy gradient) to guide the controller network to train the optimal child architecture.**  
ê·¸ë“¤ì€ ì»¨íŠ¸ë¡¤ëŸ¬ Networkê°€ ìµœì ì˜ child architectureë¥¼ Trainí•˜ë„ë¡ ì•ˆë‚´í•˜ê¸° ìœ„í•´ reinforcement learning algorithm(policy gradient)ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.

<br>

**Besides NAS, there are some other explorations to learn optimal architectures adaptively.**  
NAS ì™¸ì—ë„ ìµœì ì˜ Architectureë¥¼ ì ì‘ì ìœ¼ë¡œ í•™ìŠµí•˜ê¸° ìœ„í•œ ëª‡ ê°€ì§€ ë‹¤ë¥¸ íƒêµ¬ê°€ ìˆìŠµë‹ˆë‹¤.

<br>

**For example, conditional convolutional neural network (c-CNN) dynamically activated sets of kernels according to modalities of samples;**  
ì˜ˆë¥¼ ë“¤ì–´, c-CNN(Conditional Convolutional Neural Network)ì€ ìƒ˜í”Œ ì–‘ì‹ì— ë”°ë¼ ì»¤ë„ ì„¸íŠ¸ë¥¼ ë™ì ìœ¼ë¡œ í™œì„±í™”í–ˆìŠµë‹ˆë‹¤.

<br>

**Han et al. proposed a novel contrastive convolution consisted of a trunk CNN and a kernel generator, which is beneficial owing to its dynamistic generation of contrastive kernels based on the pair of faces being compared.**  
Han et al.ì€ íŠ¸ë í¬ CNNê³¼ kernel generatorë¡œ êµ¬ì„±ëœ novel contrastive convolutionì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤.    

<br>
<br>
<br>

### Joint alignment-recognition networks

<br>
<br>
<br>

**Recently, an endto-end system was proposed to jointly train FR with several modules (face detection, alignment, and so forth) together.**
ìµœê·¼ì—ëŠ” FRì„ ì—¬ëŸ¬ Module(ì–¼êµ´ ê°ì§€, ì •ë ¬ ë“±)ê³¼ í•¨ê»˜ ê³µë™ìœ¼ë¡œ Trainí•˜ê¸° ìœ„í•œ end-to-end ì‹œìŠ¤í…œì´ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤.

<br>

**Compared to the existing methods in which each module is generally optimized separately according to different objectives, this end-to-end system optimizes each module according to the recognition objective, leading to more adequate and robust inputs for the recognition model.**  
ì¼ë°˜ì ìœ¼ë¡œ ê° Moduleì´ ì„œë¡œ ë‹¤ë¥¸ ëª©í‘œì— ë”°ë¼ ê°œë³„ì ìœ¼ë¡œ ìµœì í™”ë˜ëŠ” ê¸°ì¡´ ë°©ë²•ê³¼ ë¹„êµí•˜ì—¬ ì´ end-to-end ì‹œìŠ¤í…œì€ ì¸ì‹ ëª©í‘œì— ë”°ë¼ ê° Moduleì„ ìµœì í™”í•˜ì—¬ ì¸ì‹ Modelì— ëŒ€í•œ ë” ì ì ˆí•˜ê³  ê°•ë ¥í•œ ì…ë ¥ì„ ìœ ë„í•©ë‹ˆë‹¤.

<br>

**For example, inspired by spatial transformer, Hayat et al. proposed a CNN-based data-driven approach that learns to simultaneously register and represent faces (Fig.10), while Wu et al. designed a novel recursive spatial transformer (ReST) module for CNN allowing face alignment and recognition to be jointly optimized.**  
ì˜ˆë¥¼ ë“¤ì–´, ê³µê°„ ë³€í™˜ê¸°ì—ì„œ ì˜ê°ì„ ì–»ì€ Hayat et al. Wu et al.ì€ ì–¼êµ´ì„ ë™ì‹œì— ë“±ë¡í•˜ê³  í‘œí˜„í•˜ëŠ” ë°©ë²•ì„ ë°°ìš°ëŠ” CNN ê¸°ë°˜ ë°ì´í„° ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤(ê·¸ë¦¼ 10). ì–¼êµ´ ì •ë ¬ ë° ì¸ì‹ì„ í•¨ê»˜ ìµœì í™”í•  ìˆ˜ ìˆë„ë¡ CNNì„ ìœ„í•œ ìƒˆë¡œìš´ ì¬ê·€ ê³µê°„ ë³€í™˜ê¸°(ReST) Moduleì„ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤.

<br>
<br>
<br>
  <img src="/assets/DeepFaceRecognitionSurvey/Fig_10.png">
<p align="center">
</p>
<br>
<br>

## 2) Assembled Networks 

<br>
<br>
<br>

### Multi-input networks

<br>
<br>
<br>

**In â€œone-to-many augmentationâ€, multiple images with variety are generated from one image in order to augment training data.**  
ì¼ëŒ€ë‹¤ ì¦ê°•(one-to-many Augmentation)ì€ Train ë°ì´í„°ë¥¼ ì¦ê°•ì‹œí‚¤ê¸° ìœ„í•´ í•˜ë‚˜ì˜ ì´ë¯¸ì§€ì—ì„œ ë‹¤ì–‘í•œ ì´ë¯¸ì§€ë¥¼ ì—¬ëŸ¬ ê°œ ìƒì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

<br>

**Taken these multiple images as input, multiple networks are also assembled together to extract and combine features of different type of inputs, which can outperform an individual network.**  
ì´ëŸ¬í•œ ì—¬ëŸ¬ ì´ë¯¸ì§€ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ë©´ ì—¬ëŸ¬ Networkë„ í•¨ê»˜ ì¡°ë¦½ë˜ì–´ ê°œë³„ Networkë¥¼ ëŠ¥ê°€í•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ì…ë ¥ ìœ í˜•ì˜ ê¸°ëŠ¥ì„ ì¶”ì¶œí•˜ê³  ê²°í•©í•©ë‹ˆë‹¤.

<br>

**Assembled networks are built after different face patches are cropped, and then different types of patches are fed into different sub-networks for representation extraction.**  
ì¡°ë¦½ëœ NetworkëŠ” ì„œë¡œ ë‹¤ë¥¸ ì–¼êµ´ íŒ¨ì¹˜ê°€ ì˜ë¦° í›„ êµ¬ì¶•ëœ ë‹¤ìŒ representation extractionì„ ìœ„í•´ ì„œë¡œ ë‹¤ë¥¸ ìœ í˜•ì˜ íŒ¨ì¹˜ê°€ ì„œë¡œ ë‹¤ë¥¸ í•˜ìœ„ Networkì— ê³µê¸‰ë©ë‹ˆë‹¤.

<br>

**By combining the results of subnetworks, the performance can be improved. Other papers used assembled networks to recognize images with different poses.**  
ì„œë¸Œ Networkì˜ ê²°ê³¼ë¥¼ ê²°í•©í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë…¼ë¬¸ì—ì„œëŠ” assembled Networkë¥¼ ì‚¬ìš©í•˜ì—¬ í¬ì¦ˆê°€ ë‹¤ë¥¸ ì´ë¯¸ì§€ë¥¼ ì¸ì‹í–ˆìŠµë‹ˆë‹¤.      

<br>
<br>
<br>

**A multi-view deep network (MvDN) [95] consists of view-specific subnetworks and common subnetworks; the former removes view-specific variations, and the latter obtains common representations.**  
MvDN(multi-view deep network)[95]ì€ view-specific subnetworksì™€ common subnetworksë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ì „ìëŠ” view-specific variationsì„ ì œê±°í•˜ê³  í›„ìëŠ” common representationsì„ ì–»ìŠµë‹ˆë‹¤.

<br>

**Multi-task networks. FR is intertwined with various factors, such as pose, illumination, and age. To solve this problem, multitask learning is introduced to transfer knowledge fromother relevant tasks and to disentangle nuisance factors.**  
Multi-task Network. FRì€ í¬ì¦ˆ, ì¡°ëª…, ì—°ë ¹ ë“± ë‹¤ì–‘í•œ ìš”ì†Œì™€ ì–½í˜€ ìˆìŠµë‹ˆë‹¤. ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ multitask learningì„ ë„ì…í•˜ì—¬ ë‹¤ë¥¸ ê´€ë ¨ ì‘ì—…ì—ì„œ ì§€ì‹ì„ ì´ì „í•˜ê³  ë°©í•´ ìš”ì†Œë¥¼ ë¶„ë¦¬í•©ë‹ˆë‹¤.

<br>

**In multi-task networks, identity classification is the main task and the side tasks are pose, illumination, and expression estimations, among others.**  
multi-task Networkì—ì„œ identity classificationê°€ ì£¼ìš” ì‘ì—…ì´ê³  ë¶€ìˆ˜ ì‘ì—…ì€ ë¬´ì—‡ë³´ë‹¤ë„ í¬ì¦ˆ, ì¡°ëª… ë° í‘œì • ì¶”ì •ì…ë‹ˆë‹¤.

<br>

**The lower layers are shared among all the tasks, and the higher layers are disentangled into different sub-networks to generate the task-specific outputs.**  
í•˜ìœ„ ê³„ì¸µì€ ëª¨ë“  ì‘ì—… ê°„ì— ê³µìœ ë˜ë©° ìƒìœ„ ê³„ì¸µì€ ì„œë¡œ ë‹¤ë¥¸ í•˜ìœ„ Networkë¡œ ë¶„ë¦¬ë˜ì–´ ì‘ì—…ë³„ ì¶œë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤.

<br>

**The task-specific sub-networks are branched out to learn face detection, face alignment, pose estimation, gender recognition, smile detection, age estimation and FR.**  
ì‘ì—…ë³„ í•˜ìœ„ NetworkëŠ” ì–¼êµ´ ê°ì§€, ì–¼êµ´ ì •ë ¬, ìì„¸ ì¶”ì •, ì„±ë³„ ì¸ì‹, ë¯¸ì†Œ ê°ì§€, ì—°ë ¹ ì¶”ì • ë° FRì„ í•™ìŠµí•˜ê¸° ìœ„í•´ ë¶„ê¸°ë©ë‹ˆë‹¤.

<br>

**Yin et al. proposed to automatically assign the dynamic loss weights for each side task.**  
Yin et al.ì€ ê° ë¶€ì—…ì— ëŒ€í•œ ë™ì  ì†ì‹¤ ê°€ì¤‘ì¹˜ë¥¼ ìë™ìœ¼ë¡œ í• ë‹¹í•˜ë„ë¡ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤.

<br>

**Peng et al. used a feature reconstruction metric learning to disentangle a CNN into subnetworks for jointly learning the identity and non-identity features as shown in Fig. 11.**  
Peng et al.ì€ ê·¸ë¦¼ 11ê³¼ ê°™ì´ ì‹ ì› ë° ë¹„ì‹ ì› íŠ¹ì§•ì„ í•¨ê»˜ í•™ìŠµí•˜ê¸° ìœ„í•´ CNNì„ í•˜ìœ„ Networkë¡œ ë¶„ë¦¬í•˜ê¸° ìœ„í•´ íŠ¹ì§• ì¬êµ¬ì„± ë©”íŠ¸ë¦­ í•™ìŠµì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. 

<br>
<br>
<br>
  <img src="/assets/DeepFaceRecognitionSurvey/Fig_11.png">
<p align="center">
</p>
<br>
<br>

### C. Face Matching by deep features

<br>
<br>
<br>

**During testing, the cosine distance and L2 distance are generally employed to measure the similarity between the deep features x1 and x2; then, threshold comparison and the nearest neighbor (NN) classifier are used to make decision for verification and identification.**  
í…ŒìŠ¤íŠ¸ ì¤‘ì— cosine distanceì™€ L2 distanceëŠ” ì¼ë°˜ì ìœ¼ë¡œ Deep features x1ê³¼ x2 ì‚¬ì´ì˜ ìœ ì‚¬ì„±ì„ ì¸¡ì •í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ì„ê³„ê°’ ë¹„êµ ë° ê°€ì¥ ê°€ê¹Œìš´ ì´ì›ƒ(NN) ë¶„ë¥˜ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ í™•ì¸ ë° ì‹ë³„ì„ ìœ„í•œ ê²°ì •ì„ ë‚´ë¦½ë‹ˆë‹¤.

<br>

**In addition to these common methods, there are some other explorations.**  
ì´ëŸ¬í•œ ì¼ë°˜ì ì¸ ë°©ë²• ì™¸ì—ë„ ëª‡ ê°€ì§€ ë‹¤ë¥¸ íƒìƒ‰ì´ ìˆìŠµë‹ˆë‹¤.

<br>
<br>

### 1) Face verification

<br>
<br>

**Metric learning, which aims to find a new metric to make two classes more separable, can also be used for face matching based on extracted deep features.**  
ë‘ í´ë˜ìŠ¤ë¥¼ ë” ë¶„ë¦¬í•  ìˆ˜ ìˆë„ë¡ ìƒˆë¡œìš´ ë©”íŠ¸ë¦­ì„ ì°¾ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•˜ëŠ” ë©”íŠ¸ë¦­ í•™ìŠµì€ ì¶”ì¶œëœ deep featuresì„ ê¸°ë°˜ìœ¼ë¡œ ì–¼êµ´ ë§¤ì¹­ì—ë„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<br>

**The JB model is a well-known metric learning method, and Hu et al. proved that it can improve the performance greatly.**  
JB Modelì€ ì˜ ì•Œë ¤ì§„ ë©”íŠ¸ë¦­ í•™ìŠµ ë°©ë²•ì´ë©° Hu et al.ì€ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŒì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

<br>

**In the JB model, a face feature x is modeled as x = Âµ+Îµ, where Âµ and Îµ are identity and intra-personal variations, respectively.**  
JB Modelì—ì„œ ì–¼êµ´ íŠ¹ì§• xëŠ” x = Âµ+Îµë¡œ Modelingë˜ë©°, ì—¬ê¸°ì„œ Âµì™€ ÎµëŠ” ê°ê° identityê³¼ intra-personal variationsì…ë‹ˆë‹¤.

<br>

**The similarity score r(x1, x2) can be represented as follows:**  

<br>
<br>
<br>
  <img src="/assets/DeepFaceRecognitionSurvey/Formula_07.png">
<p align="center">
</p>
<br>
<br>

where P(x1, x2|HI ) is the probability that two faces belong to the same identity and P(x1, x2|HE) is the probability that two faces belong to different identities.  

<br>
<br>
<br>

### 2) Face identification

<br>
<br>
<br>

**After cosine distance was computed,Cheng et al. proposed a heuristic voting strategy at the similarity score level to combine the results of multiple CNN models and won first place in Challenge 2 of MSceleb-1M 2017.**  
cosine distanceë¥¼ ê³„ì‚°í•œ í›„ Cheng et al.ì€ ì—¬ëŸ¬ CNN Modelì˜ ê²°ê³¼ë¥¼ ê²°í•©í•˜ê¸° ìœ„í•´ ìœ ì‚¬ì„± ì ìˆ˜ ìˆ˜ì¤€ì—ì„œ heuristic voting strategyì„ ì œì•ˆí•˜ê³  MSceleb-1M 2017ì˜ Challenge 2ì—ì„œ 1ìœ„ë¥¼ ì°¨ì§€í–ˆìŠµë‹ˆë‹¤.

<br>

**Yang et al. extracted the local adaptive convolution features from the local regions of the face image and used the extended SRC for FR with a single sample per person.**  
Yang et al.ì€ ì–¼êµ´ ì´ë¯¸ì§€ì˜ ë¡œì»¬ ì˜ì—­ì—ì„œ local adaptive convolution featuresì„ ì¶”ì¶œí•˜ê³  1ì¸ë‹¹ ë‹¨ì¼ ìƒ˜í”Œë¡œ FRìš© extended SRCë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.

<br>

**Guo et al. combined deep features and the SVM classifier to perform recognition.**  
Guoet al.ì€ deep featuresê³¼ SVM ë¶„ë¥˜ê¸°ë¥¼ ê²°í•©í•˜ì—¬ ì¸ì‹ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

<br>

**Wang et al. first used product quantization (PQ) to directly retrieve the topk most similar faces and re-ranked these faces by combining similarities from deep features and the COTS matcher.**  
Wang et al.ì€ ë¨¼ì € product quantization(PQ)ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ì¥ ìœ ì‚¬í•œ ì–¼êµ´ì„ ì§ì ‘ ê²€ìƒ‰í•˜ê³  Deep Featuresê³¼ COTS ë§¤ì²˜ì˜ ìœ ì‚¬ì„±ì„ ê²°í•©í•˜ì—¬ ì´ëŸ¬í•œ ì–¼êµ´ì˜ ìˆœìœ„ë¥¼ ë‹¤ì‹œ ë§¤ê²¼ìŠµë‹ˆë‹¤.

<br>

**In addition, Softmax can be also used in face matching when the identities of training set and test set overlap.**  
ë˜í•œ SoftmaxëŠ” Train ì„¸íŠ¸ì™€ test ì„¸íŠ¸ì˜ IDê°€ ì¤‘ë³µë˜ëŠ” ê²½ìš° ì–¼êµ´ ë§¤ì¹­ì—ë„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<br>

**For example, in Challenge 2 of MS-celeb-1M, Ding et al. [142] trained a 21,000-class softmax classifier to directly recognize faces of one-shot classes and normal classes after augmenting feature by a conditional GAN; Guo et al. trained the softmax classifier combined with underrepresented-classes promotion (UP) loss term to enhance the performance on one-shot classes.**  
ì˜ˆë¥¼ ë“¤ì–´ MS-celeb-1Mì˜ Challenge 2ì—ì„œ Ding et al. [142]ì€ conditional GANìœ¼ë¡œ Featuresì„ ë³´ê°•í•œ í›„ ì›ìƒ· í´ë˜ìŠ¤ì™€ ì¼ë°˜ í´ë˜ìŠ¤ì˜ ì–¼êµ´ì„ ì§ì ‘ ì¸ì‹í•˜ë„ë¡ 21,000 í´ë˜ìŠ¤ì˜ Softmax ë¶„ë¥˜ê¸°ë¥¼ Trainí–ˆìŠµë‹ˆë‹¤. Guoet al.ì€ ì›ìƒ· í´ë˜ìŠ¤ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ underrepresented-classes promotion (UP) loss termê³¼ ê²°í•©ëœ Softmax classifierë¥¼ Trainí–ˆìŠµë‹ˆë‹¤.

<br>

**When the distributions of training data and testing data are the same, the face matching methods mentioned above are effective.**  
training dataì™€ testing data ì˜ ë¶„í¬ê°€ ê°™ì„ ë•Œ ìœ„ì—ì„œ ì–¸ê¸‰í•œ ì–¼êµ´ ë§¤ì¹­ ë°©ë²•ì´ íš¨ê³¼ì ì…ë‹ˆë‹¤.

<br>

**However, there is always a distribution change or domain shift between two data domains that can degrade the performance on test data.**  
ê·¸ëŸ¬ë‚˜ Test Dataì˜ ì„±ëŠ¥ì„ ì €í•˜ì‹œí‚¬ ìˆ˜ ìˆëŠ” ë‘ ë°ì´í„° ë„ë©”ì¸ ì‚¬ì´ì—ëŠ” í•­ìƒ distribution change ë˜ëŠ” domain shiftì´ ìˆìŠµë‹ˆë‹¤.

<br>

**Transfer learning has recently been introduced into deep FR to address the problem of domain shift.**  
domain shift ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ Transfer learningì´ ìµœê·¼ ì‹¬ì¸µ FRì— ë„ì…ë˜ì—ˆìŠµë‹ˆë‹¤.

<br>

**It learns transferable features using a labeled source domain (training data) and an unlabeled target domain (testing data) such that domain discrepancy is reduced and models trained on source domain will also perform well on target domain.**  
ë ˆì´ë¸”ì´ ì§€ì •ëœ ì†ŒìŠ¤ ë„ë©”ì¸(training data)ê³¼ ë ˆì´ë¸”ì´ ì§€ì •ë˜ì§€ ì•Šì€ ëŒ€ìƒ ë„ë©”ì¸(testing data)ì„ ì‚¬ìš©í•˜ì—¬ ì´ì „ transferable featuresì„ í•™ìŠµí•˜ë¯€ë¡œ domain discrepancyê°€ ì¤„ì–´ë“¤ê³  ì†ŒìŠ¤ ë„ë©”ì¸ì—ì„œ Trainëœ Modelì´ ëŒ€ìƒ ë„ë©”ì¸ì—ì„œë„ ì˜ ìˆ˜í–‰ë©ë‹ˆë‹¤.

<br>

**Sometimes, this technology is applied to face matching. For example, Crosswhite et al. and Xiong et al. adopted template adaptation to the set of media in a template by combining CNN features with template-specific linear SVMs.**  
ë•Œë•Œë¡œ ì´ ê¸°ìˆ ì€ ì–¼êµ´ ë§¤ì¹­ì— ì ìš©ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ Crosswhite et al. ë° Xiong et al.ì€ CNN featuresê³¼ template-specific linear SVMsì„ ê²°í•©í•˜ì—¬ templateì˜ ë¯¸ë””ì–´ ì§‘í•©ì— ëŒ€í•œ template ì ì‘ì„ ì±„íƒí–ˆìŠµë‹ˆë‹¤.

<br>

**But most of the time, it is not enough to do transfer learning only at face matching stage.**  
ê·¸ëŸ¬ë‚˜ ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ì–¼êµ´ ë§¤ì¹­ ë‹¨ê³„ì—ì„œë§Œ transfer learningì„ í•˜ëŠ” ê²ƒìœ¼ë¡œëŠ” ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

<br>

**Transfer learning should be embedded in deep models to learn more transferable representations. Kan et al. proposed a bi-shifting autoencoder network (BAE) for domain adaptation across view angle, ethnicity, and imaging sensor; while Luo et al. utilized the multi-kernels maximum mean discrepancy (MMD) to reduce domain discrepancies.**  
transfer learningì€ ë” ë§ì€ ì „ì´ ê°€ëŠ¥í•œ í‘œí˜„ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ Deep Modelì— í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. Kanet al.ì€ ì‹œì•¼ê°, ì¸ì¢… ë° ì´ë¯¸ì§• ì„¼ì„œì— ê±¸ì¹œ ë„ë©”ì¸ ì ì‘ì„ ìœ„í•œ BAE(bi-shifting autoencoder network)ë¥¼ ì œì•ˆí–ˆìŠµë‹ˆë‹¤. ë°˜ë©´ Luo et al.ì€ domain discrepanciesë¥¼ ì¤„ì´ê¸° ìœ„í•´ multi-kernels maximum mean discrepancy(MMD)ë¥¼ í™œìš©í–ˆìŠµë‹ˆë‹¤.

<br>

**Sohn et al. used adversarial learning [150] to transfer knowledge from still image FR to video FR.**  
Sohn et al.ì€ ì •ì§€ ì´ë¯¸ì§€ FRì—ì„œ ë¹„ë””ì˜¤ FRë¡œ ì§€ì‹ì„ ì „ë‹¬í•˜ê¸° ìœ„í•´ ì ëŒ€ì  í•™ìŠµ(adversarial learning )[150]ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.

<br>

**Moreover, finetuning the CNN parameters from a prelearned model using a target training dataset is a particular type of transfer learning, and is commonly employed by numerous methods.**  
ë˜í•œ ëŒ€ìƒ target training datasetë¥¼ ì‚¬ìš©í•˜ì—¬ prelearned Modelì—ì„œ CNN ë§¤ê°œë³€ìˆ˜ë¥¼ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ê²ƒì€ íŠ¹ì • ìœ í˜•ì˜ Transfer Learningì´ë©° ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ì–‘í•œ ë°©ë²•ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.    

<br>
<br>
<br>
<br>

# IV. FACE PROCESSING FOR TRAINING AND RECOGNITION

<br>
<br>
<br>

**We present the development of face processing methods in chronological order in Fig. 12.**  
ìš°ë¦¬ëŠ” ê·¸ë¦¼ 12ì—ì„œ ì–¼êµ´ ì²˜ë¦¬ ë°©ë²•ì˜ ê°œë°œì„ ì—°ëŒ€ìˆœìœ¼ë¡œ ì œì‹œí•©ë‹ˆë‹¤.

<br>
<br>
<br>
  <img src="/assets/DeepFaceRecognitionSurvey/Fig_12.png">
<p align="center">
</p>
<br>
<br>

**As we can see from the figure, most papers attempted to perform face processing by autoencoder model in 2014 and 2015**  
ê·¸ë¦¼ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ ëŒ€ë¶€ë¶„ì˜ ë…¼ë¬¸ì€ 2014ë…„ê³¼ 2015ë…„ì— Autoencoder Modelë¡œ ì–¼êµ´ ì²˜ë¦¬ë¥¼ ì‹œë„í–ˆìŠµë‹ˆë‹¤. 

<br>

**while 3D model played an important role in 2016.**  
3D Modelì€ 2016ë…„ì— ì¤‘ìš”í•œ ì—­í• ì„ í–ˆìŠµë‹ˆë‹¤. 

<br>

**GAN [40] has drawn substantial attention from the deep learning and computer vision community since it was first proposed by Goodfellow et al.**  
GAN[40]ì€ Goodfellow ë“±ì´ ì²˜ìŒ ì œì•ˆí•œ ì´í›„ Deep Learning ë° computer vision communityì—ì„œ ìƒë‹¹í•œ ê´€ì‹¬ì„ ëŒì—ˆìŠµë‹ˆë‹¤.

<br>

**It can be used in different fields and was also introduced into face processing in 2017.**  
ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©ì´ ê°€ëŠ¥í•˜ë©° 2017ë…„ì—ëŠ” face processingì—ë„ ë„ì…ë˜ì—ˆìŠµë‹ˆë‹¤.

<br>

**GAN can be used to perform â€œone-tomany augmentationâ€ and â€œmany-to-one normalizationâ€, and it broke the limit that face synthesis should be done under supervised way.**  
GANì€ "one-to-many augmentation" ë° "many-to-one normalization"ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©° ê°ë… ë°©ì‹ìœ¼ë¡œ ì–¼êµ´ í•©ì„±ì„ ìˆ˜í–‰í•´ì•¼ í•˜ëŠ” í•œê³„ë¥¼ ê¹¨ëœ¨ë ¸ìŠµë‹ˆë‹¤.

<br>

**Although GAN has not been widely used in face processing for training and recognition, it has great latent capacity for preprocessing, for example, Dual-Agent GANs (DA-GAN) won the 1st places on verification and identification tracks in the NIST IJB-A 2017 FR competitions.**  
GANì€ Train ë° recognitionì„ ìœ„í•œ ì–¼êµ´ ì²˜ë¦¬ì— ë„ë¦¬ ì‚¬ìš©ë˜ì§€ëŠ” ì•Šì•˜ì§€ë§Œ preprocessingë¥¼ ìœ„í•œ ì ì¬ ëŠ¥ë ¥ì´ í½ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ DA-GAN(Dual-Agent GAN)ì€ NIST IJB-Aì˜ 2017ë…„ í”„ë‘ìŠ¤ ëŒ€íšŒì—ì„œ verification and identification ë¶€ë¬¸ì—ì„œ 1ìœ„ë¥¼ ì°¨ì§€í–ˆìŠµë‹ˆë‹¤.      

<br>
<br>
<br>

### A. One-to-Many Augmentation

<br>
<br>
<br>

**Collecting a large database is extremely expensive and time consuming.**  
ëŒ€ê·œëª¨ Database ìˆ˜ì§‘ì€ ë¹„ìš©ê³¼ ì‹œê°„ì´ ë§ì´ ì†Œìš”ë©ë‹ˆë‹¤.

<br>

**The methods of â€œone-to-many augmentationâ€ can mitigate the challenges of data collection, and they can be used to augment not only training data but also the gallery of test data.**  
"one-to-many augmentation" ë°©ë²•ì€ ë°ì´í„° ìˆ˜ì§‘ ë¬¸ì œë¥¼ ì™„í™”í•  ìˆ˜ ìˆìœ¼ë©° Train ë°ì´í„°ë¿ë§Œ ì•„ë‹ˆë¼ Test Data Galleryë„ í™•ëŒ€í•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<br>

**we categorized them into four classes: data augmentation, 3D model, autoencoder model and GAN model.**  
One-to-Many Augmentationì€ data augmentation, 3D Model, autoencoder model ë° GAN Modelì˜ ë„¤ ê°€ì§€ í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.    

<br>
<br>
<br>

#### Data augmentation

<br>
<br>
<br>

**Common data augmentation methods consist of photometric transformations and geometric transformations, such as oversampling (multiple patches obtained by cropping at different scales), mirroring, and rotating the images.**  
ì¼ë°˜ì ì¸ data augmentation ë°©ë²•ì€ oversampling(ì„œë¡œ ë‹¤ë¥¸ ì¶•ì²™ìœ¼ë¡œ ì˜ë¼ì„œ ì–»ì€ ì—¬ëŸ¬ íŒ¨ì¹˜), ë¯¸ëŸ¬ë§ ë° ì´ë¯¸ì§€ íšŒì „ê³¼ ê°™ì€ ê´‘ë„ ë³€í™˜ ë° ê¸°í•˜í•™ì  ë³€í™˜ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

<br>

**Recently, data augmentation has been widely used in deep FR algorithms.**  
ìµœê·¼ ë°ì´í„° ì¦ê°€ëŠ” ì‹¬ì¸µ FR ì•Œê³ ë¦¬ì¦˜ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.

<br>

**for example, Sun et al. cropped 400 face patches varying in positions, scales, and color channels and mirrored the images.**  
ì˜ˆë¥¼ ë“¤ì–´ Sun et al.ì€ ìœ„ì¹˜, í¬ê¸° ë° ìƒ‰ìƒ ì±„ë„ì´ ë‹¤ë¥¸ 400ê°œì˜ ì–¼êµ´ íŒ¨ì¹˜ë¥¼ ìë¥´ê³  ì´ë¯¸ì§€ë¥¼ ë¯¸ëŸ¬ë§í–ˆìŠµë‹ˆë‹¤.

<br>

**Liu et al. generated seven overlapped image patches centered at different landmarks on the face region and trained them with seven CNNs with the same structure.**  
Liu et al.ì€ ì–¼êµ´ ì˜ì—­ì˜ ì„œë¡œ ë‹¤ë¥¸ ëœë“œë§ˆí¬ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ 7ê°œì˜ ì¤‘ì²©ëœ ì´ë¯¸ì§€ íŒ¨ì¹˜ë¥¼ ìƒì„±í•˜ê³  ë™ì¼í•œ êµ¬ì¡°ë¥¼ ê°€ì§„ 7ê°œì˜ CNNìœ¼ë¡œ Trainí–ˆìŠµë‹ˆë‹¤.

<br>
<br>
<br>

#### 3D model

<br>
<br>
<br> 

**3D face reconstruction is also a way to enrich the diversity of training data.**  
3D ì–¼êµ´ ì¬êµ¬ì„±ì€ Train ë°ì´í„°ì˜ ë‹¤ì–‘ì„±ì„ í’ë¶€í•˜ê²Œ í•˜ëŠ” ë°©ë²•ì´ê¸°ë„ í•©ë‹ˆë‹¤.

<br>

**They utilize 3D structure information to model the transformation between poses.**  
3D êµ¬ì¡° ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ í¬ì¦ˆ ê°„ì˜ ë³€í˜•ì„ Modelë§í•©ë‹ˆë‹¤.

<br>

**3D models first use 3D face data to obtain morphable displacement fields and then apply them to obtain 2D face data in different pose angles.**  
3D Modelì€ ë¨¼ì € 3D ì–¼êµ´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë³€í˜• ê°€ëŠ¥í•œ ë³€ìœ„ í•„ë“œë¥¼ ì–»ì€ ë‹¤ìŒ ì´ë¥¼ ì ìš©í•˜ì—¬ ë‹¤ì–‘í•œ í¬ì¦ˆ ê°ë„ì—ì„œ 2D ì–¼êµ´ ë°ì´í„°ë¥¼ ì–»ìŠµë‹ˆë‹¤.

<br>

**There is a large number of papers about this domain, but we only focus on the 3D face reconstruction using deep methods or used for deep FR.**  
ì´ ì˜ì—­ì— ëŒ€í•œ ë§ì€ ë…¼ë¬¸ì´ ìˆì§€ë§Œ ìš°ë¦¬ëŠ” deep methodë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ deep FRì— ì‚¬ìš©ë˜ëŠ” 3D ì–¼êµ´ ì¬êµ¬ì„±ì—ë§Œ ì§‘ì¤‘í•©ë‹ˆë‹¤.

<br>

**Masi et al. generated face images with new intra-class facial appearance variations, including pose, shape and expression, and then trained a 19-layer VGGNet with both real and augmented data.**  
Masi et al.ì€ í¬ì¦ˆ, ëª¨ì–‘ ë° í‘œì •ì„ í¬í•¨í•œ ìƒˆë¡œìš´ í´ë˜ìŠ¤ ë‚´ ì–¼êµ´ ëª¨ì–‘ ë³€í˜•ìœ¼ë¡œ ì–¼êµ´ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•œ ë‹¤ìŒ ì‹¤ì œ ë°ì´í„°ì™€ ì¦ê°• ë°ì´í„°ë¥¼ ëª¨ë‘ ì‚¬ìš©í•˜ì—¬ 19ê³„ì¸µ VGGNetì„ Trainí–ˆìŠµë‹ˆë‹¤.

<br>

**Masi et al. used generic 3D faces and rendered fixed views to reduce much of the computational effort.**  
Masi et al.ì€ ì¼ë°˜ 3D facesë¥¼ ì‚¬ìš©í•˜ê³  ê³ ì •ëœ ë·°ë¥¼ ë Œë”ë§í•˜ì—¬ ë§ì€ ê³„ì‚° ì‘ì—…ì„ ì¤„ì˜€ìŠµë‹ˆë‹¤.

<br>

**Richardson et al. employed an iterative 3D CNN by using a secondary input channel to represent the previous networkâ€™s output as an image for reconstructing a 3D face as shown in Fig. 13.**  
Richardson et al.ì€ ê·¸ë¦¼ 13ê³¼ ê°™ì´ 3D ì–¼êµ´ì„ ì¬êµ¬ì„±í•˜ê¸° ìœ„í•œ ì´ë¯¸ì§€ë¡œ ì´ì „ Networkì˜ ì¶œë ¥ì„ í‘œí˜„í•˜ê¸° ìœ„í•´ ë³´ì¡° ì…ë ¥ ì±„ë„ì„ ì‚¬ìš©í•˜ì—¬ ë°˜ë³µ 3D CNNì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.

<br>

**Dou et al. used a multi-task CNN to divide 3D face reconstruction into neutral 3D reconstruction and expressive 3D reconstruction.**  
Dou et al.ì€ multi-task CNNì„ ì‚¬ìš©í•˜ì—¬ 3D ì–¼êµ´ ì¬êµ¬ì„±ì„ neutral 3D reconstructionê³¼ expressive 3D reconstructionìœ¼ë¡œ ë‚˜ëˆ„ì—ˆìŠµë‹ˆë‹¤.

<br>

**Tran et al. directly regressed 3D morphable face model (3DMM) [155] parameters from an input photo by a very deep CNN architecture.**  
Tran et al.ì€ 3DMM(3D morphable face model)[155] ë§¤ê°œë³€ìˆ˜ë¥¼ very deep CNN Architectureì— ì˜í•´ ì…ë ¥ ì‚¬ì§„ì—ì„œ ì§ì ‘ íšŒê·€(regress)í–ˆìŠµë‹ˆë‹¤.

<br>

**An et al. synthesized face images with various poses and expressions using the 3DMM method, then reduced the gap between synthesized data and real data with the help of MMD.**  
An et al.ì€ ë‹¤ì–‘í•œ í¬ì¦ˆì™€ í‘œì •ì˜ ì–¼êµ´ ì´ë¯¸ì§€ë¥¼ 3DMM ë°©ì‹ìœ¼ë¡œ í•©ì„±í•œ í›„ MMDë¥¼ í†µí•´ synthesized data ì™€ real data ì˜ ê²©ì°¨ë¥¼ ì¤„ì˜€ìŠµë‹ˆë‹¤.

<br>
<br>
<br>
  <img src="/assets/DeepFaceRecognitionSurvey/Fig_13.png">
<p align="center">
</p>
<br>
<br>

### Autoencoder model

<br>
<br>
<br>

**Rather than reconstructing 3D models from a 2D image and projecting it back into 2D images of different poses, autoencoder models can generate 2D target images directly.**  
2D ì´ë¯¸ì§€ì—ì„œ 3D Modelì„ ì¬êµ¬ì„±í•˜ê³  ë‹¤ë¥¸ í¬ì¦ˆì˜ 2D ì´ë¯¸ì§€ë¡œ ë‹¤ì‹œ íˆ¬ì˜í•˜ëŠ” ëŒ€ì‹  Autoencoder Modelì€ 2D ëŒ€ìƒ ì´ë¯¸ì§€ë¥¼ ì§ì ‘ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<br>

**Taken a face image and a pose code encoding a target pose as input, an encoder first learns pose-invariant face representation, and then a decoder generates a face image with the same identity viewed at the target pose by using the pose-invariant representation and the pose code.**  
ì–¼êµ´ ì´ë¯¸ì§€ì™€ ëª©í‘œ í¬ì¦ˆë¥¼ Encodingí•œ í¬ì¦ˆ ì½”ë“œë¥¼ ì…ë ¥ë°›ì•„ EncoderëŠ” ë¨¼ì € í¬ì¦ˆ ë¶ˆë³€ ì–¼êµ´ í‘œí˜„(pose-invariant face representation)ì„ í•™ìŠµí•˜ê³ , DecoderëŠ” pose-invariant representationê³¼ pose codeì„ ì´ìš©í•˜ì—¬ ëª©í‘œ í¬ì¦ˆì—ì„œ ë³¸ ë™ì¼í•œ ì •ì²´ì„±ì„ ê°€ì§„ ì–¼êµ´ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•œë‹¤.

<br>

**For example, given the target pose codes, multi-view perceptron (MVP) trained some deterministic hidden neurons to learn pose-invariant face representations, and simultaneously trained some random hidden neurons to capture pose features, then a decoder generated the target images by combining poseinvariant representations with pose features.**  
ì˜ˆë¥¼ ë“¤ì–´, ëŒ€ìƒ í¬ì¦ˆ ì½”ë“œê°€ ì£¼ì–´ì§€ë©´ multi-view perceptron(MVP)ì€ pose-invariant face representationì„ í•™ìŠµí•˜ê¸° ìœ„í•´ ì¼ë¶€ ê²°ì •ë¡ ì  ìˆ¨ê²¨ì§„ ë‰´ëŸ°ì„ Trainì‹œí‚¤ê³  ë™ì‹œì— í¬ì¦ˆ íŠ¹ì§•ì„ ìº¡ì²˜í•˜ê¸° ìœ„í•´ ì¼ë¶€ ì„ì˜ì˜ ìˆ¨ê²¨ì§„ ë‰´ëŸ°ì„ Trainì‹œí‚¨ ë‹¤ìŒ DecoderëŠ” ë‹¤ìŒì„ ê²°í•©í•˜ì—¬ ëŒ€ìƒ ì´ë¯¸ì§€ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.

<br>

**As shown in Fig. 14, Yim et al. and Qian et al. introduced an auxiliary CNN to generate better images viewed at the target poses.**  
ê·¸ë¦¼ 14ì— ë„ì‹œëœ ë°”ì™€ ê°™ì´, Yim et al. ë° Qian et al.ì€ target posesì—ì„œ ë” ë‚˜ì€ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ ë³´ì¡° CNNì„ ë„ì…í–ˆìŠµë‹ˆë‹¤.

<br>
<br>
<br>
  <img src="/assets/DeepFaceRecognitionSurvey/Fig_14.png">
<p align="center">
</p>
<br>
<br>

**First, an autoencoder generated the desired pose image, then the auxiliary CNN reconstructed the original input image back from the generated target image, which guarantees that the generated image is identity-preserving.**  
ë¨¼ì € Autoencoderê°€ ì›í•˜ëŠ” í¬ì¦ˆ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•œ ë‹¤ìŒ ë³´ì¡° CNNì´ ìƒì„±ëœ ëŒ€ìƒ ì´ë¯¸ì§€ì—ì„œ ì›ë³¸ ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ë‹¤ì‹œ ì¬êµ¬ì„±í•˜ì—¬ ìƒì„±ëœ ì´ë¯¸ì§€ê°€ IDë¥¼ ë³´ì¡´í•˜ë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤.

<br>

**Two groups of units are embedded between encoder and decoder.**  
ë‘ ê·¸ë£¹ì˜ unitsê°€ Encoderì™€ Decoder ì‚¬ì´ì— ë‚´ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

<br>

**The identity units remain unchanged and the rotation of images is achieved by taking actions to pose units at each time step.**  
identity unitsì€ ë³€ê²½ë˜ì§€ ì•Šê³  ì´ë¯¸ì§€ íšŒì „ì€ ê° ì‹œê°„ ë‹¨ê³„ì—ì„œ í¬ì¦ˆ ë‹¨ìœ„ì— ëŒ€í•œ ì¡°ì¹˜ë¥¼ ì·¨í•¨ìœ¼ë¡œì¨ ë‹¬ì„±ë©ë‹ˆë‹¤.      

<br>
<br>
<br>

### GAN model

<br>
<br>
<br>

**In GAN models, a generator aims to fool a discriminator through generating images that resemble the real images, while the discriminator aims to discriminate the generated samples from the real ones.**  
GAN Modelì—ì„œ GeneratorëŠ” ì‹¤ì œ ì´ë¯¸ì§€ì™€ ìœ ì‚¬í•œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ì—¬ íŒë³„ìë¥¼ ì†ì´ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•˜ê³ , DiscriminatorëŠ” ìƒì„±ëœ ìƒ˜í”Œê³¼ ì‹¤ì œ ìƒ˜í”Œì„ êµ¬ë³„í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.

<br>

**By this minimax game between generator and discriminator, GAN can successfully generate photo-realistic images with different poses.**  
GANì€ generatorì™€ discriminator ì‚¬ì´ì˜ ì´ ë¯¸ë‹ˆë§¥ìŠ¤ ê²Œì„ì„ í†µí•´ ë‹¤ì–‘í•œ í¬ì¦ˆë¡œ ì‚¬ì§„ê³¼ ê°™ì€ ì´ë¯¸ì§€ë¥¼ ì„±ê³µì ìœ¼ë¡œ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<br>

**After using a 3D model to generate profile face images, DA-GAN[56] refined the images by a GAN, which combines prior knowledge of the data distribution and knowledge of faces (pose and identity perception loss).**  
3D Modelì„ ì‚¬ìš©í•˜ì—¬ í”„ë¡œí•„ ì–¼êµ´ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•œ í›„ DA-GAN[56]ì€ ë°ì´í„° ë¶„í¬ì— ëŒ€í•œ data distributionì™€ knowledge of faces (pose and identity perception loss)ì„ ê²°í•©í•˜ëŠ” GANìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ì •ì œí–ˆìŠµë‹ˆë‹¤.

<br>

**CVAE-GAN [159] combined a variational auto-encoder with a GAN for augmenting data, and took advantages of both statistic and pairwise feature matching to make the training process converge faster and more stably.**  
CVAE-GAN[159]ì€ variational auto-encoderë¥¼ GANê³¼ ê²°í•©í•˜ì—¬ ë°ì´í„°ë¥¼ ë³´ê°•í•˜ê³  í†µê³„ ë° ìŒë³„ íŠ¹ì§• ë§¤ì¹­ì„ ëª¨ë‘ í™œìš©í•˜ì—¬ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ê°€ ë” ë¹ ë¥´ê³  ì•ˆì •ì ìœ¼ë¡œ ìˆ˜ë ´ë˜ë„ë¡ í–ˆìŠµë‹ˆë‹¤.

<br>

**In addition to synthesizing diverse faces from noise, some papers also explore to disentangle the identity and variation, and synthesize new faces by exchanging identity and variation from different people.**  
ë…¸ì´ì¦ˆì—ì„œ ë‹¤ì–‘í•œ ì–¼êµ´ì„ í•©ì„±í•˜ëŠ” ê²ƒ ì™¸ì—ë„ ì¼ë¶€ ë…¼ë¬¸ì—ì„œëŠ” ì •ì²´ì„±ê³¼ ë³€ì´ë¥¼ í’€ê³  ë‹¤ë¥¸ ì‚¬ëŒì˜ ì •ì²´ì„±ê³¼ ë³€ì´ë¥¼ êµí™˜í•˜ì—¬ ìƒˆë¡œìš´ ì–¼êµ´ì„ í•©ì„±í•˜ëŠ” ë°©ë²•ì„ ëª¨ìƒ‰í•©ë‹ˆë‹¤.

<br>

**In CG-GAN, a generator directly resolves each representation of input image into a variation code and an identity code and regroups these codes for cross-generating, simultaneously, a discriminator ensures the reality of generated images.**  
CG-GANì—ì„œ generatorëŠ” ì…ë ¥ ì´ë¯¸ì§€ì˜ ê° í‘œí˜„ì„ ë³€í˜• ì½”ë“œì™€ ì‹ë³„ ì½”ë“œë¡œ ì§ì ‘ í•´ê²°í•˜ê³  ì´ëŸ¬í•œ ì½”ë“œë¥¼ ë‹¤ì‹œ ê·¸ë£¹í™”í•˜ì—¬ êµì°¨ ìƒì„±í•˜ëŠ” ë™ì‹œì— discriminatorëŠ” ìƒì„±ëœ ì´ë¯¸ì§€ì˜ ì‚¬ì‹¤ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.

<br>

**Bao et al. extracted identity representation of one input image and attribute representation of any other input face image, then synthesized new faces by recombining these representations.**  
Bao et al.ì€ í•˜ë‚˜ì˜ ì…ë ¥ ì´ë¯¸ì§€ì˜ ì‹ ì› í‘œí˜„ê³¼ ë‹¤ë¥¸ ì…ë ¥ ì–¼êµ´ ì´ë¯¸ì§€ì˜ ì†ì„± í‘œí˜„ì„ ì¶”ì¶œí•œ ë‹¤ìŒ ì´ëŸ¬í•œ í‘œí˜„ì„ ì¬ê²°í•©í•˜ì—¬ ìƒˆë¡œìš´ ì–¼êµ´ì„ í•©ì„±í•©ë‹ˆë‹¤.

<br>

**This work shows superior performance in generating realistic and identity preserving face images, even for identities outside the training dataset.**  
ì´ ì‘ì—…ì€ training dataset ì™¸ë¶€ì˜ IDì— ëŒ€í•´ì„œë„ ì‚¬ì‹¤ì ì´ê³  IDë¥¼ ë³´ì¡´í•˜ëŠ” ì–¼êµ´ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ë° íƒì›”í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

<br>

**Unlike previous methods that treat classifier as a spectator, FaceID-GAN [162] proposed a three-player GAN where the classifier cooperates together with the discriminator to compete with the generator from two different aspects, i.e. facial identity and image quality respectively.**  
classifierë¥¼ ê´€ì¤‘ìœ¼ë¡œ ì·¨ê¸‰í•˜ëŠ” ì´ì „ ë°©ë²•ê³¼ ë‹¬ë¦¬ FaceID-GAN[162]ì€ classifierê°€ discriminatorì™€ í˜‘ë ¥í•˜ì—¬ ë‘ ê°€ì§€ ë‹¤ë¥¸ ì¸¡ë©´, ì¦‰ ê°ê° ì–¼êµ´ ì •ì²´ì„±ê³¼ ì´ë¯¸ì§€ í’ˆì§ˆì—ì„œ ìƒì„±ìì™€ ê²½ìŸí•˜ëŠ” three-player GANì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤.

<br>
<br>
<br>

## B. Many-to-One Normalization

<br>
<br>
<br>

**In contrast to â€œone-to-many augmentationâ€, the methods of â€œmany-to-one normalizationâ€ produce frontal faces and reduce appearance variability of test data to make faces align and compare easily. It can be categorized as autoencoder model, CNN model and GAN model.**  
"one-to-many augmentation"ì™€ ë‹¬ë¦¬ "many-to-one normalization" ë°©ë²•ì€ ì •ë©´ ì–¼êµ´ì„ ìƒì„±í•˜ê³  Test Dataì˜ ëª¨ì–‘ ë³€ë™ì„±ì„ ì¤„ì—¬ ì–¼êµ´ì„ ì‰½ê²Œ ì •ë ¬í•˜ê³  ë¹„êµí•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. Autoencoder Model, CNN Model ë° GAN Modelë¡œ ë¶„ë¥˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.    

<br>
<br>
<br>

### Autoencoder model

<br>
<br>
<br>


**Autoencoder can also be applied to â€œmany-to-one normalizationâ€.**  
AutoencoderëŠ” "many-to-one normalization"ì—ë„ ì ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<br>

**Different from the autoencoder model in â€œone-to-many augmentationâ€ which generates the desired pose images with the help of pose codes, autoencoder model here learns pose-invariant face representation by an encoder and directly normalizes faces by a decoder without pose codes.**  
í¬ì¦ˆ ì½”ë“œì˜ ë„ì›€ìœ¼ë¡œ ì›í•˜ëŠ” í¬ì¦ˆ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” "one-to-many augmentation"ì˜ Autoencoder Modelê³¼ ë‹¬ë¦¬, ì—¬ê¸°ì„œ Autoencoder Modelì€ í¬ì¦ˆ ì½”ë“œ ì—†ì´ í¬ì¦ˆ ë¶ˆë³€ ì–¼êµ´ í‘œí˜„ì„ í•™ìŠµí•˜ê³  Decoderë¡œ ì–¼êµ´ì„ ì§ì ‘ ì •ê·œí™”í•©ë‹ˆë‹¤. 

<br>

**Zhu et al. selected canonicalview images according to the face imagesâ€™ symmetry and sharpness and then adopted an autoencoder to recover the frontal view images by minimizing the reconstruction loss error.**  
Zhuet al.ì€ ì–¼êµ´ ì˜ìƒì˜ ëŒ€ì¹­ì„±ê³¼ ì„ ëª…ë„ì— ë”°ë¼ canonicalview ì˜ìƒì„ ì„ íƒí•˜ê³  autoencoderë¥¼ ì±„íƒí•˜ì—¬ ì¬êµ¬ì„± ì†ì‹¤ ì˜¤ë¥˜ë¥¼ ìµœì†Œí™”í•˜ì—¬ ì •ë©´ ì˜ìƒì„ ë³µêµ¬í•©ë‹ˆë‹¤.

<br>

**The proposed stacked progressive autoencoders (SPAE) progressively map the nonfrontal face to the frontal face through a stack of several autoencoders.**  
ì œì•ˆëœ SPAE(Stacked Progressive Autoencoders)ëŠ” ì—¬ëŸ¬ ìë™ Encoder ìŠ¤íƒì„ í†µí•´ nonfrontal faceë¥¼ frontal faceë¡œ ì ì§„ì ìœ¼ë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.

<br>

**Each shallow autoencoders of SPAE is designed to convert the input face images at large poses to a virtual view at a smaller pose, so the pose variations are narrowed down gradually layer by layer along the pose manifold.**  
SPAEì˜ ê°ê°ì˜ shallow AutoencoderëŠ” í° í¬ì¦ˆì˜ ì…ë ¥ ì–¼êµ´ ì´ë¯¸ì§€ë¥¼ ë” ì‘ì€ í¬ì¦ˆì˜ virtual viewë¡œ ë³€í™˜í•˜ë„ë¡ ì„¤ê³„ë˜ì–´ í¬ì¦ˆ ë³€í˜•ì´ í¬ì¦ˆ ë§¤ë‹ˆí´ë“œë¥¼ ë”°ë¼ ë ˆì´ì–´ë³„ë¡œ ì ì§„ì ìœ¼ë¡œ ì¢í˜€ì§‘ë‹ˆë‹¤. 

<br>

**Zhang et al. built a sparse many-to-one encoder to enhance the discriminant of the pose free feature by using multiple random faces as the target values for multiple encoders.**  
Zhang et al.ì€ ì—¬ëŸ¬ Encoderì˜ ëŒ€ìƒ ê°’ìœ¼ë¡œ ì—¬ëŸ¬ ì„ì˜ì˜ ì–¼êµ´ì„ ì‚¬ìš©í•˜ì—¬ í¬ì¦ˆ ì—†ëŠ” ê¸°ëŠ¥ì˜ íŒë³„ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ í¬ì†Œí•œ many-to-one encoderë¥¼ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤.    

<br>
<br>
<br>

### CNN model

<br>
<br>
<br>

**CNN models usually directly learn the 2D mappings between non-frontal face images and frontal images, and utilize these mapping to normalize images in pixel space.**  
CNN Modelì€ ì¼ë°˜ì ìœ¼ë¡œ non-frontal face imagesì™€ frontal imagesê°„ì˜ 2D ë§¤í•‘ì„ ì§ì ‘ í•™ìŠµí•˜ê³  ì´ëŸ¬í•œ ë§¤í•‘ì„ í™œìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ í”½ì…€ ê³µê°„ì—ì„œ ì •ê·œí™”í•©ë‹ˆë‹¤.

<br>

**The pixels in normalized images are either directly the pixels or the combinations of the pixels in non-frontal images.**  
ì •ê·œí™”ëœ ì´ë¯¸ì§€ì˜ í”½ì…€ì€ ë°”ë¡œ í”½ì…€ì´ê±°ë‚˜ non-frontal imagesì˜ í”½ì…€ ì¡°í•©ì…ë‹ˆë‹¤.

<br>

**In LDF-Net, the displacement field network learns the shifting relationship of two pixels, and the translation layer transforms the input non-frontal face image into a frontal one with this displacement field.**  
LDF-Netì—ì„œ displacement field NetworkëŠ” ë‘ í”½ì…€ì˜ ì´ë™ ê´€ê³„ë¥¼ í•™ìŠµí•˜ê³  ë³€í™˜ ë ˆì´ì–´ëŠ” ì´ ë³€ìœ„ í•„ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ëœ non-frontal face imageë¥¼ frontal ì´ë¯¸ì§€ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

<br>

**In GridFace shown in Fig. 15, first, the rectification network normalizes the images by warping pixels from the original image to the canonical one according to the computed homography matrix,**  
ê·¸ë¦¼ 15ì˜ GridFaceì—ì„œ ë¨¼ì € ë³´ì • NetworkëŠ” ê³„ì‚°ëœ í˜¸ëª¨ê·¸ë˜í”¼ í–‰ë ¬ì— ë”°ë¼ ì›ë³¸ ì´ë¯¸ì§€ì—ì„œ í‘œì¤€ ì´ë¯¸ì§€ë¡œ í”½ì…€ì„ ì›Œí•‘í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ì •ê·œí™”í•œ ë‹¤ìŒ ì •ê·œí™”ëœ ì¶œë ¥ì„ ì•”ì‹œì  í‘œì¤€ ë·° í˜ì´ìŠ¤ì— ì˜í•´ ì •ê·œí™”í•©ë‹ˆë‹¤. 

<br>

**then the normalized output is regularized by an implicit canonical view face prior, finally, with the normalized faces as input, the recognition network learns discriminative face representation via metric learning.**  
ê·¸ëŸ° ë‹¤ìŒ,ì •ê·œí™”ëœ ì–¼êµ´ì„ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ì¸ì‹ NetworkëŠ” ë©”íŠ¸ë¦­ í•™ìŠµì„ í†µí•´ ì°¨ë³„ì ì¸ ì–¼êµ´ í‘œí˜„ì„ í•™ìŠµí•©ë‹ˆë‹¤.

<br>
<br>
<br>
  <img src="/assets/DeepFaceRecognitionSurvey/Fig_15.png">
<p align="center">
</p>
<br>
<br>

### GAN model

<br>
<br>
<br>

**Huang et al. proposed a two-pathway generative adversarial network (TP-GAN) that contains four landmark-located patch networks and a global encoderdecoder network.**  
Huang et al.ì€ 4ê°œì˜ ëœë“œë§ˆí¬ì— ìœ„ì¹˜í•œ íŒ¨ì¹˜ Networkì™€ ê¸€ë¡œë²Œ Encoder/Decoder Networkë¥¼ í¬í•¨í•˜ëŠ” two-pathway generative adversarial network(TP-GAN)ë¥¼ ì œì•ˆí–ˆìŠµë‹ˆë‹¤.

<br>

**Through combining adversarial loss, symmetry loss and identity-preserving loss, TP-GAN generates a frontal view and simultaneously preserves global structures and local details as shown in Fig. 16.**  
adversarial loss, symmetry loss ë° identity-preserving lossì„ ê²°í•©í•˜ì—¬ TP-GANì€ ì •ë©´ ë·°ë¥¼ ìƒì„±í•˜ê³  ë™ì‹œì— ê·¸ë¦¼ 16ê³¼ ê°™ì´ ì „ì—­ êµ¬ì¡° ë° ë¡œì»¬ ì„¸ë¶€ ì •ë³´ë¥¼ ë³´ì¡´í•©ë‹ˆë‹¤.

<br>

**In a disentangled representation learning generative adversarial network (DR-GAN), the generator serves as a face rotator, in which an encoder produces an identity representation, and a decoder synthesizes a face at the specified pose using this representation and a pose code.**  
DR-GAN(disentangled representation learning generative adversarial network)ì—ì„œ generatorëŠ” Encoderê°€ ID í‘œí˜„ì„ ìƒì„±í•˜ê³  Decoderê°€ ì´ í‘œí˜„ê³¼ í¬ì¦ˆ ì½”ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì§€ì •ëœ í¬ì¦ˆì—ì„œ ì–¼êµ´ì„ í•©ì„±í•˜ëŠ” ì–¼êµ´ íšŒì „ê¸° ì—­í• ì„ í•©ë‹ˆë‹¤.

<br>

**And the discriminator is trained to not only distinguish real vs. synthetic images, but also predict the identity and pose of a face.**  
ê·¸ë¦¬ê³  discriminatorëŠ” ì‹¤ì œ ì´ë¯¸ì§€ì™€ í•©ì„± ì´ë¯¸ì§€ë¥¼ êµ¬ë³„í•  ë¿ë§Œ ì•„ë‹ˆë¼ ì–¼êµ´ì˜ ì •ì²´ì„±ê³¼ í¬ì¦ˆë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ Trainë©ë‹ˆë‹¤.

<br>

**Yin et al. incorporated 3DMM into the GAN structure to provide shape and appearance priors to guide the generator to frontalization.**  
Yin et al.ì€ 3DMMì„ GAN êµ¬ì¡°ì— í†µí•©í•˜ì—¬ generatorë¥¼ ì „ë©´í™”(frontalization)ë¡œ ì•ˆë‚´í•˜ê¸° ì „ì— ëª¨ì–‘ê³¼ ëª¨ì–‘ì„ ì œê³µí•©ë‹ˆë‹¤.    

<br>
<br>
<br>
  <img src="/assets/DeepFaceRecognitionSurvey/Fig_16.png">
<p align="center">
</p>
<br>
<br>
<br>
<br>
<br>

## V. FACE DATABASES AND EVALUATION PROTOCOLS

<br>
<br>
<br>

**In the past three decades, many face databases have been constructed with a clear tendency from small-scale to largescale, from single-source to diverse-sources, and from labcontrolled to real-world unconstrained condition, as shown in Fig. 17.**  
ì§€ë‚œ 30ë…„ ë™ì•ˆ ê·¸ë¦¼ 17ê³¼ ê°™ì´ ì†Œê·œëª¨ì—ì„œ ëŒ€ê·œëª¨ë¡œ, ë‹¨ì¼ ì†ŒìŠ¤ì—ì„œ ë‹¤ì–‘í•œ ì†ŒìŠ¤ë¡œ, ì‹¤í—˜ì‹¤ ì œì–´(labcontrolled)ì—ì„œ ì‹¤ì œ ë¹„ì œì•½ ì¡°ê±´(real-world unconstrained condition)ìœ¼ë¡œ ë§ì€ ì–¼êµ´ Databaseê°€ ëª…í™•í•œ ê²½í–¥ìœ¼ë¡œ êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤.

<br>

**As the performance of some simple databases become saturated, e.g. LFW, more and more complex databases were continually developed to facilitate the FR research.**
ì¼ë¶€ ë‹¨ìˆœ Databaseì˜ ì„±ëŠ¥ì´ í¬í™”ë¨ì— ë”°ë¼, ì˜ˆë¥¼ ë“¤ì–´. LFW, ì ì  ë” ë³µì¡í•œ Databaseê°€ FR ì—°êµ¬ë¥¼ ìš©ì´í•˜ê²Œ í•˜ê¸° ìœ„í•´ ì§€ì†ì ìœ¼ë¡œ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤.

<br>

**It can be said without exaggeration that the development process of the face databases largely leads the direction of FR research.**  
ì–¼êµ´ Databaseì˜ ê°œë°œ ê³¼ì •ì€ FR ì—°êµ¬ì˜ ë°©í–¥ì„±ì„ í¬ê²Œ ì´ëŒì–´ê°„ë‹¤ê³  í•´ë„ ê³¼ì–¸ì´ ì•„ë‹ˆë‹¤.

<br>

**In this section, we review the development of major training and testing academic databases for the deep FR.**  
ì´ ì„¹ì…˜ì—ì„œëŠ” Deep FRì„ ìœ„í•œ ì£¼ìš” training and testing academic databasesì˜ ê°œë°œì„ ê²€í† í•©ë‹ˆë‹¤.

<br>
<br>
<br>
  <img src="/assets/DeepFaceRecognitionSurvey/Fig_17.png">
<p align="center">
</p>
<br>
<br>

### A. Large-scale training data sets

<br>
<br>
<br>

**The prerequisite of effective deep FR is a sufficiently large training dataset.**  
íš¨ê³¼ì ì¸ Deep FRì˜ ì „ì œ ì¡°ê±´ì€ ì¶©ë¶„íˆ í° Train Datasetì…ë‹ˆë‹¤

<br>

**Zhou et al. [59] suggested that large amounts of data with deep learning improve the performance of FR.**  
Zhou et al.ì€ [59]ëŠ” Deep Learningì„ í†µí•´ ë§ì€ ì–‘ì˜ ë°ì´í„°ê°€ FRì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¨ë‹¤ê³  ì œì•ˆí–ˆìŠµë‹ˆë‹¤.

<br>

**The results of Megaface Challenge also revealed that premier deep FR methods were typically trained on data larger than 0.5M images and 20K people.**  
Megaface Challengeì˜ ê²°ê³¼ëŠ” ë˜í•œ ìµœê³ ì˜ deep FR ë°©ë²•ì´ ì¼ë°˜ì ìœ¼ë¡œ 0.5M ì´ë¯¸ì§€ì™€ 20,000ëª…ë³´ë‹¤ í° ë°ì´í„°ì— ëŒ€í•´ Trainë˜ì—ˆìŒì„ ë°í˜”ìŠµë‹ˆë‹¤.

<br>

**The early works of deep FR were usually trained on private training datasets.**  
Deep FRì˜ ì´ˆê¸° ì‘ì—…ì€ ì¼ë°˜ì ìœ¼ë¡œ ê°œì¸ Train Datasetì—ì„œ Trainë˜ì—ˆìŠµë‹ˆë‹¤.

<br>

**Facebookâ€™s Deepface model was trained on 4M images of 4K people; Googleâ€™s FaceNet was trained on 200M images of 3M people; DeepID serial models were trained on 0.2M images of 10K people.**  
Facebookì˜ Deepface Modelì€ 4,000ëª…ì— ëŒ€í•œ 4ë°±ë§Œ ê°œì˜ ì´ë¯¸ì§€ë¡œ Trainë˜ì—ˆìŠµë‹ˆë‹¤. Googleì˜ FaceNetì€ 3ë°±ë§Œ ëª…ì˜ ì´ë¯¸ì§€ 2ì–µ ê°œë¡œ Trainë˜ì—ˆìŠµë‹ˆë‹¤. DeepID serial Modelì€ 10,000ëª…ì— ëŒ€í•œ 0.2M ì´ë¯¸ì§€ë¡œ Trainë˜ì—ˆìŠµë‹ˆë‹¤.

<br>

**Although they reported ground-breaking performance at this stage, researchers cannot accurately reproduce or compare their models without public training datasets.**  
ì´ ë‹¨ê³„ì—ì„œ íšê¸°ì ì¸ ì„±ëŠ¥ì„ ë³´ê³ í–ˆì§€ë§Œ ì—°êµ¬ì›ì€ public training datasets ì—†ì´ëŠ” Modelì„ ì •í™•í•˜ê²Œ ì¬í˜„í•˜ê±°ë‚˜ ë¹„êµí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

<br>

**To address this issue, CASIA-Webface [120] provided the first widely-used public training dataset for the deep model training purpose, which consists of 0.5M images of 10K celebrities collected from the web.**  
ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ CASIA-Webface[120]ëŠ” ì›¹ì—ì„œ ìˆ˜ì§‘í•œ 10,000ëª…ì˜ ìœ ëª…ì¸ì‚¬ ì´ë¯¸ì§€ 0.5Më¡œ êµ¬ì„±ëœ Deep Model êµìœ¡ ëª©ì ìœ¼ë¡œ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ìµœì´ˆì˜ widely-used public training datasetë¥¼ ì œê³µí–ˆìŠµë‹ˆë‹¤.

<br>

**Given its moderate size and easy usage, it has become a great resource for fair comparisons for academic deep models.**  
ì ë‹¹í•œ í¬ê¸°ì™€ ì‰¬ìš´ ì‚¬ìš©ë²•ì„ ê°ì•ˆí•  ë•Œ í•™ë¬¸ì  ì‹¬ì¸µ Modelì— ëŒ€í•œ ê³µì •í•œ ë¹„êµë¥¼ ìœ„í•œ í›Œë¥­í•œ ë¦¬ì†ŒìŠ¤ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.

<br>

**However, its relatively small data and ID size may not be sufficient to reflect the power of many advanced deep learning methods.**  
ê·¸ëŸ¬ë‚˜ ìƒëŒ€ì ìœ¼ë¡œ ì‘ì€ ë°ì´í„°ì™€ ID í¬ê¸°ëŠ” ë§ì€ ê³ ê¸‰ Deep Learning ë°©ë²•ì˜ ì„±ëŠ¥ì„ ë°˜ì˜í•˜ê¸°ì— ì¶©ë¶„í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<br>

**Currently, there have been more databases providing public available large-scale training dataset (Table VI), especially three databases with over 1M images, namely MS-Celeb-1M, VGGface2, and Megaface, and we summary some interesting findings about these training sets, as shown in Fig. 18.**  
í˜„ì¬ ê³µê°œëœ ëŒ€ê·œëª¨ êµìœ¡ Dataset(í‘œ VI)ë¥¼ ì œê³µí•˜ëŠ” ë” ë§ì€ Database, íŠ¹íˆ MS-Celeb-1M, VGGface2 ë° Megafaceì™€ ê°™ì€ 1M ì´ìƒì˜ ì´ë¯¸ì§€ê°€ í¬í•¨ëœ 3ê°œì˜ Databaseê°€ ìˆìœ¼ë©° ì´ëŸ¬í•œ êµìœ¡ ì„¸íŠ¸ì— ëŒ€í•œ ëª‡ ê°€ì§€ í¥ë¯¸ë¡œìš´ ê²°ê³¼ë¥¼ ê·¸ë¦¼ 18ì— ë„ì‹œëœ ë°”ì™€ ê°™ì´ ìš”ì•½í•©ë‹ˆë‹¤.    

<br>
<br>
<br>
  <img src="/assets/DeepFaceRecognitionSurvey/Table_06.png">
<p align="center">
</p>
<br>
<br>
<br>
<br>
<br>
  <img src="/assets/DeepFaceRecognitionSurvey/Fig_18.png">
<p align="center">
</p>
<br>
<br>

### Depth v.s. breadth

<br>
<br>
<br>

**These large training sets are expanded from depth or breadth. VGGface2 provides a large-scale training dataset of depth, which have limited number of subjects but many images for each subjects.**  
ì´ëŸ¬í•œ ëŒ€ê·œëª¨ Train ì„¸íŠ¸ëŠ” ê¹Šì´ ë˜ëŠ” í­ì—ì„œ í™•ì¥ë©ë‹ˆë‹¤. VGGface2ëŠ” í”¼í—˜ìì˜ ìˆ˜ëŠ” ì œí•œë˜ì–´ ìˆì§€ë§Œ ê° í”¼í—˜ìì— ëŒ€í•œ ì´ë¯¸ì§€ê°€ ë§ì€ ëŒ€ê·œëª¨ í•™ìŠµ Datasetë¥¼ ì œê³µí•©ë‹ˆë‹¤.

<br>

**The depth of dataset enforces the trained model to address a wide range intraclass variations, such as lighting, age, and pose.**  
Datasetì˜ ê¹Šì´ëŠ” Trainëœ Modelì´ ì¡°ëª…, ì—°ë ¹ ë° í¬ì¦ˆì™€ ê°™ì€ ê´‘ë²”ìœ„í•œ í´ë˜ìŠ¤ ë‚´ ë³€í˜•ì„ ì²˜ë¦¬í•˜ë„ë¡ í•©ë‹ˆë‹¤.

<br>

**In contrast, MS-Celeb-1M and Mageface (Challenge 2) offers large-scale training datasets of breadth, which contains many subject but limited images for each subjects.**  
ëŒ€ì¡°ì ìœ¼ë¡œ MS-Celeb-1M ë° Mageface(Challenge 2)ëŠ” ë§ì€ ì£¼ì œë¥¼ í¬í•¨í•˜ì§€ë§Œ ê° ì£¼ì œì— ëŒ€í•´ ì œí•œëœ ì´ë¯¸ì§€ë¥¼ í¬í•¨í•˜ëŠ” í­ë„“ì€ ëŒ€ê·œëª¨ Train Datasetë¥¼ ì œê³µí•©ë‹ˆë‹¤.

<br>

**The breadth of dataset ensures the trained model to cover the sufficiently variable appearance of various people.**  
Datasetì˜ í­ì´ ë„“ê¸° ë•Œë¬¸ì— Trainëœ Modelì´ ë‹¤ì–‘í•œ ì‚¬ëŒë“¤ì˜ ì¶©ë¶„íˆ ë‹¤ì–‘í•œ ëª¨ìŠµì„ ë‹¤ë£° ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

<br>

**Cao et al. [39] conducted a systematic studies on model training using VGGface2 and MSCeleb-1M, and found an optimal model by first training on MS-Celeb-1M (breadth) and then fine-tuning on VGGface2 (depth).**  
Caoet al. [39]ëŠ” VGGface2ì™€ MSCeleb-1Mì„ ì‚¬ìš©í•œ Model Trainì— ëŒ€í•œ ì²´ê³„ì ì¸ ì—°êµ¬ë¥¼ ìˆ˜í–‰í–ˆìœ¼ë©°, MS-Celeb-1M(breadth)ì—ì„œ ë¨¼ì € Trainí•œ ë‹¤ìŒ VGGface2(depth)ì—ì„œ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ ìµœì ì˜ Modelì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.    

<br>
<br>
<br>

### Long tail distribution

<br>
<br>
<br>

**The utilization of long tail distribution is different among datasets.**  
ë¡±í…Œì¼ ë¶„í¬ì˜ í™œìš©ì€ Datasetë§ˆë‹¤ ë‹¤ë¦…ë‹ˆë‹¤. 

<br>

**For example, in Challenge 2 of MS-Celeb-1M, the novel set specially uses the tailed data to study low-shot learning**  
ì˜ˆë¥¼ ë“¤ì–´, MS-Celeb-1Mì˜ ì±Œë¦°ì§€ 2ì—ì„œ ìƒˆë¡œìš´ ì„¸íŠ¸ëŠ” íŠ¹íˆ ê¼¬ë¦¬ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œìš° ìƒ· í•™ìŠµì„ ì—°êµ¬í•©ë‹ˆë‹¤. 

<br>

**central part of the long tail distribution is used by the Challenge 1 of MS-Celeb1M and imagesâ€™ number is approximately limited to 100 for each celebrity;**  
ë¡±í…Œì¼ ë¶„í¬ì˜ ì¤‘ì•™ ë¶€ë¶„ì€ MS-Celeb1Mì˜ Challenge 1ì—ì„œ ì‚¬ìš©ë˜ë©° ì´ë¯¸ì§€ ìˆ˜ëŠ” ìœ ëª…ì¸ë‹¹ ì•½ 100ê°œë¡œ ì œí•œë©ë‹ˆë‹¤. 

<br>

**VGGface and VGGface2 only use the head part to construct deep databases;**  
VGGface ë° VGGface2ëŠ” í—¤ë“œ ë¶€ë¶„ë§Œ ì‚¬ìš©í•˜ì—¬ ì‹¬ì¸µ Databaseë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤. 

<br>

**Megaface utilizes the whole distribution to contain as many images as possible, the minimal number of images is 3 per person and the maximum is 2469.**  
MegafaceëŠ” ê°€ëŠ¥í•œ í•œ ë§ì€ ì´ë¯¸ì§€ë¥¼ í¬í•¨í•˜ê¸° ìœ„í•´ ì „ì²´ ë¶„í¬ë¥¼ í™œìš©í•˜ë©° ìµœì†Œ ì´ë¯¸ì§€ ìˆ˜ëŠ” 1ì¸ë‹¹ 3ê°œ, ìµœëŒ€ 2469ê°œì…ë‹ˆë‹¤.    

<br>
<br>
<br>

### Data engineering

<br>
<br>
<br>

**Several popular benchmarks, such as LFW unrestricted protocol, Megaface Challenge 1, MS-Celeb1M Challenge 1&2, explicitly encourage researchers to collect and clean a large-scale data set for enhancing the capability of deep neural network.**
LFW ë¬´ì œí•œ Protocol, Megaface Challenge 1, MS-Celeb1M Challenge 1&2ì™€ ê°™ì€ ëª‡ ê°€ì§€ ì¸ê¸° ìˆëŠ” benchmarkëŠ” ì—°êµ¬ì›ì´ ì‹¬ì¸µ neural networkì˜ ê¸°ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ëŒ€ê·œëª¨ Datasetë¥¼ ìˆ˜ì§‘í•˜ê³  ì •ë¦¬í•˜ë„ë¡ ëª…ì‹œì ìœ¼ë¡œ ê¶Œì¥í•©ë‹ˆë‹¤.

<br>

**Although data engineering is a valuable problem to computer vision researchers, this protocol is more incline to the industry participants.**  
ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ì€ ì»´í“¨í„° ë¹„ì „ ì—°êµ¬ìì—ê²Œ ì¤‘ìš”í•œ ë¬¸ì œì´ì§€ë§Œ ì´ Protocolì€ ì—…ê³„ ì°¸ì—¬ìì—ê²Œ ë” ì í•©í•©ë‹ˆë‹¤.

<br>

**As evidence, the leaderboards of these experiments are mostly occupied by the companies holding invincible hardwares and data scales.**  
ì¦ê±°ë¡œ, ì´ëŸ¬í•œ ì‹¤í—˜ì˜ ìˆœìœ„í‘œëŠ” ëŒ€ë¶€ë¶„ ë¬´ì ì˜ í•˜ë“œì›¨ì–´ì™€ ë°ì´í„° ê·œëª¨ë¥¼ ë³´ìœ í•œ íšŒì‚¬ê°€ ì°¨ì§€í•˜ê³  ìˆìŠµë‹ˆë‹¤.

<br>

**This phenomenon may not be beneficial for developments of new models in academic community.**  
ì´ëŸ¬í•œ í˜„ìƒì€ í•™ê³„ì—ì„œ ìƒˆë¡œìš´ Modelì„ ê°œë°œí•˜ëŠ” ë° ë„ì›€ì´ ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<br>
<br>
<br>

### Data noise

<br>
<br>
<br>

**Owing to data source and collecting strategies, existing large-scale datasets invariably contain label noises.**  
ë°ì´í„° ì†ŒìŠ¤ ë° ìˆ˜ì§‘ ì „ëµìœ¼ë¡œ ì¸í•´ ê¸°ì¡´ ëŒ€ê·œëª¨ Datasetì—ëŠ” í•­ìƒ ë ˆì´ë¸” ë…¸ì´ì¦ˆê°€ í¬í•¨ë©ë‹ˆë‹¤.

**Wang et al. [124] profiled the noise distribution in existing datasets in Fig. 19 and showed that the noise percentage increases dramatically along the scale of data.**  
Wang et al. [124]ëŠ” ê·¸ë¦¼ 19ì˜ ê¸°ì¡´ Datasetì—ì„œ ë…¸ì´ì¦ˆ ë¶„í¬ë¥¼ í”„ë¡œíŒŒì¼ë§í–ˆìœ¼ë©° ë…¸ì´ì¦ˆ ë¹„ìœ¨ì´ ë°ì´í„° ê·œëª¨ì— ë”°ë¼ ê·¹ì ìœ¼ë¡œ ì¦ê°€í•¨ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.

<br>
<br>
<br>
  <img src="/assets/DeepFaceRecognitionSurvey/Fig_19.png">
<p align="center">
</p>
<br>
<br>

**Moreover, they found that noise is more lethal on a 10,000-class problem of FR than on a 10-class problem of object classification and that label flip noise severely deteriorates the performance of a model, especially the model using A-softmax [84].**  
ë˜í•œ ê°ì²´ ë¶„ë¥˜ì˜ 10ê°œ í´ë˜ìŠ¤ ë¬¸ì œë³´ë‹¤ FRì˜ 10,000ê°œ í´ë˜ìŠ¤ ë¬¸ì œì—ì„œ ë…¸ì´ì¦ˆê°€ ë” ì¹˜ëª…ì ì´ë©° ë ˆì´ë¸” í”Œë¦½ ë…¸ì´ì¦ˆê°€ Model, íŠ¹íˆ A-softmaxë¥¼ ì‚¬ìš©í•˜ëŠ” Modelì˜ ì„±ëŠ¥ì„ ì‹¬ê°í•˜ê²Œ ì €í•˜ì‹œí‚¨ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤[84].

<br>

**Therefore, building a sufficiently large and clean dataset for academic research is very meaningful.**  
ë”°ë¼ì„œ í•™ìˆ  ì—°êµ¬ë¥¼ ìœ„í•´ ì¶©ë¶„íˆ í¬ê³  ê¹¨ë—í•œ Datasetë¥¼ êµ¬ì¶•í•˜ëŠ” ê²ƒì€ ë§¤ìš° ì˜ë¯¸ê°€ ìˆìŠµë‹ˆë‹¤.

<br>

**Deng et al. [106] found there are serious label noise in MS-Celeb-1M [45], and they cleaned the noise of MS-Celeb-1M, and made the refined dataset public available.**  
Deng et al. [106]ì€ MS-Celeb-1M[45]ì—ì„œ ì‹¬ê°í•œ ë ˆì´ë¸” ë…¸ì´ì¦ˆê°€ ìˆìŒì„ ë°œê²¬í–ˆìœ¼ë©° MS-Celeb-1Mì˜ ë…¸ì´ì¦ˆë¥¼ ì •ë¦¬í•˜ê³  ì •ì œëœ Datasetë¥¼ ê³µê°œí–ˆìŠµë‹ˆë‹¤.

<br>

**Microsoft and Deepglint jointly released the largest public data set [163] with cleaned labels, which includes 4M images cleaned from MS-Celeb-1M dataset and 2.8M aligned images of 100K Asian celebrities.**  
Microsoftì™€ DeepglintëŠ” MS-Celeb-1M ë°ì´í„°ì„¸íŠ¸ì—ì„œ ì •ë¦¬ëœ 4ë°±ë§Œ ê°œì˜ ì´ë¯¸ì§€ì™€ 100,000ëª…ì˜ ì•„ì‹œì•„ ìœ ëª…ì¸ì‚¬ì˜ ì •ë ¬ëœ ì´ë¯¸ì§€ 280ë§Œ ê°œë¥¼ í¬í•¨í•˜ëŠ” ì •ë¦¬ëœ ë ˆì´ë¸”ì´ í¬í•¨ëœ ê°€ì¥ í° ê³µê°œ Dataset[163]ë¥¼ ê³µë™ìœ¼ë¡œ ë°œí‘œí–ˆìŠµë‹ˆë‹¤.

<br>

**Moreover, Zhan et al. [167] shifted the focus from cleaning the datasets to leveraging more unlabeled data.**  
ë˜í•œ, Zhan et al. [167]ì€ Dataset ì •ë¦¬ì—ì„œ ë ˆì´ë¸”ì´ ì§€ì •ë˜ì§€ ì•Šì€ ë” ë§ì€ ë°ì´í„° í™œìš©ìœ¼ë¡œ ì´ˆì ì„ ì´ë™í–ˆìŠµë‹ˆë‹¤. 

<br>

**Through automatically assigning pseudo labels to unlabeled data with the help of relational graphs, they obtained competitive or even better results over the fullysupervised counterpart.**  
ê´€ê³„í˜• ê·¸ë˜í”„ì˜ ë„ì›€ìœ¼ë¡œ ë ˆì´ë¸”ì´ ì§€ì •ë˜ì§€ ì•Šì€ ë°ì´í„°ì— ì˜ì‚¬ ë ˆì´ë¸”ì„ ìë™ìœ¼ë¡œ í• ë‹¹í•¨ìœ¼ë¡œì¨ fullysupervised counterpartë³´ë‹¤ ê²½ìŸë ¥ ìˆê±°ë‚˜ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ì–»ì—ˆìŠµë‹ˆë‹¤.      

<br>
<br>
<br>

### Data bias

<br>
<br>
<br>

**Large-scale training datasets, such as CASIAWebFace [120], VGGFace2 [39] and MS-Celeb-1M [45], are typically constructed by scraping websites like Google Images, and consist of celebrities on formal occasions: smiling, makeup, young, and beautiful.**  
CASIAWebFace [120], VGGFace2 [39] ë° MS-Celeb-1M [45]ê³¼ ê°™ì€ ëŒ€ê·œëª¨ êµìœ¡ DatasetëŠ” ì¼ë°˜ì ìœ¼ë¡œ Google ì´ë¯¸ì§€ì™€ ê°™ì€ ì›¹ ì‚¬ì´íŠ¸ë¥¼ ìŠ¤í¬ë©í•˜ì—¬ êµ¬ì„±ë˜ë©° ê³µì‹ í–‰ì‚¬ì—ì„œ ì›ƒê³ , í™”ì¥í•˜ê³  ì Šê³  ì•„ë¦„ë‹¤ìš´ ìœ ëª…ì¸ì‚¬ì˜ ì´ë¯¸ì§€ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

<br>

**They are largely different from databases captured in the daily life (e.g. Megaface).**  
ì¼ìƒ ìƒí™œì—ì„œ ìº¡ì²˜í•œ Database(ì˜ˆ: ë©”ê°€í˜ì´ìŠ¤)ì™€ëŠ” í¬ê²Œ ë‹¤ë¦…ë‹ˆë‹¤.

<br>

**The biases can be attributed to many exogenous factors in data collection, such as cameras, lightings, preferences over certain types of backgrounds, or annotator tendencies.**  
í¸í–¥ì€ ì¹´ë©”ë¼, ì¡°ëª…, íŠ¹ì • ìœ í˜•ì˜ ë°°ê²½ì— ëŒ€í•œ ì„ í˜¸ë„ ë˜ëŠ” ì£¼ì„ ì‘ì„±ì ê²½í–¥ê³¼ ê°™ì€ ë°ì´í„° ìˆ˜ì§‘ì˜ ë§ì€ ì™¸ìƒì  ìš”ì¸ì— ê¸°ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<br>

**Dataset biases adversely affect cross-dataset generalization; that is, the performance of the model trained on one dataset drops significantly when applied to another one.**  
Dataset biasesëŠ” cross-dataset generalizationì— ì•…ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤. ì¦‰, í•œ Datasetì—ì„œ Trainëœ Modelì˜ ì„±ëŠ¥ì€ ë‹¤ë¥¸ Datasetì— ì ìš©ë  ë•Œ í¬ê²Œ ë–¨ì–´ì§‘ë‹ˆë‹¤.

<br>

**One persuasive evidence is presented by P.J. Phillipsâ€™ study [168] which conducted a cross benchmark assessment of VGGFace model [37] for face recognition.**  
P.J. Phillipsì˜ ì—°êµ¬[168]ëŠ” ì–¼êµ´ ì¸ì‹ì„ ìœ„í•œ VGGFace Model[37]ì˜ êµì°¨ benchmark Evaluationë¥¼ ìˆ˜í–‰í•œ ì„¤ë“ë ¥ ìˆëŠ” ì¦ê±° ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.

<br>

**The VGGFace model achieves 98.95% on LFW [23] and 97.30% on YTF [169], but only obtains 26%, 52% and 85% on Ugly, Bad and Good partition of GBU database [170].**  
VGGFace Modelì€ LFW[23]ì—ì„œ 98.95%, YTF[169]ì—ì„œ 97.30%ë¥¼ ë‹¬ì„±í–ˆì§€ë§Œ GBU Database[170]ì˜ Ugly, Bad ë° Good íŒŒí‹°ì…˜ì—ì„œëŠ” 26%, 52% ë° 85%ë§Œ ì–»ì—ˆìŠµë‹ˆë‹¤.

<br>

**Demographic bias (e.g., race/ethnicity, gender, age) in datasets is a universal but urgent issue to be solved in data bias field.**  
ë°ì´í„°ì…‹ì˜ ì¸êµ¬í•™ì  í¸í–¥(ì˜ˆ: ì¸ì¢…/ë¯¼ì¡±, ì„±ë³„, ì—°ë ¹)ì€ ë³´í¸ì ì´ì§€ë§Œ ë°ì´í„° í¸í–¥ ë¶„ì•¼ì—ì„œ í•´ê²°í•´ì•¼ í•  ì‹œê¸‰í•œ ë¬¸ì œì…ë‹ˆë‹¤.

<br>

**In existing training and testing datasets, the male, White, and middle-aged cohorts always appear more frequently, as shown in Table VII, which inevitably causes deep learning models to replicate and even amplify these biases resulting in significantly different accuracies when deep models are applied to different demographic groups.**  
ê¸°ì¡´ Train ë° Test Datasetì—ì„œ ë‚¨ì„±, ë°±ì¸ ë° ì¤‘ë…„ ì½”í˜¸íŠ¸ëŠ” í‘œ VIIì— í‘œì‹œëœ ê²ƒì²˜ëŸ¼ í•­ìƒ ë” ìì£¼ ë‚˜íƒ€ë‚©ë‹ˆë‹¤. ì´ëŠ” í•„ì—°ì ìœ¼ë¡œ Deep Learning Modelì´ ë‹¤ì–‘í•œ ì¸êµ¬ í†µê³„ ê·¸ë£¹ì— ì ìš©ë  ë•Œ ìƒë‹¹íˆ ë‹¤ë¥¸ ì •í™•ë„ë¥¼ ì´ˆë˜í•©ë‹ˆë‹¤. 

<br>
<br>
<br>
  <img src="/assets/DeepFaceRecognitionSurvey/Table_07.png">
<p align="center">
</p>
<br>
<br>

