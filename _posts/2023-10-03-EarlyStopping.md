---
title: "Early Stopping in Tensorflow"
date: 2023-09-06 08:26:28 -0400
categories: Deep Learning
---

# Early Stopping in Tensorflow

<br>
<br>
<br>

* ì´ë²ˆ Postì—ì„œëŠ” Tensorflowì˜ Callbadkì¤‘ í•˜ë‚˜ì¸, EarlyStoppingì— ëŒ€í•´ì„œ ì•Œì•„ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

<br>
<br>

## 0. Callback   

<br>

Tensorflowì—ì„œ Trainì„ ì‹œì‘í•˜ëŠ” í•¨ìˆ˜ì¸ .fit()ì„ í˜¸ì¶œí•˜ë©´ TensorflowëŠ” ë§ˆì¹˜ ë¸Œë ˆì´í¬ ê³ ì¥í•œ í­ì£¼ ê¸°ê´€ì°¨ì™€ ê°™ì€ ìƒíƒœê°€ ë©ë‹ˆë‹¤.

ì§€ì •í•œ Epochì„ ë‹¤ ëë§ˆì¹  ë•Œê¹Œì§€ ë©ˆì¶œìˆ˜ë„ ì—†ê³ , í˜„ì¬ ìƒíƒœê°€ ì–´ë–¤ì§€ ì•Œìˆ˜ë„ ì—†ìœ¼ë©°

ê°ì¢… Trainingê´€ë ¨ ì§€í‘œë“¤(Loss , Accuracy ë“±ë“±)ì´ ì–´ë–»ê²Œ ë°”ë€Œê³  ìˆëŠ”ì§€ í™•ì¸í•  ë°©ë²•ì´ ì—†ìŠµë‹ˆë‹¤.

ê·¸ë˜ì„œ Tensorflowì—ì„œëŠ” ì´ë ‡ê²Œ Trainì´ ì§„í–‰ë˜ëŠ” ë™ì•ˆ ë‹¤ì–‘í•œ ì œì–´ ë° ê´€ì°°ì„ í•  ìˆ˜ ìˆë„ë¡ ë‹¤ì–‘í•œ Callback ê¸°ëŠ¥ì„ êµ¬í˜„í•´ ë‘ì—ˆìŠµë‹ˆë‹¤.

Tensorflowì—ì„œ ì§€ì›í•˜ëŠ” ë‹¤ì–‘í•œ Callbackë“¤ì€ ì•„ë˜ Linkì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.   

https://www.tensorflow.org/api_docs/python/tf/keras/callbacks

<br>
<br>

## 1. Early Stopping

<br>
<br>

ê·¸ ì¤‘ì—ì„œ ì´ë²ˆ Postì—ì„œ ì•Œì•„ë³¼ Callbackì€ EarlyStoppingì´ë¼ëŠ” Callbackì…ë‹ˆë‹¤.

ì•ì„œ ë§í–ˆë“¯ì´, Trainì´ ì‹œì‘ë˜ë©´ TensorflowëŠ” .fit()ì—ì„œ ì§€ì •í•œ Epochì„ ë‹¤ ì§„í–‰í•  ë•Œê¹Œì§€ Trainì„ ë©ˆì¶”ì§€ ì•ŠìŠµë‹ˆë‹¤.

ê·¸ë˜ì„œ ì–´ëŠ ì •ë„ Modelì˜ ì„±ëŠ¥ì´ ë‚˜ì˜¤ëŠ” ì‹œì ì´ ë˜ë”ë¼ë„ Train ì¤‘ê°„ì— ë©ˆì¶œ ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.

ì´ ë•Œ í•„ìš”í•œ ê²ƒì´ Early Stopping Callbackì…ë‹ˆë‹¤.

EarlyStoppingì€ ìš°ë¦¬ê°€ ì§€ì •í•´ì¤€ íŠ¹ì • ì§€í‘œê°€ ê¸°ì¤€ì¹˜ì— ë„ë‹¬í•˜ë©´ Trainì„ ë©ˆì¶”ë„ë¡ í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.

ì•„ë˜ê°€ EarlyStopping Classì…ë‹ˆë‹¤.   

<br>
<br>

```python
tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    min_delta=0,
    patience=0,
    verbose=0,
    mode='auto',
    baseline=None,
    restore_best_weights=False,
    start_from_epoch=0
)
```

<br>

* **monitor**
  - Monitoringí•  ê°’. ê¸°ë³¸ê°’ì€ 'val_loss'  

<br>

* **min_delta**
  - ê°œì„ ìœ¼ë¡œ ê°„ì£¼ë  Monitoringëœ ìˆ˜ëŸ‰ì˜ ìµœì†Œ ë³€í™”ëŸ‰. ì¦‰, min_deltaë³´ë‹¤ ì ì€ ì ˆëŒ€ì ì¸ ë³€í™”ëŠ” ê°œì„ ìœ¼ë¡œ ê°„ì£¼ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
  - ê¸°ë³¸ê°’ì€ 0

<br>

* **patience**
  - ê°œì„ ì´ ì—†ëŠ” ìƒíƒœë¡œ ì§€ì†ë˜ëŠ” Epoch íšŸìˆ˜. ì´ íšŸìˆ˜ ì´í›„ì— í›ˆë ¨ì´ ì¤‘ë‹¨ë©ë‹ˆë‹¤.
  - ê¸°ë³¸ê°’ì€ 0

<br>

* **verbose**
  - ë©”ì‹œì§€ ì¶œë ¥ ëª¨ë“œ, 0 ë˜ëŠ” 1. 0ì´ë©´ ì•„ë¬´ëŸ° ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•˜ì§€ ì•Šê³ , 1ì´ë©´ Callbackì´ ì‘ë™í•  ë•Œ ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
  - ê¸°ë³¸ê°’ì€ 0

<br>

* **mode**
  - {"auto", "min", "max"} ì¤‘ í•˜ë‚˜.
  - "min" ëª¨ë“œì—ì„œëŠ” Monitoringë˜ëŠ” ìˆ˜ëŸ‰ì˜ ê°ì†Œê°€ ë©ˆì¶œ ë•Œ í›ˆë ¨ì´ ì¤‘ë‹¨ë©ë‹ˆë‹¤.
  - "max" ëª¨ë“œì—ì„œëŠ” ì¦ê°€ê°€ ë©ˆì¶œ ë•Œ ì¤‘ë‹¨ë©ë‹ˆë‹¤.
  - "auto" ëª¨ë“œì—ì„œëŠ” Monitoringë˜ëŠ” ìˆ˜ëŸ‰ì˜ ì´ë¦„ìœ¼ë¡œë¶€í„° ë°©í–¥ì´ ìë™ìœ¼ë¡œ ì¶”ë¡ ë©ë‹ˆë‹¤.
  - ê¸°ë³¸ê°’ì€ 'auto'

<br>

* **baseline**
  - Monitoringë˜ëŠ” ìˆ˜ëŸ‰ì— ëŒ€í•œ ê¸°ì¤€ê°’. ëª¨ë¸ì´ ê¸°ì¤€ê°’ë³´ë‹¤ í–¥ìƒë˜ì§€ ì•Šìœ¼ë©´ í›ˆë ¨ì´ ì¤‘ë‹¨ë©ë‹ˆë‹¤.
  - ê¸°ë³¸ê°’ì€ None

<br>

* **restore_best_weights**
  - Monitoringëœ ìˆ˜ëŸ‰ì˜ ìµœì ê°’ì„ ê°€ì§„ ì—í­ì—ì„œ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë³µì›í• ì§€ ì—¬ë¶€.
  - Falseì¸ ê²½ìš° í›ˆë ¨ì˜ ë§ˆì§€ë§‰ ë‹¨ê³„ì—ì„œ ì–»ì€ ëª¨ë¸ ê°€ì¤‘ì¹˜ê°€ ì‚¬ìš©ë©ë‹ˆë‹¤.
  - ê¸°ì¤€ì— ìƒëŒ€ì ì¸ ì„±ëŠ¥ê³¼ ê´€ê³„ì—†ì´ Epochì´ ë³µì›ë©ë‹ˆë‹¤.
  - ì–´ë–¤ Epochë„ ê¸°ì¤€ì„ í–¥ìƒì‹œí‚¤ì§€ ì•Šìœ¼ë©´ í›ˆë ¨ì€ Patience Epochë™ì•ˆ ì‹¤í–‰ë˜ê³  í•´ë‹¹ ì„¸íŠ¸ì—ì„œ ìµœì ì˜ Epochì˜ ê°€ì¤‘ì¹˜ê°€ ë³µì›ë©ë‹ˆë‹¤.
  - ê¸°ë³¸ê°’ì€ False

<br>

* **start_from_epoch**
  - ê°œì„ ì„ Monitoringí•˜ê¸° ì‹œì‘í•˜ê¸° ì „ì— ëŒ€ê¸°í•  Epochì˜ íšŸìˆ˜.
  - ì´ë¥¼ í†µí•´ ì´ˆê¸°ì—ëŠ” ê°œì„ ì´ ê¸°ëŒ€ë˜ì§€ ì•ŠëŠ” Warming Up ê¸°ê°„ì„ ì„¤ì •í•  ìˆ˜ ìˆìœ¼ë©°, ì´ ê¸°ê°„ ë™ì•ˆ í›ˆë ¨ì´ ì¤‘ë‹¨ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
  - ê¸°ë³¸ê°’ì€ 0

<br>
<br>
<br>

## 2. Example

<br>
<br>

MNIST Datasetì˜ Classification Exampleë¡œ ì‹¤ì œë¡œ ì–´ë–»ê²Œ ì‚¬ìš©í•˜ëŠ”ì§€ í™•ì¸í•´ ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.   

<br>

* í•„ìš”í•œ Packageë¥¼ Loadí•©ë‹ˆë‹¤.   

<br>

```python
import tensorflow as tf
import tensorflow_datasets as tfds
```

<br>
<br>

* Trainì— ì‚¬ìš©í•  2ê°œì˜ Datasetì„ MNISTë¡œ ë§Œë“­ë‹ˆë‹¤.   

<br>

```python
(ds_train, ds_test), ds_info = tfds.load(
  'mnist',
  split=['train', 'test'],
  shuffle_files=True,
  as_supervised=True,
  with_info=True,
)

print(type(ds_train))
```

<br>

    [1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\Users\Moon\tensorflow_datasets\mnist\3.0.1...[0m
    Dl Completed...: 0 url [00:00, ? url/s]
    Dl Size...: 0 MiB [00:00, ? MiB/s]
    Extraction completed...: 0 file [00:00, ? file/s]
    Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]
    Generating train examples...: 0 examples [00:00, ? examples/s]
    Shuffling C:\Users\Moon\tensorflow_datasets\mnist\3.0.1.incompleteXYPQ2U\mnist-train.tfrecord*...:   0%|      â€¦
    Generating test examples...: 0 examples [00:00, ? examples/s]
    Shuffling C:\Users\Moon\tensorflow_datasets\mnist\3.0.1.incompleteXYPQ2U\mnist-test.tfrecord*...:   0%|       â€¦
    [1mDataset mnist downloaded and prepared to C:\Users\Moon\tensorflow_datasets\mnist\3.0.1. Subsequent calls will reuse this data.[0m
    <class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>

<br>
<br>

* Imageë¥¼ Preprocessingí•´ì„œ Labelê³¼ ê°™ì´ Returní•´ì£¼ëŠ” Mapping í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤.   

<br>

```python
def normalize_img(image, label):
    return tf.cast(image, tf.float32) / 255., label
```

<br>
<br>

* Train & Val. Datasetì„ ë§Œë“­ë‹ˆë‹¤. ì¢€ ì „ì— ë§Œë“  Mapping Functionì„ ì ìš©í•´ ì¤ë‹ˆë‹¤.   

<br>

```python
dataset_train = ds_train.map(normalize_img)
dataset_train = ds_train.batch(128)

dataset_test = ds_test.map(normalize_img)
dataset_test = ds_test.batch(128)
```

<br>
<br>

* Model êµ¬ì¡°ëŠ” Simplieí•˜ê²Œ ë§Œë“¤ì–´ ì¤ì‹œë‹¤.   

<br>

```python
model = tf.keras.models.Sequential([
tf.keras.layers.Flatten(input_shape=(28, 28)),
tf.keras.layers.Dense(128, activation='relu'),
tf.keras.layers.Dense(10)
])
```

<br>
<br>

* ì‘ì„±í•œ Modelì„ Optimizerì™€ Loss Function ë“±ì„ ì •ì˜í•´ì„œ Compileí•´ ì¤ë‹ˆë‹¤.   

<br>

```python
model.compile(
  optimizer=tf.keras.optimizers.Adam(0.006),
  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],
)
```

<br>
<br>

* ì, ì´ì œ ë“œë””ì–´ Early Stoppingì„ ì •ì˜í•©ë‹ˆë‹¤.
* ì•„ë˜ì˜ Exampleì—ì„œëŠ” Monitoringí•  ê°’ì„ 'loss'ë¥¼ ì„ íƒí–ˆìŠµë‹ˆë‹¤.
* ì´ ê°’ì€ Train Lossë¥¼ ë§í•©ë‹ˆë‹¤.
* 'patience'ê°’ì€ 2ë¡œ ì„¤ì •í–ˆìœ¼ë©°, ì´ëŠ” **2 Epochì´ ì§€ë‚˜ë„ Train Lossê°€ ë‚˜ì•„ì§€ì§€ ì•ŠëŠ”ë‹¤ë©´ Trainì„ ì¢…ë£Œ**í•˜ë¼ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.

<br>

```python
callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2)
```

<br>

* 'monitor' ê°’ì— ë“¤ì–´ê°ˆ ìˆ˜ ìˆëŠ” ê°’ë“¤ì€ ë‹¤ìŒê³¼ ê°™ì€ ê²ƒë“¤ì´ ìˆìŠµë‹ˆë‹¤.
  - **loss** : Train Datasetì˜ Loss ê°’
  - **val_loss** : Validation Datasetì˜ Loss ê°’
  - **acc** : Train Datasetì˜ Accuracy ê°’
  - **val_acc** : Validation Datasetì˜ Accuracy ê°’

<br>
 
* ì €ëŠ” ì£¼ë¡œ val_lossì„ ë§ì´ ì‚¬ìš©í•˜ëŠ”ë°, ìƒí™©ì— ë§ê²Œ ì ì ˆí•œ ê°’ì„ ì‚¬ìš©í•˜ì‹œë©´ ë©ë‹ˆë‹¤.

<br>
<br>

* ì´ì œ fit()ì„ í˜¸ì¶œí•  ë•Œ, **callback=[]** ì´ë¼ëŠ” Parameterì— ì‚¬ìš©í•  Callbackë“¤ì„ ì¶”ê°€í•´ ì£¼ë©´ ë©ë‹ˆë‹¤.
* ì´ë²ˆ Exampleì—ì„œëŠ” EarlyStoppingë§Œ ì‚¬ìš©í–ˆê¸° ë•Œë¬¸ì— EarlyStopping Callbackë§Œ ì¶”ê°€ëœ ëª¨ìŠµì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<br>

```python
history = model.fit(
  dataset_train,
  epochs=100,
  validation_data=dataset_test,
  callbacks=[callback]
)

print(len(history.history['loss']))
```

<br>

    Epoch 1/100
    469/469 [==============================] - 4s 5ms/step - loss: 4.8275 - sparse_categorical_accuracy: 0.7964 - val_loss: 0.5860 - val_sparse_categorical_accuracy: 0.8636
    Epoch 2/100
    469/469 [==============================] - 2s 4ms/step - loss: 0.4836 - sparse_categorical_accuracy: 0.8838 - val_loss: 0.4888 - val_sparse_categorical_accuracy: 0.8925
    Epoch 3/100
    469/469 [==============================] - 2s 4ms/step - loss: 0.3803 - sparse_categorical_accuracy: 0.9041 - val_loss: 0.4292 - val_sparse_categorical_accuracy: 0.9078
    Epoch 4/100
    469/469 [==============================] - 2s 4ms/step - loss: 0.3369 - sparse_categorical_accuracy: 0.9140 - val_loss: 0.3596 - val_sparse_categorical_accuracy: 0.9195
    Epoch 5/100
    469/469 [==============================] - 2s 4ms/step - loss: 0.3433 - sparse_categorical_accuracy: 0.9141 - val_loss: 0.3465 - val_sparse_categorical_accuracy: 0.9153
    Epoch 6/100
    469/469 [==============================] - 2s 4ms/step - loss: 0.3428 - sparse_categorical_accuracy: 0.9160 - val_loss: 0.4269 - val_sparse_categorical_accuracy: 0.8972
    6

<br>
<br>

* ê° Epochë§ˆë‹¤ Train Lossë¥¼ ì˜ ë³´ì‹œë©´, 4ë²ˆì§¸ Epochì—ì„œ Lossê°€ 0.3369ë¥¼ ê¸°ë¡í•˜ê³  ê·¸ ë‹¤ìŒ 2ë²ˆì˜ Epochì—ì„œ ë” ì´ìƒ Lossê°€ ì¤„ì–´ë“¤ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
* ì´ ì¡°ê±´ì´ Early Stopping ì¡°ê±´ì„ ë§Œì¡±í–ˆê¸° ë•Œë¬¸ì— Trainì´ ìë™ìœ¼ë¡œ ë©ˆì¶”ì—ˆìŠµë‹ˆë‹¤.

<br>
